[["index.html", "Úvod do statistiky Kapitola 1 Úvod 1.1 Příklad popisné statistiky 1.2 Příklad inferenční statistiky 1.3 Simulace 1.4 O modelech", " Úvod do statistiky Jan Schubert 2022-02-18 Kapitola 1 Úvod Tato kniha navazuje na kurz Základy logiky a matematiky (JSB536), ale jeho absolvování není nutné k pochopení látky. Statistika je nástroj, který aplikuje matematiku na získání užitečných informací z dat. Roli statistiky je možné rozdělit na dva úkoly: Popisování většího množství dat (popisná statistika) Předpovídání nějakého fenomenu/určení míry nejistoty (inferenční statistika) 1.1 Příklad popisné statistiky Mnoho fenomenů v každodenním světě je možné vyjádřit pomocí dat. Fotka se dá vyjádřit jako 3D matice (red, green, blue) řádků a sloupců, která vyjadřuje jednotlivé pixely. Lidl Stiftung &amp; Co. KG, Public domain, via Wikimedia Commons obrazek &lt;- png::readPNG(&quot;../imgs/img1.1.svg.png&quot;) dim(obrazek) ## [1] 480 480 3 Můžeme například zobrazit hodnoty červené barvy prvních 5x5 pixelů. obrazek[1:5, 1:5, 1] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1.0000000 ## [2,] 1 1 1 1 1.0000000 ## [3,] 1 1 1 1 1.0000000 ## [4,] 1 1 1 1 1.0000000 ## [5,] 1 1 1 1 0.3215686 Můžeme zobrazit pouze nějaké řádky a sloupce: plot(0:1, 0:1, type = &quot;n&quot;, ann = FALSE, axes = FALSE) rasterImage(obrazek[125:325, 100:300, ], 0, 0, 1, 1) Můžeme vypočítat průměrnou nebo medianovou barvu. # vypocitame prumer pro kazdou barvu (cervena, zelena, modra) prumerna_barva &lt;- sapply(1:3, function(i) mean(obrazek[, , i])) # vypocitame barvu na rgb skale prumerna_barva &lt;- rgb(prumerna_barva[1], prumerna_barva[2], prumerna_barva[3]) # vypocitame median pro kazdou barvu (cervena, zelena, modra) median_barva &lt;- sapply(1:3, function(i) median(obrazek[, , i])) # vypocitame barvu na rgb skale median_barva &lt;- rgb(median_barva[1], median_barva[2], median_barva[3]) # zobrazime v grafu plot(1,1, type = &quot;n&quot;, axes = FALSE, ann = FALSE, xlim = c(0, 2), ylim = c(0, 1)) rect(0, 0, 1, 0.9, col = prumerna_barva) text(0.45, 1, labels = paste0(&quot;Průměrná barva: &quot;, prumerna_barva)) rect(1, 0, 2, 0.9, col = median_barva) text(1.45, 1,labels = paste0(&quot;Mediánová barva: &quot;, median_barva)) Protože obrázek je matice dat, můžeme na ni uplatnit různé statistické metody. Zajímá nás například, jaké odstiny barev jsou použité v tomto logu. Vidíme, že logo se skládá ze tří barev a můžeme extrahovat 3 barevy pomocí shlukovacího algoritmu. set.seed(42) k &lt;- 3 cervena &lt;- as.vector(obrazek[, , 1]) zelena &lt;- as.vector(obrazek[, , 2]) modra &lt;- as.vector(obrazek[, , 3]) m &lt;- kmeans(cbind(cervena, zelena, modra), centers = k) barvy &lt;- m$centers barvy_rgb &lt;- rep(NA, k) plot(seq(1, k * 10 + 10, length.out = 10), seq(1, 10, length.out = 10), axes = FALSE, ann = FALSE, type = &quot;n&quot; ) for (i in 1:k) { barvy_rgb[i] &lt;- rgb(barvy[i, 1], barvy[i, 2], barvy[i, 3]) rect(i * 10, 1, i * 10 + 10, 8, col = barvy_rgb[i]) text(i * 10 + 5, 9, labels = barvy_rgb[i]) } 1.2 Příklad inferenční statistiky Většina jevů okolo nás je ovlivněna náhodou, ať už z důvodu náhodného výběru nebo protože jsou součástí nějakého komplikovaného systému, který ovliňuje hodnoty jevu, který nás zajímá. Počet aut, které projedou na mostě, délka toaletního paríru, který je vyrobený v továrně, to jsou některé příklady jevů, které jsou ovlivněny náhodou. Statistika nám poskytuje soubor nástrojů, jak tuto náhodu (nejistotu) kvantifikovat. Pohlaví dětí je určeno náhodou a nově narozené dítě má zhruba stejnou pravděpodobnost, že bude děvče nebo chlapec. Řekněme, že z důvodu kapacitního plánování nás zajímá, kolik chlapců se narodí, pokud se v porodnoci denně narodí 25 dětí (sám počet narozených dětí by se dal modelovat jako náhodná proměnná). n &lt;- 25 p &lt;- 0.5 x &lt;- c(0:25) pmf &lt;- dbinom(x = x, size = n, p) plot(x, pmf, type = &quot;h&quot;, xlab = &quot;Počet chlapců z 25 narozených dětí&quot;, ylab = &quot;Pravděpodobnost&quot;, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 1.1: Pravděpodobnostní rozložení počtu chlapců z 25 narozených dětí Vidíme, že nejpravděpodobněji se narodí 12 chlapců (v 31% případů). Můžeme také z grafu vypočítat, že více než 15 chlapců se narodí zhruba v 11% případů, tedy zhruba 42 dní v roce. 1.3 Simulace K pochopení statistiky budeme používat programovací jazyk. Ten nám umožní, abychom si statistické koncepty osahali detailně. Budeme simulovat data, u kterých budeme vědět pravé hodnoty a sledovat, jak (ne)úspěšně různé statistické postupy pravé hodnoty odhadují. Cílem je, abychom statistiku pochopili tak, že ji budeme moci použít na konkrétní problém. Chceme dosáhnout toho, aby statistika byla jazykem, který můžeme použít na různé datové problémy. K pochopení statistiky budeme používat programovací jazyk R. Našim cílem je ale koncepty vysvětlovat a kódovat obecně tak, aby postupy byly lehko přenositelné do jiného programovacího jazyka. Vždy si tedy vysvětlíme konkétní výpočet nebo proceduru do podrobna a to i když existuje balíček nebo funkce, která by daný výpočet provedla za nás. 1.4 O modelech Modely jsou reprezentaci reality. Jsou (někdy) užitečné, protože zjednodušeně ukazují vlastnosti toho, co nás zajímá. Glóbus je příkladem modelu planety země. Glóbus nevystihuje přesně to, jak planeta vypadá. Nejsou na něm zaznamenány všechny ostrovy, jeho tvar neodpovídá přesně tvaru naší planety. Přesto jsou glóbusy užitečné k pochopení toho, jak planeta vypadá. Dalším příkladem modelu je mapa. Mapa je ještě více zkresleným modelem terénu než glóbus (mapa musí vněstnat 3D svět do 2D modelu). Mnoho map dokonce velmi nepřesně reprezentuje terén, přesto jsou ale mapy nesmírně užitečné když se potřebujeme dostat z bodu A do bodu B. Stejně je tomu s modely statistickými. Nejsou přesným vyjádřením reality, ale mohou být užitečným vyjádřením reality. Jejich užitečnost bude záviset na činnosti, pro který jsme tento model stvořili. Důležité je dodat, že naše modely (ne)fungují na datech, které jsme jim dodali. Model, který je užitečný na jedněch datech může být bezcenný na jiných datech. Je tedy vždy potřeba přemýšlet o tom, zda je náš model vhodný pro data a situaci, na kterou se ho snažíme použít. "],["popisná-statistika.html", "Kapitola 2 Popisná statistika 2.1 Míry centrální tendence 2.2 Míry rozptýlenosti 2.3 Cvičení", " Kapitola 2 Popisná statistika Jak jsme zmínili v úvodu popisná statistika je jeden z hlavních cílů statistiky. Úkolem popisné statistiky je shrnout informace o našem výběru do pár čísel, které nám pomohou pochopit jaké má náš výběr vlastnosti. Hlavními vlastnostmi, které nás zajímají je: Jaká je typická hodnota měřené proměnné (míra centrální tendence) Na kolik se liší hodnoty jendotlivých pozorování (míra rozptýlenosti) Abychom si popisnou statistiku představili, budeme používat Novoroční/Vánoční projev prezidenta republiky. Nejdříve si data načteme. Text můžeme načíst různými způsoby, my použijeme funkci readLines, která vrátí zpět vektor. prezident &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/prezident.txt&quot;, encoding = &quot;UTF-8&quot;) Nejdříve text očistíme o mezery a čárky, přeneseme vše do malých písmen, rozdělíme na věty a potom na slova. Protože tuto proceduru budeme dělat vícekrát uděláme si na to funkci. Výstupem této funkce bude list, jehož každý element reprezentuje jednu větu a v rámci této věty jaká obsahuje slova. vycistit_text &lt;- function(.text) { # vse malym pismem .text &lt;- tolower(.text) # odstranime prazdne radky .text &lt;- .text[.text != &quot;&quot;] # odstranime carky .text &lt;- gsub(pattern = &quot;,&quot;, replacement = &quot;&quot;, x = .text) # rozdelit na vety, pokud najdeme &quot;.&quot; nebo &quot;!&quot; nebo &quot;?&quot; .text &lt;- unlist(strsplit(.text, split = &quot;\\\\.|\\\\!|\\\\?&quot;)) # vymazeme mezery na zacatku a konci .text &lt;- trimws(.text, which = c(&quot;left&quot;)) .text &lt;- trimws(.text, which = c(&quot;right&quot;)) # odstranime prazdne prvky, ktere vznikly protoze po tecce neni zadny text .text &lt;- .text[.text != &quot;&quot;] # rozdelime na slova slova &lt;- list() for (i in 1:length(.text)) { slova[[i]] &lt;- unlist(strsplit(.text[i], split = &quot; &quot;)) } return(slova) } prezident_clean &lt;- vycistit_text(prezident) 2.1 Míry centrální tendence Míry centrální tendence se snaží popsat nějakou typickou hodnotu proměnné. My si představíme modus, medián, průměr, absolutní a relativní četnost. Jaké míry centrální tendence můžeme na proměnné vypočítat se liší podle typu proměnné. Nominální proměnná je taková proměnná, u které nemůžeme hodnoty seřadit od nejmenšího po největší a nemůžeme ani určit o kolik je jedna hodnota větší než jiná. Tou nejzákladnější popisnou statistikou je četnost nějakého jevu a z ní odvozená míra centrální tendence modus. Modus je tedy nejčastější hodnota proměnné. Když se zamyslíte, tak u proměnné u které nemůžeme hodnoty seřadit ani jinak matematicky porovnat je nejčastější hodnota nejvíce vypovídající o typické hodnotě proměnné. Pojďme si jako příklad vypočítat nejčastější slovo z projevu. # nejprve prevedeme list na vekor slova &lt;- unlist(prezident_clean) tabulka_slov &lt;- table(slova) Naše tabulka četností je velká, obsahuje 651 hodnot. To mimo jiné znamená, že v projevu bylo použito 651 slov. Abychom získali nejčastější hodnotu musíme si tabulku seřadit od největší četnost po nejmenší a zobrazit prvních 10 hodnot. tabulka_slov_serazena &lt;- sort(tabulka_slov, decreasing = TRUE) Modus této proměnné by byla hodnota “a,” která se vyskytla 45x. To je v textové analýze typické a tato slova se označují jako “stopwords” a jsou zpravidla a textové analýzy vyřazena. Další měrou centrální tendence je median. Medián nám značí prostřední hodnotu nějaké proměnné. Můžeme si jeho výpočet představit tak, že hodnoty proměnné seřadíme od nejmenší po největší a vyberete hodnotu, která bude přesně uprostřed. Tato hodnota je medián. Matematicky se medián u proměnné \\(x_i\\) vypočítá jako \\[median(x) = x_{(n + 1)/2}\\] Pokud má naše proměnná sudý počet čísel, vypočítá se medián zpravidla jako průměr dvou prostředních hodnot, tedy \\[median(x) = \\frac{x_{n/2} + x_{n/2+1}}{2}\\] Řekněme, že bychom chtěli vědět medián počtu slov ve větě. Nejdříve si musíme pro každou větu (prvek listu prezident_clean) vypočítat počet slov. pocet_slov &lt;- sapply(prezident_clean, length) Velmi dobrým zvykem je si rozložení hodnot proměnné zobrazit graficky. Grafické zobrzení nám vždy poví nejenom o typické hodnotě proměnné, ale i o tom, jak se hodnoty liší (viz Míry rozptýlenosti @ref(#desc-vars)). Pokud máme číselnou proměnnou je nejčastějším způsobem, jak zobrazit hodnoty proměnné histogram. Histogram je: …je to grafické znázornění distribuce dat pomocí sloupcového grafu se sloupci stejné šířky, vyjadřující šířku intervalů (tříd), přičemž výška sloupců vyjadřuje četnost sledované veličiny v daném intervalu. Zdroj: Wikipedia Můžeme ho vytvořit pomocí funkce hist. Vidíme, že nejvíce nejčastěji věta obsahuje mezi 5 až 25 slovy, ale některé věty obsahují i více než 35 slov. hist(pocet_slov, col = &quot;#1f77b4&quot;, xlab = &quot;Počet slov ve větě&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram počtu slov ve větě&quot;) Graf 2.1: Příklad histogramu na číselné (kardinální) proměnné Funkce hist automaticky zvolí vhodné intervaly pro sloupce v grafu. Pokud byste je chtěli změnit, můžete tak udělat pomocí argumentu breaks. hist(pocet_slov, col = &quot;#1f77b4&quot;, xlab = &quot;Počet slov ve větě&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram počtu slov ve větě&quot;, breaks = 15) n &lt;- length(pocet_slov) # protoze mame sudy pocet slov vypocitame prumer dvou prostrednich hodnot i1 &lt;- n / 2 i2 &lt;- i1 + 1 .median &lt;- mean(pocet_slov[c(i1, i2)]) # nebo pomoci funkce median(pocet_slov) Medián můžeme také vypočítat pomocí funkce median. Mediánový počet slov ve větě je tedy 15.5. Pojďme si výpočet zobrazit graficky a seřadit si počet slov od nejmenšího po největší a zobrazit si medián na grafu. plot(1:n, sort(pocet_slov), col = &quot;#1f77b4&quot;, ylab = &quot;Počet slov ve větě&quot;, xlab = &quot;Pořadí&quot;, main = &quot;Počet slov ve větě seřazený podle velikosti&quot;, type = &quot;h&quot;, lwd = 5 ) # pridame prostredni hodnoty lines(i1, pocet_slov[i1], col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 5) lines(i2, pocet_slov[i2], col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 5) # pridame median lines(mean(c(i1, i2)), .median, col = &quot;red&quot;, type = &quot;h&quot;, lwd = 1, lty = 2) lines(c(-2, mean(c(i1, i2))), c(.median, .median), col = &quot;red&quot;, type = &quot;l&quot;, lwd = 1, lty = 2) legend(&quot;topleft&quot;, col = c(&quot;grey&quot;, &quot;red&quot;), lwd = c(4, 4), legend = c(&quot;Prostřední hodnoty&quot;, &quot;Median&quot;), cex = 0.7 ) Graf 2.2: Seřazení proměnné pro ilustraci výpočtu mediánu Asi nejčastější mírou centrální tendence, která se používá je průměr. Průměr je možné vypočítat pouze pro kardinální proměnné. Technicky kardinální proměnné rozlišujeme na diskrétní a spojitou. Diskrétní nabývá celých čísel (1,2,3,4 etc., například počet dětí), tedy \\(\\in Z\\). Spojitá proměnná pak teoreticky nebývá nekonečně mnoho hodnot, prakticky je ale omezena tím, jak přesně dokážeme danou metriku měřit. Platí ale, že spojité proměnné nabývají racionálních čísel, tedy \\(\\in R\\). Průměr proměnné \\(x_i\\) vypočítáme jako \\[\\overline{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\] Pojďme tedy vypočítat průměrný počet slov ve větě. V R můžeme použít funkci mean. sum(pocet_slov) / length(pocet_slov) ## [1] 16.56061 Někdy nechceme všem pozorováním při výpočtu průměru dát stejnou váhu. V takovém případě vypočítáme vážený průměr. Jeho vzorec je \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} w_ix_i}{\\sum_{i=1}^{n}w_i}\\] Řekněme například, že bychom průměrný počet slov chtěli vážit pozicí v textu a dát slovům v první větě menší váhu, než slovům, které se objevily později v textu. w &lt;- 1:length(pocet_slov) sum(w * pocet_slov) / sum(w) ## [1] 16.21257 Důležitý rozdíl mezi mediánem a průměr nastává pokud nejsou data symetricky rozdělena. Symetricky rozdělená data jsou taková, která mají podobný počet hodnot nalevo a napravo od průměru. V našem případě rozložení počtu slov ve větě je medián menší než průměr. To je typické pro rozložení, která mají delší konec napravo od průměru. Průměr je totiž náchylný na extrémní pozorování. Medián je založený na pořadí, takže ho extrémní pozorování tolik neovlivní. Většína proměnných, která je ohraničená zleva (nemůže mít menší hodnotu než nějaká hranice) má asymetrické rozložení s více hodnotami napravo od průměru, např. příjem. V takových případech může být medián lepší mírou centrální tendence, ale výběr bude vždy záležet na otázce, kterou daty chceme zodpovědět. I pomocí mír centrální tendence se dají dělat zajívé analýzy. Řekněme, že nás zajímá rozdíl v relativní četnosti slov v projevu prezidenta a premiéra. # nacteme text projevu premiera premier &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/vlada.txt&quot;) premier_clean &lt;- vycistit_text(premier) # extrahujeme slova z vet do vektoru slova_premier &lt;- unlist(premier_clean) # nacteme a odstranime stopwords stopwords &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/stopwords.txt&quot;, encoding = &quot;UTF-8&quot;) # vybereme pouze slova, ktera nejsou ve stopwords slova_premier &lt;- slova_premier[!slova_premier %in% stopwords] slova &lt;- slova[!slova %in% stopwords] # vypocitame absolutni cetnost slov cetnost_premier &lt;- table(slova_premier) cetnost_prezident &lt;- table(slova) # vypocitame relativni cetnost cetnost_premier &lt;- cetnost_premier / sum(cetnost_premier) cetnost_prezident &lt;- cetnost_prezident / sum(cetnost_prezident) # vypocitame relativni cetnost # ted slouzime obe tabulky rel_cetnost &lt;- merge(data.frame(cetnost_premier), data.frame(cetnost_prezident), by.x = &quot;slova_premier&quot;, by.y = &quot;slova&quot;) # prejmenuje sloupce, aby davaly vice smysl colnames(rel_cetnost) &lt;- c(&quot;slova&quot;, &quot;premier&quot;, &quot;prezident&quot;) head(rel_cetnost) ## slova premier prezident ## 1 alespoň 0.001686341 0.001547988 ## 2 bezpečnostní 0.001686341 0.001547988 ## 3 bojovat 0.001686341 0.001547988 ## 4 ceny 0.003372681 0.001547988 ## 5 české 0.006745363 0.001547988 ## 6 cesta 0.005059022 0.003095975 # vybereme nejcastejsich 20 slov pro každého top_premier &lt;- order(rel_cetnost$premier, decreasing = TRUE)[1:20] top_prezident &lt;-order(rel_cetnost$prezident, decreasing = TRUE)[1:20] # vyfiltrujeme top_index &lt;- unique(c(top_premier, top_prezident)) rel_cetnost_top &lt;- rel_cetnost[top_index, ] x &lt;- 1:nrow(rel_cetnost_top) plot(x, rel_cetnost_top$premier, type = &quot;h&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;Relativní četnost&quot;, col = &quot;black&quot;, lwd = 3) lines(x+0.3, rel_cetnost_top$prezident, type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 3) axis(1, at=x, labels=rel_cetnost_top$slova, las = 3, cex.axis = 0.8) legend(&quot;topright&quot;, legend = c(&quot;Premiér&quot;, &quot;Prezident&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(3,3), cex = 0.8) Graf 2.3: Top 20 slov podle relativní četnosti v projevu prezidenta a premiéra 2.2 Míry rozptýlenosti Míry centrální tendence nám udávájí hodnotu typického pozorování proměnné. Míry rozptýlenosti nám říkají, jak jsou hodnoty rozptýleny daleko od nějaké typické hodnoty. Pro kardinální proměnné se nejčastěji používá rozptyl, který vypočítáme tak, že každou hodnotu proměnné odečteme od průměru a umocníme. Tyto hodnoty sečteme a vydělíme počtem pozorování. Matematicky bychom rozptyl \\(\\sigma^2\\) proměnné \\(x\\) vypočítali jako \\[\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{x})^2\\] Ve statistice se také používá pro výpočet rozptýlenosti směrodatná odchylka \\(\\sigma\\), které se vypočítá jako \\[\\sigma = \\sqrt{\\sigma^2}\\] Ukažme si příklad na počtu slov. n &lt;- length(pocet_slov) rozptyl &lt;- sum((pocet_slov - mean(pocet_slov))^2) / n # nebo v R pomoci var smerodatna_odchylka &lt;- sqrt(rozptyl) # nebo v R pomoci sd Rozptyl počtu slov ve větě je tedy 93.6705693 a směrodatná odchylka je 9.6783557. Ukažme si princip rozptylu/směrodatné odchylky na imaginárních datech. Na ukázku si vytvoříme proměnnou, která má 10 pozorování a zobrazíme je do grafu jako body. Červená čára označuje průměr těchto bodů. Horizontální čáry potom označují vzdálenost každého pozorování od průměrné hodnoty. Nejdříve si ukážeme příklad s menším rozptylem hodnot a pod ním příklad rozložení s větším rozptylem hodnot. Protože mají oba příklady stejný počet pozorování (10), můžete si rozdíl v jejich směrodatné odhylce představit jako rozdíl vertikálních úseček, které vedou od průměru. par(mfrow = c(2, 1)) x &lt;- rnorm(1e4, mean = 5, sd = 1) prumer &lt;- mean(x) n &lt;- seq(1, 10) smerodatna_odchylka &lt;- sd(x) # prvni graf plot(x[n], n, main = paste0(&quot;Směrodatná odchylka: &quot;, round(smerodatna_odchylka, 2)), xlim = c(0,10), xlab = &quot;&quot;, ylab = &quot;Číslo pozorovaní&quot;, pch = 19, col = &quot;#1f77b4&quot;) abline(v = prumer, col = &quot;black&quot;, lwd = 2) for(i in n) { lines(c(prumer, x[i]), c(i,i), col = &quot;#1f77b4&quot;, lwd = 2) } legend(&quot;topright&quot;, legend = c(&quot;Průměr&quot;, &quot;Vzdal. od průměru&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2,2), cex = 0.7) # druhy graf x2 &lt;- rnorm(1e4, mean = 5, sd = 2.5) prumer &lt;- mean(x2) smerodatna_odchylka &lt;- sd(x2) plot(x2[n], n, main = paste0(&quot;Směrodatná odchylka: &quot;, round(smerodatna_odchylka, 2)), xlim = c(0,10), xlab = &quot;&quot;, ylab = &quot;Číslo pozorovaní&quot;, pch = 19, col = &quot;#1f77b4&quot;) abline(v = prumer, col = &quot;black&quot;, lwd = 2) for(i in n) { lines(c(prumer, x2[i]), c(i,i), col = &quot;#1f77b4&quot;,lwd = 2) } legend(&quot;topright&quot;, legend = c(&quot;Průměr&quot;, &quot;Vzdal. od průměru&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2,2), cex = 0.7) Graf 2.4: Ukázka výpočtu směrodatné odchylky Pro úplnost, jak by vypadal histogram rozložení obou proměnných. V tomto případě jsme data generovali z normálního rozložení (viz kapitola Pravděpodobnostní rozložení). hist(x, col = adjustcolor(&quot;#1f77b4&quot;, 0.9), breaks = 20, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;, xlim = c(0,10)) hist(x2, col = adjustcolor(&quot;black&quot;, 0.4), breaks = 40, add = TRUE) Graf 2.5: Histogram bodů z normálního rozložení o průměru 5 a směrodatné odchylce 1 (modrá) a 2.5 (černá) Poslední mírou rozptylu, kteru si ukážeme je kvantil. Kvantile je: je ve statistice čísla (hodnoty), která dělí soubor seřazených (například naměřených) hodnot na několik zhruba stejně velkých částí. Kvantil je tedy míra polohy rozdělení pravděpodobnosti náhodné veličiny. Zdroj: Wikipedia. Rozdělit data na stejný počet velkých částí lze různě. Proto jsou různé typy rozdělení různě nazvány: Medián Kvantil rozdělující statistický soubor na dvě stejně početné množiny se nazývá medián, tzn. jedná se o kvantil \\(Q_{0.5}\\). Tercil Dva tercily rozdělují statistický soubor na třetiny. 1/3 prvků má hodnoty menší nebo rovné hodnotě prvního tercilu \\(Q_{1/3}\\), 2/3 prvků mají hodnoty menší nebo rovné hodnotě tercilu druhého \\(Q_{2/3}\\). Kvartil Tři kvartily rozdělují statistický soubor na čtvrtiny. 25 % prvků má hodnoty menší než dolní kvartil \\(Q_{0.25}\\) a 75 % prvků hodnoty menší než horní kvartil \\(Q_{0.75}\\); někdy se označují \\(Q_1\\) a \\(Q_3\\). Kvintil Čtyři kvintily dělí statistický soubor na pět stejných dílů. 20 % prvků souboru má hodnoty menší (nebo rovné) hodnotě prvního kvintilu, 80 % hodnoty větší (nebo &gt;rovné). Decil Decil dělí statistický soubor na desetiny. Jako k-tý decil označujeme \\(Q_{10/k}\\). Percentil Percentil dělí statistický soubor na setiny. Jako k-tý percentil označujeme \\(Q_{100/k}\\). Používá se například při vyhodnocení testů: Pokud má účastník umístění na 85. percentilu, znamená to, že 85 % účastníků mělo horší výsledek (a 15 % účastníků je lepších nebo stejných jako on [včetně jeho samého]). Znamená to, že účastník s nejlepším umístěním nebude mít percentil 100 %, ale nižší (o část vyjadřující procento jeho vlastního &gt;podílu na výsledku). Percentil tak vypočteme: \\(PR = \\frac{CF - (0.5 * F)}{N} * 100\\). Kde PR je hodnota percentilu, CF je kumulativní počet výsledků a F je počet výskytů počítaného výsledku (percentilu). Zdroj: Wikipedia V R můžeme kvantily vypočítat pomocí funkce quantile. V argumentu prob stanovíme jakou pravděpodobnost (percentil) chceme vypočítat. Například vypočítejme kvartily počtu slov. kvartil &lt;- quantile(pocet_slov, prob = c(0.25, 0.5, 0.75)) kvartil ## 25% 50% 75% ## 9.00 15.50 22.75 Vidíme, že 25% vět má méně než 9 slov a že 25% vět má více než 23 slov (nebo že 75% vět má méně než 23 slov). Někdy se hodnoty kvantilů používají k výpočtu mezikvantilových rozpětí tak, že se honodty kvantilů odečtou mezi sebou. Například mazikvartilové rozpětí se vypočítá jako \\(Q_{0.75} - Q_{0.25}\\). 2.3 Cvičení Vytvořte funkci na výpočet percentilu. "],["pravděpodobnostní-rozložení.html", "Kapitola 3 Pravděpodobnostní rozložení 3.1 Pravděpodobnostní rozložení jako jazyk statistiky 3.2 Binomické 3.3 Normální 3.4 T 3.5 Uniformní 3.6 Poisson 3.7 Chi-kvadrát 3.8 Jak vybrat správný model pro data", " Kapitola 3 Pravděpodobnostní rozložení Tuto kapitolu začneme videm. Matemateca (IME USP) / name of the photographer when stated, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons Proč kuličky v tomto videu skončí v určitém množství v určitém sloupci? A proč skončí podobné množství kuliček v každém sloupci, pokud bychom kuličky pustili znova? Jak by se počet kuliček ve soupcích lišil? Pokud si z této knihy máte odnést jednu kapitolu, pak by to měla být tako :). Jak jsme zmínili v úvodu knihy, většina proměnných okolo nás se chová náhodně. To znamená, že může nabývat náhodných hodnot podle nějakého klíče (procesu). Náhodnost může vycházet z faktu, že měříme jenom nějakou část populace (např. výběrové šetření), z fyzikálních vlastností (např. váha součástky vyrobená v továrně nebude vždy stejná) nebo z chyby měření (např. teploměr nezměří stejnou teplotu vždy stejně, ale hodnoty měření budou kolísat okolo nějakého čísla). Pravděpodobnostní rozložení nám pomáhají kvantifikovat a predikovat míru nahodilosti. Typ pravděpodobnostního rozložení, který na popsání náhodnosti uplatníme vychází z našeho porumění vlastnostní rozložení a jeho vhodnosti na daný problém. A vlastnosti pravděpodobnostních rozložení je to, co si v této kapitole ukážeme. Na začátku jsme řekli, že budeme používat statistické modely a že tyto modely nejsou přesným vyjádřením reality, ale mohou být užitečným popsáním reality. V této kapitole začneme modely používat. 3.1 Pravděpodobnostní rozložení jako jazyk statistiky V této kapitole si ukážeme spoustu nových značení, která se stanou našim jazykem, kterým budeme ve statistice komunikovat. Budeme je používat k tomu, abychom popsali očekávané chování náhodné proměnné. Každé rozložení má svoje parametry, pomocí kterým ho můžeme popsat. Každé rozložení má taky svoje míry centrální tendence (očekávanou hodnotu) a svoje míry rozptýlenosti. To, kolik hodnot náhodné proměnné nabývá určitých hodnot popisuje hustota pravděpodobnosti. V angličtině se u spojitých proměnných (která nabývají reálných čísel) vyjadřuje hustota pravděpodobnosti jako probability density function (PDF). U diskrétních proměnných (nabývají celých čísel) se hustota pravděpodobnosti nazývá probability mass function (PMF). Integrál hustoty pravděpodobnosti je vždy rovný nule. To tedy znamá, že kdybychom sečetli všechny hodnoty hustoty pravděpodobnosti (pro spojité proměnné integrovali), tak by se výsledek rovnal jedné. To plyne z toho, že pravděpodobnost jevu nemůže být vyšší než jedna. Nakonec ještě trocha terminologie, které budeme používat. Náhodnou proměnnou budeme označovat jako \\(x_i\\). Budeme používat malá písmena, pokud výsledkem procesu bude vektor a velká písmena, pokud výsledkem bude matice. \\(i\\) označuje jednotlivá pozorování. Teda první hodnota náhodné proměnné, by se označovala jako \\(x_1\\). \\(n\\) označuje zpravidla počet pozorování proměnné. 3.2 Binomické Vraťme se k videu ze začátku kapitoly. Tomuto přístroji se říká Galton Box. Míčky jsou puštěny do přístroje z jednoho stejného bodu a procházejí několika vrstvami (ve videu 10 vrstvami). V každé vrstvě míček narazí na bod, který ho může poslat na levou nebo na pravou stranu (zhruba se stejnou pravděpodobností 0.5). To, na jakou stranu se míček vydá je určeno náhodou. Míčky potom spadnou do jednoho ze sloupců (ve videu 13 sloupců). Kdybysme nechali všechny míčky spadnout, posbírali je a znovu spustili, tak skoro stejné množství míčků skončí v každém sloupci. Jak je možné, že když spustíme tisíce míčků, každý se může 10x odrazit nalevo nebo napravo, tak výsledný počet míčků v každém sloupci je skoro stejný? Příčinou je pravděpodovnoství rozložení. Prvním rozložením, které si ukážeme je binomické rozložení. Jevy mohou být generovany procesem, který vede k binomickému rozložení, pokud máme pokus, jehož výsledkem je úspěch nebo neúspěch, pokusy jsou nezávislé a mají konstantní pravděpodobnost úspěchu \\(p\\) (konstantní pro všechny pokusy). Obecně platí, že proměnná \\(x_i\\) pochází z binomického rozložení, kde \\[x_i \\sim B(n, p)\\] a kde \\(n\\) značí počet pokusů a \\(p\\) pravděpodobnost úspěchu. Proměnná \\(x_i\\) je potom diskrétní. Pojďme si ukázat, jak se binomické rozložení vztahuje ke Galtonově boxu. Každý míček prochází 10 pokusy, kde výsledek může být buď úspěch (řekněme, že míček spadne napravo) nebo neúspěch (řeknemě, že míček spadne nalevo). Výsledkem je potom zařazení do jednoho ze 13 sloupců. V extrému může náš proces skončit tak, že všechny míčky budou nalevo nebo že všechny míčky budou napravo. Míčky jsou na sobě relativně nezávislé a všechny mají při každém pokusu relativně konstatní pravděpodobnost. Opět je nutné si uvědomit, že náš model dat (binomické rozložení) není přesnou reprezentací procesu, který se snažíme popsat. Míčky nejsou ve skutečnosti kompletně nezávislé, mohou do sebe narazit a tím se ovlivnit. Protože ale prochazí zhruba 10 pokusy (i když to se může lišit, některé míčky se mohou odrazit po nárazu znovu nahoru), bereme proces jako dostatečně náhodný a nezávislý. Důležité je, jestli jsou předpoklady našeho modelu splněny dostatečně na to, aby jeho matematické vyjádření bylo validní pro reálný process, který se snažíme popsat. Pojďme si nyní ukázat, jak se Galtonův box dá připodobnit binomickým rozložením. Náhodný process z binomického rozložení můžeme generovat pomocí funkce rbinom. Ta jako argumenty předpokládá n počet pozorování, která chceme generovat, size počet pokusů (způsobů, jak může proces skončit) a p pravděpodobnost úspěchu. Budeme uvaživat, že spustíme 10 000 míčků, tedy \\[x_i \\sim B(10000, 0.5)\\] set.seed(4) # pocet micku n &lt;- 10000 # pravdepodobnost uspechu (napravo) p &lt;- 0.5 # 0 by znamenalo všechny míčky nalevo, 12 všechny míčky napravo, # celkem 13 možností, jak mohou míčku skončit s &lt;- 12 vysledek &lt;- rbinom(n = n, size = s, prob = p) cetnost &lt;- table(vysledek) plot(cetnost, xlab = &quot;x&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;Galtonův box s &quot;, n, &quot; míčky&quot;), col = &quot;#1f77b4&quot;, type = &quot;h&quot;, lwd = 15, xlim = c(0, 12) ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) Graf 3.1: Simulace Galtonova boxu s 13 sloupci Očekávanou hodnotu proměnné pocházející z binomické proměnné můžeme vypočítat jako \\[E(x_i) = np\\] rozptyl jako \\[Var(x_i) = npq\\] a směrodatbou odchylku jako \\[Sd(x_i) = \\sqrt{Var(X_i)}\\]V našem případě, kdy máme 10^{4} míčků je tedy očekávaná hodnota 5000 a směrodatná odchylka 2500. A ještě ukázka, že počet míčků v každém sloupci je zhruba stejný, pokud bychom je pustili znova, řekněme 11x. N &lt;- 11 for (i in 1:N) { vysledek &lt;- rbinom(n = n, size = s, prob = p) micek &lt;- vysledek[1] cetnosti &lt;- table(vysledek) # pouzijeme funkci plot, abychom mohli pridat micek plot(cetnosti, xlab = &quot;x&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;Galtonův box s &quot;, n, &quot; míčky; pokus #&quot;, i), col = &quot;#1f77b4&quot;, lwd = 15, ylim = c(0, 2500), xlim = c(0, 12) ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) points(x = micek, y = 2, pch = 19, col = &quot;red&quot;) } Graf 3.2: Opakované spuštění míčků v Galtonově boxu Jak jsme zmínili v 3.1 frekvenci hodnot diskrétní náhodné proměnné můžeme popsat matematicky pomocí hustoty pravděpodobnosti (PMF). U binomického rozložení můžeme PMF vypočítat jako \\[PMF(x_i) = \\binom{n}{k}p^kq^{n-k}\\] kde \\(k\\) je počet úspěšných pokusů a \\(q=1-p\\). V praxi můžeme využít funkce dbinom, která PMF vypočítá. Tato funkce argument x kam dosadíme hodnoty proměnné \\(x_i\\), pro které chceme PMF vypočítat. Argumenty size a prob jsou stejné jako u rbinom. Protože PMF je vypočítáno deterministicky (není tam žádná náhoda jako u generování náhodných čísel nahoře, čísla pouze dosadíme do vzorce), bude jeho výpočet stejný pokaždé, když zadáme stejné argumenty funkce. # muze padnou minimalne 0 (uplne nalevo) a maximalne 12 (uplne napravo) x &lt;- 0:12 s &lt;- 12 p &lt;- 0.5 pmf &lt;- dbinom(x = x, size = s, prob = p) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;Galtonův box&quot;), col = &quot;#1f77b4&quot; ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) Graf 3.3: PMF pro Galtonův box s 13 sloupci Protože hodnoty \\(x_i\\) mohou nabývat pouze celých čísel, je PMF vyjádřením pravděpodobnosti toho, že náhodná proměnná \\(x_i\\) nabyde nějaké hodnoty. Nejpravděpodobněji skončí míček ve sloupci 6 s pravděpodobností 7, nebo matematickým zápisem \\(P(x_i=\\) 6 \\()=\\) 0.2255859. Pravěpodobnost, že míček skončí v 4 sloupci zleva je \\(P(x_i = 2)\\)=0.0537109. To, že dokážeme predikovat, kolik míčků skončí v jakém sloupci, ještě neznamená, že víme, kde skončí jeden konkrétní míček. V animaci nahoře si všimněte červené tečky. Ta reprezentuje první míček, který jsme vhodili. Jak je vidět míček cestuje mezi sloupci. To s jakou pravděpobností skončí v daném sloupci nám říká právě PMF. Stejně tak můžeme spočítat pravděpodobnost, že skončí v 7 sloupci a více napravo, tedy \\(P(x \\ge 7)\\)=0.387207. Tomuto typu úlohy, kdy nás zajímá, zda náhodná proměnná \\(x_i\\) bude mít hodnotu menší/větší než nějaká hodnota \\(q\\), říkáme kumulativní pravděpodobnosti (anglicky cumulative distribution function CDF). Matematicky bychom tento typ úlohy označili jako \\(P(x_i &lt; q)\\) pokud by nás zajímala pravděpodobnost, že náhodné proměnné \\(x_i\\) bude menší než nějaká hodnota \\(q\\) a \\(P(x_i &gt; q)\\), pokud by nás zajímala pravděpodobnost, že hodnota náhodné proměnné \\(x_i\\) bude větší než nějaká hodnota \\(q\\). R můžeme na výpočet CDF u binomického rozložení použít funkci pbinom. Jako argumenty očekává tato funkce q hodnota náhodné proměnné vůči které porovnáváme, size a prob mají stejný význam jako u funkcí nahoře a argument lower.tail vyjadřující, zda chceme zjistit pravděpodobnost, že \\(x_i\\) bude menší nebo větší a rovno než \\(q\\). Pokud má argument lower.tail hodnotu TRUE, pak počítáme \\(P(x_i \\le q)\\), pokud má hodnotu FALSE, pak počítáme \\(P(x_i &gt; q)\\). Pojďme si ukázat různé výpočty a jejich zobrazení na grafu. Začneme \\(P(x &gt; 7)\\). # P(x &gt; 7) p_x7 &lt;- pbinom(q = 7, size = s, prob = p, lower.tail = FALSE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &gt; 7)=&quot;, round(p_x7, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &gt; 7], pmf[x &gt; 7], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.4: Ukázka výpočtu P(x &gt; 7) pomocí PMF Dále si ukažme jak vypočítat \\(P(x \\le 5)\\). # P(x &lt;= 5) p_x5 &lt;- pbinom(q = 5, size = s, prob = p, lower.tail = TRUE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &lt;= 5)=&quot;, round(p_x5, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &lt;= 5], pmf[x &lt;= 5], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.5: Ukázka výpočtu P(x &lt;= 5) pomocí PMF A nakonec \\(P(x_i \\ge 6)\\). # P(x &gt;= 6) # pouzijeme FALSE na vypocet upper tail a za 1 dosadime 5, protoze... # ...lower.tail = FALSE pocita P(x_i &gt; q) p_x6 &lt;- pbinom(q = 5, size = s, prob = p, lower.tail = FALSE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &gt;= 6)=&quot;, round(p_x6, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &gt;= 6], pmf[x &gt;= 6], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.6: Ukázka výpočtu P(x &gt;= 6) pomocí PMF 3.3 Normální Normální rozložení popisuje náhodný proces spojitých proměnných. Za určitých podmínek (velký výběr a \\(p\\) neblížící se 0 nebo 1) se binomické rozdělení bude podobat normálnímu. Normální rozložení popisuje proces, kterým jsou generované náhodné spojité proměnné, které nejsou ohraničené zleva ani zprava. Jednotlivá pozorování jsou potom opět na sobě nezávislá. Protože mnoho fyzikálních jevů je spojitých1. a nejsou ohraničeny2, normální rozložení se často vyskuteje v přírodě. Normální rozložení má dva parametry, které určují jeho tvar, průměr \\(\\mu\\) a směrodatnou odchylku \\(\\sigma\\). Náhodná proměnná \\(x_i\\) pocházející z normálního rozložení se zapíše jako \\[x_i \\sim N(\\mu, \\sigma)\\]Parameter \\(\\mu\\) ovlivňuje polohu rozložení a parameter \\(\\sigma\\) jeho roztaženost nalevo a napravo od \\(\\mu\\). Protože náhodná proměnná \\(x_i\\) je spojitá, nabývá reálných čísel, může mít nekonečně mnoho hodnot. To představuje problém, pokud chceme vypočítat hustotu pravděpodobnosti (PDF). Aby byly zachovány vlastnosti pravděpodobnosti (\\(p \\in [0, 1]\\)), je integrál PDF rovný jedné To ale znamená, že hodnota PDF už nevyjadřuje pravděpodobnost nějakého jevu. V angličtině se proto používá pojem density (odtud probability density function PDF). Vzorec pro PDF je \\[PDF(x_i) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\] v R můžeme pro výpočet PDF normálného rozložení použít funkci dnorm, které má argumenty x hodnoty náhodné proměnné \\(x_i\\), pro které chceme PDF vypočítat, mean průměr a sd standardní odchylku. Pojďme si ukázat, jak by vypadala PDF pro tyto náhodné proměnné: \\[x_i \\sim N(0, 1)\\] \\[y_i \\sim N(3, 2)\\] \\[z_i \\sim N(-2, 3)\\] # definujeme si parametry mu &lt;- c(0, 3, -2) s &lt;- c(1, 2, 3) # definuje hodnoty nahodne promenne, pro kterou budeme chtit vypocitat pdf x &lt;- seq(-12, 12, length.out = 1000) # vypocitame pdf pdf_x &lt;- dnorm(x = x, mean = mu[1], sd = s[1]) pdf_y &lt;- dnorm(x = x, mean = mu[2], sd = s[2]) pdf_z &lt;- dnorm(x = x, mean = mu[3], sd = s[3]) # zobrazime plot(x, pdf_x, col = &quot;#1f77b4&quot;, lwd = 2, type = &quot;l&quot;, xlab = &quot;&quot;, ylab = &quot;PDF&quot;, xlim = c(-12, 12) ) lines(x, pdf_y, col = &quot;black&quot;, lwd = 2) lines(x, pdf_z, col = &quot;orange&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;x ~ N(0, 1)&quot;, &quot;y ~ N(3, 2)&quot;, &quot;z ~ N(-2, 3)&quot;), lwd = rep(2, 3), col = c(&quot;#1f77b4&quot;, &quot;black&quot;, &quot;orange&quot;), cex = 0.7 ) Graf 3.7: Ukázka vlivu průměru a směrodatné odchylky na tvar normálního rozložení Normální rozložení je symetrické. To znamená, že se nachází stejně hodnot nalevo a napravo od průměru. Podle hodnoty směrodatné odchylky dokážeme určit kolik případů bychom a jak daleko bychom očekávali, že budou ležet od průměru. Následující obrázek ukazuje, že zhruba 68% hodnot náhodné proměnné bude ležet +/- jednu směrodatnou odchylku od průměru a zhruba 95% hodnot bude ležet +/- dvě směrodatnou odchylku od průměru a 99% hodnot +/- tři směrodatné odchylky od průměru. Graf 3.8: M. W. Toews, CC BY 2.5 https://creativecommons.org/licenses/by/2.5, via Wikimedia Commons Očekávanou hodnotou normálního rozložení je průměr, tedy \\(E(x_i)=\\mu\\) a rozptylem je \\(var(x_i) = \\sigma^2\\), tedy směrodatnou odchylkou je \\(\\sigma = \\sqrt{\\sigma^2}\\). Pojďme se nyní podívat, co se stane s funkcí normálního rozložení, pokud k náhodné proměnné \\(x_i\\) přičteme skalár. Podíváme se na následující proměnnou \\(h_i \\sim N(178, 8)\\), tedy proměnnou, která pochází z normálního rozložení s průměrem 178 a směrodatnou odchylkou 8. K této náhodné proměnné přičteme 20, tedy \\(h&#39;_i = h_i + 20\\) a spočítáme průměr a směrodatnou odchylku. Ke generování náhodných čísel z normálního rozložení použijeme funkci rnorm, který má argumenty n počet pozorování, které chceme generovat, mean průměr rozložení, z kterého chceme generovat a sd směrodatnou odchylku rozložení, z kterého chceme generovat. n &lt;- 1e5 h &lt;- rnorm(n, mean = 178, sd = 8) h. &lt;- h + 20 par(mfrow = c(2, 1)) hist(h, col = &quot;#1f77b4&quot;, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, xlim = c(130, 240), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) hist(h., col = &quot;#1f77b4&quot;, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, xlim = c(130, 240), main = paste0( &quot;h&#39;~N(&quot;, round(mean(h.), 0), &quot;, &quot;, round(sd(h.), 0), &quot;)&quot; ) ) Graf 3.9: Efekt přičtení skalaru na rozložení proměnné Jak je vidět přičtení (nebo odečtění) má vliv pouze na průměr a tedy polohu rozložení. V případě přičtení čísla \\(a\\) se celé rozložení posune doprava o \\(a\\), v případě odečtení se potom posune celé rozložení doleva. Přičtení/odečtení nemá vliv na směrodatnou odchylku, tedy na šířku rozložení od průměru. Tento fakt můžeme zapsat jako \\[h&#39;_i \\sim N(\\mu_h \\pm a, \\sigma_h)\\] V dalších několika řádcích si představíme některé základní početní operace s náhodnými proměnnými pocházejícími z normálního rozložení. Začneme tím, co se stane s rozložením stejné náhodné proměnné \\(h_i \\sim N(178, 8)\\) pokud ji vynásobíme skalarem. Náhodnou proměnnou \\(h_i\\) vynásobíme číslem 0.66, tedy \\(h&#39;_i = h_i * 0.66\\) a spočítáme průměr a směrodatnou odchylku. Jak je vidět vynásobení (nebo jako v našem případě dělení) má vliv na průměr i směrodatnou odchylku. Oba parametry normálního rozložení se změní o faktor \\(a\\) , kterým původní náhodnou proměnnou násobíme, tedy v našem případě se zmenší o třetinu. Tento fakt můžeme zapsat jako \\[h&#39;_i \\sim N(\\mu_h * a, \\sigma_h * a)\\] h. &lt;- h * 2 / 3 par(mfrow = c(2, 1)) hist(h, col = &quot;#1f77b4&quot;, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, xlim = c(80, 230), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) hist(h., col = &quot;#1f77b4&quot;, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, xlim = c(80, 230), main = paste0( &quot;h&#39;~N(&quot;, round(mean(h.), 0), &quot;, &quot;, round(sd(h.), 0), &quot;)&quot; ) ) Graf 3.10: Efekt vynásobení skalarem na rozložení proměnné Dále se podíváme, co se stane s průměrem a směrodatnou odchylkou, pokud k sobě přičteme dvě náhodné proměnné pocházející z normálního rozložení. Budeme mít dvě náhodné proměnné, které pocházejí z normálního rozložení \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(80, 10)\\). Budeme sledovat, co se stane s průměrem a směrodatnou odchylkou nové proměnné \\(k_i\\), která vznikne sečtením \\(h_i\\) a \\(l_i\\), tedy \\(k_i = h_i + l_i\\). Jak vidíme na grafu dole, nová proměnná má průměr rovný \\(\\mu_k = \\mu_h + \\mu_l\\) a \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\), tedy \\[k_i \\sim N(\\mu_h + \\mu_l, \\sqrt{\\sigma_h^2 + \\sigma_l^2})\\] l &lt;- rnorm(n = n, mean = 80, sd = 10) k &lt;- h + l par(mfrow = c(3, 1)) hist(h, col = &quot;#1f77b4&quot;, xlab = &quot;h&quot;, ylab = &quot;Četnost&quot;, xlim = c(20, 330), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) hist(l, col = &quot;#1f77b4&quot;, xlab = &quot;l&quot;, ylab = &quot;Četnost&quot;, xlim = c(20, 330), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) hist(k, col = &quot;#1f77b4&quot;, xlab = &quot;k&quot;, ylab = &quot;Četnost&quot;, xlim = c(20, 330), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.11: Efekt přičtení dvou normálně rozložených náhodných proměnných na rozložení nové proměnné Jako další výpočetní operaci náhodných proměnných pocházejících z normálního rozložení si ukážeme, co se stane pokud od sebe odečteme dvě náhodné proměnné pocházející z normálního rozložení. Budeme opět uvažovat náhodné proměnné \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(80, 10)\\) a budeme počítat \\(k_i = h_i - l_i\\). Jak je vidět z grafu dole, nový průměr má hodnotu \\(\\mu_k = \\mu_h - \\mu_l\\), ale směrodatná odchylka je stále rovna \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\). To je důležitý poznatek o odečtu dvou normálně rozložených proměnných. Směrodatná odchylka nové proměnné tedy bude vždy součtem směrodatných odchylek původních proměnných, protože rozptyl v datech se sčítá (jinak bychom mohli dosáhnout negativní \\(\\sigma\\), což není definováno, protože \\(\\sigma \\in [0, \\infty)\\). Tento jev se dá zapsat jako \\[k_i \\sim N(\\mu_h - \\mu_l, \\sqrt{\\sigma_h^2 + \\sigma_l^2})\\] k &lt;- h - l par(mfrow = c(3, 1)) hist(h, col = &quot;#1f77b4&quot;, xlab = &quot;h&quot;, ylab = &quot;Četnost&quot;, xlim = c(20, 330), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) hist(l, col = &quot;#1f77b4&quot;, xlab = &quot;l&quot;, ylab = &quot;Četnost&quot;, xlim = c(20, 330), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) hist(k, col = &quot;#1f77b4&quot;, xlab = &quot;k&quot;, ylab = &quot;Četnost&quot;, xlim = c(20, 330), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.12: Efekt odečtení dvou normálně rozložených náhodných proměnných na rozložení nové proměnné Poslední výpočetní operaci, kterou si ukážeme náhodnou proměnnou standardizuje tak, že její průměr bude rovný nule a směrodatná odchylka bude rovná jedné. Tento typ transformace je možné provést s proměnnou pocházející z jakéhokoliv rozložení, ale my si ho zmiňujeme v kontextu normálního rozložení, protože zde jej budem vídat nejčastěji. Tuto transformaci lze provést následujícím způsobem: \\[z_i = \\frac{(x_i - \\mu_x)}{sd(x_i)}\\]přičemž tedy platí, že pokud pochází \\(x_i\\) z normálního rozložení, tak \\(z_i \\sim N(0, 1)\\). Pojďme si ukázat, jak by tato transofrmace vypadala pro náhodnou proměnnou \\(x_i \\sim N(178, 8)\\). Vzhledem k vlastnostem normálního rozložení je tedy pro standardizovanou náhodnou proměnnou pocházející z normálního rozložení snadné vypočítat kvantily (viz 3.8): \\(Q_{0.023} = -2\\) \\(Q_{0.159} = -1\\) \\(Q_{0.5} = 0\\) \\(Q_{0.841} = 1\\) \\(Q_{0.977} = 2\\) z &lt;- (h - mean(h)) / sd(h) par(mfrow = c(2, 1)) hist(h, col = &quot;#1f77b4&quot;, xlab = &quot;h&quot;, ylab = &quot;Četnost&quot;, # urcime intrval od - 4 sd do + 4 sd xlim = c(178-4*8, 178+4*8), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) hist(z, col = &quot;#1f77b4&quot;, xlab = &quot;l&quot;, ylab = &quot;Četnost&quot;, # urcime intrval od - 4 sd do + 4 sd xlim = c(-4, 4), main = paste0( &quot;z~N(&quot;, round(mean(z), 0), &quot;, &quot;, round(sd(z), 0), &quot;)&quot; ) ) Graf 3.13: Standardizovaná proměnná pocházející z normálního rozložení Jak jsme zmínili, protože spojitá proměnná může nabývat nekonečně mnoho hodnot, nevyjadřuje PDF v nějakém konkrétním bodě pravděpodobnost tak, jako tomu je u PMF (např. u binomického rozložení). Platí ale, že integrál PDF je rovný jedné. To znamená, že můžeme použít kumulativní pravděpodobnost (CDF) k výpočtu \\(P(x_i &gt; q)\\) nebo \\(P(x_i \\le q)\\). Funkce na výpočet CDF se jmenuje pnorm a stejně jako pbinom má argument q, který vyjadřuje hodnotu náhodné proměnné vůči které porovnáváme, parametr lower.tail, který opět vyjadřuje pro jaký konec rozložení chceme pravděpodobnost vypočítat (TRUE \\(P(x_i \\le q)\\), FALSE \\(P(x_i &gt; q)\\)) a potom parametry normálního rozložení, tedy mean a sd, které udávají tvar rozložení a mají stejný význam jako ve funkci rnorm a dnorm. Vraťme se zpátky k náhodné proměnné \\(h_i \\sim N(178, 8)\\). Ta představuje rozložení výšky mužů v dospělé populaci. Řekněme, že by nás zajímalo, jaká je pravděpodobnost, že muž pocházející z této populace bude 180cm a vyšší, tedy \\(P(x_i \\ge 185)\\). Pro úplnost si opět tuto pravděpodobnost3 zobrazíme v grafu. x &lt;- seq(145, 210, length.out = 1000) q &lt;- 185 # vypoceteme si PDF, abychom ji mohli zobrazit v grafu pdf &lt;- dnorm(x = x, mean = 178, sd = 8) cdf &lt;- pnorm(q = q, mean = 178, sd = 8, lower.tail = FALSE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;h&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &gt;= &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &gt;= q], pdf[x &gt;= q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.14: Ukázka výpočtu P(x &gt;= 185) pomocí PDF Pro úplnost, ještě vypočítáme příklad kdy nás zajímá pravděpodobnost, že muž pocházející z této populace bude menší než 164cm, tedy \\(P(x_i &lt; 164)\\). q &lt;- 164 cdf &lt;- pnorm(q = q, mean = 178, sd = 8, lower.tail = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;h&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &lt; &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &lt; q], pdf[x &lt; q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.15: Ukázka výpočtu P(x &lt; 164) pomocí PDF 3.4 T Dalším z rozložení, které si představíme je tzv. t-rozložení, nebo také někdy nazývané studentovo. Toto rozložení vychází z normálního rozložení, ale má větší varibilitu. PDF normálního rozložení rapidně klesá se vzdáleností od průměru (viz 3.8). Pokud potřebujeme popsat jev, který má více hodnot vzdálených od průměru, než bychom čekali u normálního rozložení, je toho možné dosáhnout pomocí t-rozložení. T-rozložení je automaticky standardizované na \\(\\mu = 0\\) a má pouze jeden paramater - stupně volnosti (degrees of freedom) \\(\\nu\\), který určuje jak pravděpodobné jsou hodnoty více vzdálené od průměru. Čím je \\(\\nu\\) menší, tím je rozložení více široké a naopak. Pokud je \\(\\nu\\) větší než 50, je t-rozdělení skoro totožné se standardizovaným normálním rozložením \\(N(1, 0)\\). Pojďme si nyní ukázat PDF při různých stupních volnosti \\(\\nu\\) a jejich podobnost k \\(N(1, 0)\\). Ukážeme si \\(T(2)\\), \\(T(4)\\), \\(T(8)\\), \\(T(16)\\), \\(T(34)\\) a \\(T(64)\\). Jak je vidět z grafu 3.16, s rostoucím počtem stupňů volnosti se t-rozložení přibližuje \\(N(1,0)\\), \\(T(64)\\) už skoro překrývá křivku PDF pro \\(N(1,0)\\) # stupne volnost sv &lt;- 2^c(1:6) # x pro ktere pocitame pdf x &lt;- seq(-5, 5, length.out = 1000) pdf_n &lt;- dnorm(x, mean = 0, sd = 1) pdf_t &lt;- lapply(sv, dt, x = x) # vybereme barvy pouzijeme nejsvetlejsi pro vetsi sv... # ...proto funkce rev, ktera obrati hodnoty cols &lt;- rev(RColorBrewer::brewer.pal(n = length(sv), name = &quot;Blues&quot;)) # zobrazime pdf N(1,0) plot(x, pdf_n, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, type= &quot;l&quot;, lwd = 3, col = &quot;black&quot;) # zobrazime pdf vsech T for(i in 1:length(sv)) { lines(x, pdf_t[[i]], type = &quot;l&quot;, lwd = 2, col = cols[i]) } legend(&quot;topright&quot;, legend = c(&quot;N(1,0)&quot;, paste0(&quot;T(&quot;, sv, &quot;)&quot;)), col = c(&quot;black&quot;, cols), lwd = rep(2, length(sv) + 1), cex = 0.7 ) Graf 3.16: Podobnost t-rozdělení k N(1,0) K výpočtu PDF t-rozdělení jsme použili funkci dt, která má argument x hodnoty proměnné x, pro které chci PDF vypočítat a df, počet stupňů volnosti. Pokud bychom chtěli generovat náhodnou proměnnou z t-rozložení, můžeme tak učinit pomocí funkce rt. Stejně tak CDF t-rozložení můžeme vypočítat pomocí funkce pt. Očekávaná hodnota t-rozdělení je 0, tedy \\(E(x_i) = 0\\) a rozptyl je roven \\(var(x_i) = \\frac{\\nu}{\\nu-2}\\). R obsahuje pouze tradiční t-rozdělení, které je definováno pouze stupni volnosti \\(\\nu\\). Pokud bychom chtěli pomocí t-rozdělení popsat rozložení, která mají jiný průměr a rozptyl, museli bychom t-rozdělení zobecnit. Pokud bychom tedy chtěli stanovit t-rozdělení s průměrem \\(\\mu\\) a směrodatnou odchylkou \\(\\sigma\\) pro náhodnou proměnnou \\(x_i\\), pak bude platit \\[x_i = \\mu + \\sigma T\\]Očekávaná hodnota náhodné proměnné pocházející z zobecněného t-rozdělení je \\(E(x_i) = \\mu\\) a rozptyl \\(var(x_i) = \\sigma^2 \\frac{\\nu}{\\nu-2}\\). PDF tohoto rozdělení bychom vypočítali jako: \\[PDF(x_i) = \\frac{1}{\\sigma} PDF_T(\\frac{x_i - \\mu}{\\sigma}, \\nu)\\]Všimněme si, že vlastně počítáme PDF t-rozdělení pro standardizovanou proměnnou a toto PDF násobíme faktorem \\(\\frac{1}{\\sigma}\\). Na grafu 3.17 ukazuje příklad PDF zobecněného t-rozdělení pro rozložení s průměrem 178, směrodatnou odchylkou 8 a 5 stupni volnosti. Náhodnou proměnnou \\(x_i\\) pocházející z tohoto rozložení bychom mohli zapsat jako \\(x_i \\sim T(178, 8, 5)\\). Jak je vidět menší stupně volnosti znamenají, že hodnoty více vzdálené od průměru jsou u takové náhodné proměnné více pravděpodobné, než u normálního rozložení. # definujeme si parametry rozlozeni mu &lt;- 178 s &lt;- 8 sv &lt;- 5 # vytvorime x pro ktere chceme vypocitat pdf x &lt;- seq(178-5*s, 178+5*s, length.out = 1000) # vypocteme pdf N(178,8) pro porovnani pdf_n &lt;- dnorm(x, mean = mu, sd = s) # vypocteme pdf T(178,8,5) pdf_t &lt;- dt((x - mu)/s, df = 5) pdf_t. &lt;- 1/s*pdf_t plot(x, pdf_n, type = &quot;l&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, col = &quot;grey&quot;) lines(x, pdf_t., type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot;) legend(&quot;topright&quot;, legend = c(&quot;N(178,8)&quot;, &quot;T(178,8,5)&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) Graf 3.17: Zobecněné t-rozdělení Pro úplnost ještě dodáme, že CDF náhodné proměnné \\(x_i\\) zobecněného t-rozdělení se vypočítá jako \\[CDF(x_i) = CDF_T(\\frac{x_i - \\mu}{\\sigma}, \\nu)\\]Zajímá nás jaká je pravdědobnost, zda hodnota náhodné proměnné pocházející z \\(x_i \\sim T(178,8,5)\\) bude mít hodnotu větší nebo rovnou než 185, tedy \\(P(x_i \\ge 185)\\). Porovnejte tuto pravděpodobnost se stejným výpočtem pro náhodnou proměnnou pocházející z \\(N(178,8)\\). q &lt;- 185 cdf_t. &lt;- pt((q - mu)/s, df = sv, lower.tail = FALSE) plot(x, pdf_t., type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &gt;= &quot;, q, &quot;)=&quot;, round(cdf_t., 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &gt;= q], pdf_t.[x &gt;= q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.18: CDF zobecněného t-rozdělení pro P(x &gt;= 185) 3.5 Uniformní Uniformní rozložení popisuje náhodný proces, v kterém mají všechny hodnoty v nějakém intervalu stejnou pravděpodobnost, že budou vybrány. Uniformní rozložení je definováno jak pro diskrétní, tak pro spojité proměnné. S diskrétním uniformním rozložením jsme se setkali minulý rok při hodu kostkou nebo hodu mincí. Pokud náhodná proměnná \\(x_i\\) pochází z uniformního rozložení, pak platí, že \\[x_i \\sim U(a,b)\\]Uniformní rozložení je tedy možná vyjádřit pomocí dvou parametrů \\(a\\) a \\(b\\), které vyjadřují minimální a maximální možné hodnoty proměnné. PMF diskrétního uniformního rozložení rozložení lze vyjádřit jako \\[PMF(x_i) = \\frac{1}{n}\\]U spojitého uniformního rozložení lze PDF vyjádřit jako \\[PDF(x_i) = \\frac{1}{b-a}; x_i \\in [a,b]\\]Pokud je hodnota \\(x_i\\) mimo \\([a,b]\\), pak je jeho PDF rovná nule (nebo jinak taková hodnota není možná). Očekávaná hodnota diskrétního i spojitéhouniformního rozložení se spočítá jako \\(E(x_i) = \\frac{1}{2}(a+b)\\). Rozptyl u spojitého uniformního rozložení se spočítá jako \\(Var(x_i) = \\frac{1}{12}(b-a)^2\\) a u diskrétního jako \\(Var(x_i) = \\frac{n^2-1}{12}\\). Protože generování čísel z diskrétního uniformního rozložení můžeme použít funkci sample, kterou jsme používali při simulacích některých klasických pravděpodobnostních problémů. Pro generování hodnot náhodné proměnné pocházející ze spojitého uniformního rozložení můžeme použít funkci runif. PDF poté vypočítáme pomocí funkce dunif a CDF pomocí punif. Jako příklad uniformního rozložení si ukážeme čekání na tramvaj. Řekněme, že přijdeme na zastávku a máme se rozhodnout, zda se nám vyplatí čekat nebo jít pěšky. Čekání na tramvaj v takovém případě můžeme modelovat (připodobnit) uniformním rozložením. Nevíme, kdy tramvaj přijede (v našem příkladu nejsou jízdní řády, nebo alespoň nejsou spolehlivé jízdní řády :). Může přijet hned (tedy za 0min), může přijet za 8min. Pravděpodobnost, za jak dlouho přijede (respektive, v jakém momentu jsme my přisli na zastávku) je rovnoměrně rozložena v celém intervalu. Pojďme si tento proces vyjádřit v animaci. Červená tečka reprezentuje nás čekající na zastávce. Modrý čtverec reprezentuje tramvaj blížící se k zastávce. Jak je vidět, někdy přijdeme na zastávku a tramvaj je blízko. Někdy přijdeme a tramvaj je stále daleko. # muzeme cekat 0 minut az 8 minut a &lt;- 0 b &lt;- 8 # pocet simulaci N &lt;- 5 # kde bude tram, kdyz prijdeme na zastavku x &lt;- seq(0, 8, by = 0.5) # pro kazdy pokus for (n in 1:N) { # vybereme nahodne jak daleko od nas tram bude ... # ...-0.5 protoze pricitame +0.5 v prvnim kroku tramvaj &lt;- runif(1, min = a, max = b) - 0.5 my &lt;- 8 # dokud neni tramvaj u nas while(tramvaj &lt; my){ # zobrazime kazdych 0.5 minuty tramvaj &lt;- tramvaj + 0.5 # protoze pridavame kazdych 0.5 min ... # ...abychom zobrazili tramvaj u nas na zastavce if(tramvaj &gt; my) tramvaj &lt;- my plot(tramvaj, 1, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, xlim = c(-1, 9), ylim = c(0, 2), pch = 22, col = &quot;#1f77b4&quot;, main = paste0(&quot;Pokus: &quot;, n), sub = paste0(&quot;Zbývající doba čekání: &quot;, round(my - tramvaj, 1), &quot;min&quot;)) # zobrazime nas points(my, 1, pch = 19, col = &quot;red&quot;) # otocime labely cekani 8-0 axis(1, at = x, labels = rev(x)) # pridame popisek text(4, 0, &quot;Vdálenost tramvaje od naší zastávky&quot;) legend(&quot;topright&quot;, legend = c(&quot;my&quot;, &quot;tramvaj&quot;), col = c(&quot;red&quot;, &quot;#1f77b4&quot;), pch = c(19, 22), cex = 0.7) } } Graf 3.19: Ukázka čekání na tramvaj jako náhodný proces z uniformního rozložení Pěšky je to do našeho cíle 6min. Tramvají 1min. Tramvaj jezdí jednou za 8min, tedy \\(t_i \\sim U(0, 8)\\). Vyplatí se nám čekat? ocekavana_hodnota &lt;- (a+b)/2 Očekávané hodnota našeho čekání je 4. Když k tomu přičteme 1min cesty tramvají, pak očekávaná hodnota naší cesty tramvají je 5min. Řekněme, že je velmi důležité, abychom nedorazili pozdě. Jaká je pravděpodobnost, že tramvají do cíle dorazíme za dobu delší než 6min (tedy, že nám bude cesta trvat déle, než pěšky)? x &lt;- seq(-1, 9, length.out = 1000) q &lt;- 5 pdf &lt;- dunif(x = x, min = a, max = b) cdf &lt;- punif(q, min = a, max = b, lower.tail = FALSE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;t&quot;, ylab = &quot;PDF&quot;, xlim = c(-1,9), main = paste0(&quot;P(x &gt; &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # type &quot;h&quot;, abychom vyjadrili integral lines(x[x&gt;q], pdf[x&gt;q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.20: Příklad PDF pro t ~ U(0, 8) a CDF pro P(t &gt; 5) Jak je vidět, pravděpodobnost, že tramvaj přijede za 5 a více minut (musíme si nechat 1 minutu na cestu tramvají, proto chceme vědět \\(P(t_i &gt; 5)\\)) je 0.38, tedy docela vysoká. Tato situace by nastala zhruba 1 ze 3 případů. Pravděděpodobně bychom tedy v takovém případě šli pěšky. 3.6 Poisson Poissonovo rozložení popisuje chování diskrétních náhodných proměnných v nějakém daném časovém intervalu. Od binomického rozložení se liší tím, že popisuje proměnné, které nemají pevně stanovený počet pozorování, tedy \\(x_i \\in [0, \\infty)\\). Toto rozložení má jediný parametr \\(\\lambda\\), který je zároveň očekávanou hodnotou a rozptylem. Stejně jako u binomického, normálního, t a uniformního rozložení předpokládáme, že jednotlivé hodnoty náhodné proměnné jsou na sobě nezávislé. Náhodnou proměnnou \\(x_i\\) pocházející z tohoto rozložení značíme jako \\(x_i \\sim Poisson(\\lambda)\\). Mnoho každodenních jevů, jejichž hodnoty jsou celé čísla se dají popsat poissonových rozložením. U mnoha jevů totiže neznáme \\(n\\) a tedy nedokážeme říct, jaká je největší očekávaná hodnota. Jako příklady jevů, které je možné approximovat (připodobnit) poissonových rozložením je například počet telefonátů za den, počet aut, které projedou silnicí za hodinu, počet gólů za zápas apod. Vrátíme se zpět do kapitoly popisné statistiky 2.1, kde jsme vypočítali počet slov ve větě v projevu prezideta republiky. Pro připomenutí ukážeme četnosti počtu slov ve větě. Graf 3.21: Četnost počtu slov ve větě v projevu prezidenta republiky Řekněme, že bychom chtěli predikovat počet slov ve větě v nějaké prezidentově dalším projevu a chtěli bychom to udělat tak, že bychom se snažili zjistit parametry procesu, který počet slov vyprodukoval. Víme, že minimálně můžeme mít ve větě jedno slovo. Maximální počet slov je sice prakticky ohraničen, ale my neznáme žádnou hranici, kterou bychom mohli uplatnit. Budeme tedy předpokládat, že ohraničený není4. Zároveň předpokládáme, že hodnoty náhodné proměnné vygenerované z poissonova rozložení jsou nezávislé. V našem případě jsou hodnoty spíše silně závislé. Počet slov v jedné větě bude určitě ovlivňovat počet slov v následující(ch) větách. Jak je vidět na 3.22 po hodně dlouhých větách, častěji následují kratší věty. Málokdy je více krátkých vět za sebou a naopak. plot(pocet_slov, type = &quot;l&quot;, lwd = 2, ylab = &quot;Počet slov&quot;, xlab = &quot;Pořadí&quot;, col = &quot;#1f77b4&quot;) Graf 3.22: Zkoumání závislosti jednotlivých hodnot proměnné počet slov lambda &lt;- mean(pocet_slov) Průměrný počet slov ve větě je 16.56, což je \\(\\lambda\\) našeho rozložení. Modelujeme tedy počet slov \\(s_i\\) ve větě \\(s_i \\sim Poisson(\\) 16.56 \\()\\).Graf 3.23 vyjadřuje PMF tohoto rozložení. Jak je vidět tento model na naše data moc nesedí. Náš model má moc malý rozptyl v porovnání s daty. Protože poissonovo rozložení má pouze jeden parametr \\(\\lambda\\), který vyjadřuje očekávanou hodnotu i rozptyl, nemůžeme rozptyl nijak kontrolovat pomocí parametru rozložení. Tento fakt je koneckonců zřejmý už z chrakteristik našich dat. Pokud by tato data opravdu byla vygenerována z poissonova rozložení jejich průměr by se zhruba rovnal rozptylu Jak je ale vidět, tak tomu není protože průměr počtu slov je 16.56 a rozptyl je 95.11. x &lt;- 1:50 n &lt;- length(pocet_slov) pmf &lt;- dpois(x, lambda) plot(x, pmf, type = &quot;h&quot;, lwd = 5, col = &quot;#1f77b4&quot;, xlab = &quot;s&quot;, ylab = &quot;PMF&quot;, main = paste0(&quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot;) ) Graf 3.23: PMF poissonova rozložení I když je náš model jednoduchý s pouze jedním parametrem \\(\\lambda\\), je to model, který můžeme použít pro predikci. Výsledky jakéhokoliv modelu je vždy dobré si zobrazit graficky, abychom si dokázali lépe představit, co predikuje. 3.24 ukazuje, kolik by náš model predikoval počtet slov ve větě. Protože PMF vyjadřuje pravděpodobnost, můžeme očekávaný (predikovaný) četnost počtu slov podle našeho modelu vypočítat jako \\(PMF * n\\). V našem případě máme 66 vět, tedy predikovaná četnost počtu slov \\(\\hat{s_i} = PMF * n\\). # zaokrouhlime, protoze musime mit cela cisla s_hat &lt;- round(pmf * n) # pridame data .x &lt;- as.numeric(names(cetnost)) # zobrazime nase data plot(.x, cetnost, type = &quot;h&quot;, lwd = 5, col = &quot;grey&quot;, xlab = &quot;Počet slov&quot;, ylab = &quot;Četnost&quot;, xlim = c(0, 50), ylim = c(0, 6), main = &quot;Porovnání modelu poissonova rozložení s daty&quot; ) # pridame ocekavane hodnoty lines(x, s_hat, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.4), type = &quot;h&quot;, lwd = 5) # aby y osa zdy vyjadrovala vsechna cisla axis(2, at = c(0:6), labels = c(0:6)) # legenda legend(&quot;topright&quot;, legend = c(&quot;Data&quot;, paste0(&quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot;)), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) Graf 3.24: Očekávaná četnost počtu slov ve větě pro n=66 vět Toto se v reálném světě často. Tento jev se v angličtině označuje jako overdispersion. Jednou z možností, jak se s overdispersion vypořádat je, že modelujeme náhodnou proměnnou jako pocházející z jiného rozložení5. Pro náš jednoduchý příklad bychom mohli využít častého triku, který převede pozitivní diskrétní data s větším rozptylem na data spojitá. Tento trik využívá transformace proměnné a to konkrétně jejího logaritmu. Jak je známo logaritmus je definován pro definiční obor \\(\\in (0, \\infty)\\). Pro zopakovaní ukazujeme funkci přirozeného logaritmu v grafu 3.25. Jak je vidno, tak funkce logaritmu převede hodnoty do oboru hodnot \\(\\in (-\\infty, \\infty)\\) a zároveň budou její hodnoty spojité. Normální rozložení je takto definováno a proto zkusíme modelovat počet slov ve větě \\(s_i\\) jako \\(log(s_i) \\sim N(\\mu_{log(s)}, \\sigma_{log(s)})\\). curve(log(x), from = 0.01, to = 100, col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;Log(x)&quot;, main = &quot;Přirozený logaritmus&quot;) abline(h = 0, v = 0, lwd = 1) Graf 3.25: Ukázka přirozeného logaritmu Graf 3.26 zobrazuje histogram logaritmu početu slov ve větě a teoretické PDF \\(N(2.61, 0.66)\\). Jak je vidět takovýto model lépe vystihuje rozptyl v počtu slov. Vzhledem k tomu, že máme pouze 66 pozorování nebude taková náhodná proměnná přesně vystihovat teoretické rozložení. Graf 3.27 ukazuje odhadnutou hustotu pravděpodobnosti pro 50 výběrů z \\(N(2.61, 0.66)\\), každý o velikosti \\(n=\\) 66. Modrá křivka ukazuje naše data, šedé křivky potom simulované výběry. Je vidět, že několiktakových výběrů, které jsou podobně nahnuté doprava nastalo, což znamená, že taková data nejsou tak nepravděpodobná (pokud pocházejí z \\(log(s_i) \\sim N(2.61, 0.66)\\)). s_log &lt;- log(pocet_slov) mu &lt;- mean(s_log) s &lt;- sd(s_log) x &lt;- seq(0.01 , 5 , length.out = 1000) pdf &lt;- dnorm(x, mean = mu, sd = s) hist(s_log, xlab = &quot;log(s)&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram logaritmu počtu slov&quot;, col = &quot;grey&quot;, breaks = 10, xlim = c(0, 5)) # pridame na druhou osu par(new = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, bty = &quot;n&quot;) legend(&quot;topright&quot;, legend = c(&quot;log(s)&quot;, paste0(&quot;N(&quot;, round(mu, 2), &quot;,&quot;, round(s, 2), &quot;)&quot;)), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) thicks &lt;- round(seq(min(pdf), max(pdf), by = 0.05), 2) axis(4, at = thicks, labels =thicks) Graf 3.26: Rozložení logaritmu dat a teoretická PDF N(2.61, 0.66) # odhadneme hustotu pravdepodobnosti dat d_data &lt;- density(s_log) S &lt;- 50 plot(d_data, type= &quot;l&quot;, xlab = &quot;log(s)&quot;, ylab = &quot;Hustota pravděpodobnosti&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, xlim = c(0, 5), ylim = c(0, 0.8), lwd = 2) for(i in 1:S) { vyber &lt;- rnorm(n, mean = mu, sd = s) lines(density(vyber), col = adjustcolor(&quot;black&quot;, alpha.f = 0.2)) } Graf 3.27: Příklad hustoty pravděpodobnosti 50 výběrů o velikosti n=66 z N(2.61, 0.66) Nakonec se tedy vrátíme k naší původní otázce - predikce počtu slov ve větě. Jak jsme si ukázali, logaritmus počtu slov ve větě můžeme dobře approximovat normálním rozložením. Na tomto rozložení můžeme počítat CDF stejně tak, jak jsme si to ukázali v 3.3. Řekněme, že z důvodu srozumitelnosti nedoporučejeme věty delší než 20 slov. Zajímá nás, jaká je pravděpodonost, že libovolně zvolená věta bude delší než 20 slov, pak vlastně počítáme \\(P(log(s_i) &gt; log(20))\\) z našeho \\(log(s_i) \\sim N(2.61, 0.66)\\)6. q &lt;- log(20) cdf &lt;- pnorm(q, mean = mu, sd = s, lower.tail = FALSE) Tato pravděpodobnost je rovna 0.28. Nebo jinak zhruba 28% vět bude delších než 20 slov. 3.7 Chi-kvadrát \\(\\chi^2\\) rozložení popisuje náhodný proces, kterého nabývá součet umocněných stadandardizovaných normálně rozložených proměnných. Jak jsme uvedli v 3.3, standardizovaná náhodná proměnná pocházející z normálního rozložení \\(z_i\\) má \\(\\mu_z = 0\\) a \\(\\sigma_z = 1\\). Pokud máme \\(k\\) standardizovaných náhodných proměnných, pak můžeme zapsat \\(Q = \\sum_{i=1}^k z_i^2\\). Toto rozložení se nepoužívá tak často k popsání jevů z fyzického světa jako spíše pro popsání chování různých parametrů, které se ve statistice odhadují. Protože mnoho odhadovaných paramtrů (jako např. průměr) mají normální rozložení, můžeme jejich pravděpodobnostní rozložení popsat právě pomocí \\(\\chi^2\\), což se hodí při posuzování významnosti různých procedur. \\(\\chi^2\\) má jediný parameter \\(k\\), který vyjadřuje počet stupňů volnsti (degrees of freedom). V 3.28 ukazujeme, jak \\(\\chi^2\\) rozložení vzniká. Děláme výběr o velikosti \\(n=200\\) ze standardizovaného normálního rozložení pro \\(k=2\\) proměnné, \\(k=4\\) a \\(k=16\\), tedy \\(z \\sim \\chi^2(2)\\), \\(z \\sim \\chi^2(4)\\) a \\(z \\sim \\chi^2(16)\\). Proměnné umocníme, sečteme a zobrazíme histogram rozložení této nově vzniklé náhodné proměnné. # velikost vyber n &lt;- 200 # pocet standardizovanych promennych K &lt;- c(2, 4, 16) #zobrazit grafy vedle sebe par(mfrow = c(3, 1)) for(k in K) { Z &lt;- sapply(1:k, function(x) rnorm(n, 0, 1)) X2 &lt;- rowSums(Z^2) hist(X2, xlab = &quot;z&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;X2(&quot;, k, &quot;)&quot;), col = &quot;#1f77b4&quot;,) } Graf 3.28: Ukázka X2 rozložení pro k=2, k=4 a k=16 Jak je vidět \\(\\chi^2\\) je definováno pouze pro \\([0, \\infty)\\), tedy pro pozitivní hodnoty spojité proměnné. To je proto že počítáme s mocninou, tedy žádné negativní hodnoty v našem definičním oboru být nemohou. Očekávanou hodnotou je \\(k\\), tedy \\(E(z_i) = k\\) a rozptyl je \\(2k\\), tedy \\(Var(z_i)=2k\\). S rostoucím počtem stupňů volnosti \\(k\\) se rozložení tvarem podobá normálnímu rozložení s velkým rozptylem (protože rozptyl je roven \\(2k\\)). Graf 3.29 zobrazuje PDF pro \\(z \\sim \\chi^2(2)\\), \\(z \\sim \\chi^2(4)\\), \\(z \\sim \\chi^2(8)\\), \\(z \\sim \\chi^2(16)\\) a \\(z \\sim \\chi^2(32)\\). x &lt;- seq(0, 50, length.out = 1000) K &lt;- 2^c(1:5) PDF &lt;- sapply(K, function(k) dchisq(x, k)) cols &lt;- rev(RColorBrewer::brewer.pal(n = length(K)+1, name = &quot;Blues&quot;)) plot(x, PDF[, 1], type= &quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;PDF&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, lwd = 2) for(j in 2:length(K)) { lines(x, PDF[, j], type = &quot;l&quot;, lwd = 2, col = cols[j]) } legend(&quot;topright&quot;, legend = paste0(&quot;X2(&quot;, K, &quot;)&quot;), col = c(&quot;#1f77b4&quot;, cols[2:length(K)]), lwd = rep(2, length(K)), cex = 0.7) Graf 3.29: PDF X2 rozložení s k=2, k=4, k = 8, k=16 a k=32 3.8 Jak vybrat správný model pro data My jsme si v této kapitole ukázali některé nejčastější rozložení, které popisují náhodné procesy vyskutující se ve fyzikálním světě okolo nás. Při analýze máte zpravidla nějaká data (náhodné proměnné), jejichž chování se nažíte popsat pomocí nějakého modelu. To jaký model na vaše data vybrat záleží především na povaze dat, tedy zda jsou data spojitá nebo diskrétní. Dále pak na tom, jaké hodnoty byste u vašich dat čekali, především, zda májí data nějakou hranici, za kterou data nejsou možná (např. data musí být pozitivní apod.). V poslední řadě pak záleží na tom, zda zvolený model věrohodně vystihuje pravděpodobnost, s kterou data nabývají různých hodnot. Znovu zopakujeme, že model představuje připodobnění reality a jeho užitečnost závisí na otázce, kterou se snažíme modelem zodpovědět. Vždy je dobrým zvykem si data zobrazit. Ze zvažovaného modelu si vygenerujte data a porovnejte je s reálnými daty. Uvažujte jakých extrémních hodnot by váš model mohl dosáhnout a zda jsou takové hodnoty pro problém který zkoumáte vůbec reálné. Zvažte, do jaké míry váše data (nebo jejich sběr) porušují předpoklady vašeho modelu7. Právě díky grafickému zobrazení různých simulací můžete pochopit, jak by mohly různé předpoklady váš model rozbít, nebo za jakých podmínek přestane věrohodně vystihovat vaše data. Nakonec představíme aplikaci, pomocí které si můžete vyzkoušet, jak různé parametry ovlivňují tvar různých rozložení nebo vypočítat pravděpodobnost, že náhodná proměnná \\(x_i\\) nabyde určitých hodnot \\(a\\). Zdroj: https://github.com/ShinyEd/intro-stats/tree/master/dist_calc Např. teplota, hmotnost, výška. Naše schonost tyto jevy měřit je ale často diskrétní, hmotnost můžeme měřit na mg apod. Protože jsou ale jednotky míry často malé, bereme je jako spojitou proměnnou↩︎ Teoreticky jsou ohraničené, ale prakticky se v přírodě extrémy často nevyskutují. Například teplota neklesá k absolutní nule. Váha zvířat je zpravidla větší než 0 mg apod.↩︎ Jedná se o pravděpodobnost, protože počítáme PDF v nějakém intervalu CDF↩︎ PMF poissonova rozložení klesá s rostoucí hodnotou \\(x_i\\) rychle k zanedbatelné hodnotě, takže bude nepravděpodobné, že bychom generovali věty s nadpřirozenými počty slov.↩︎ Tím je v generalized linear models Beta negative binomial↩︎ Pokud bychom chtěli věrohodně zachytit nejistotu ohledně počtu slov v našich datech, měli bychom počítat s nejistotou odhadu průměru a směrodatné odchylky rozložení. Něco, co si ukážeme v příští kapitole.↩︎ Skoro všechna data porušují do nějaké míry alespoň jedne z předpokladů modelu :)↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
