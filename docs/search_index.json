[["index.html", "Úvod do statistiky Kapitola 1 Úvod 1.1 Příklad popisné statistiky 1.2 Příklad inferenční statistiky 1.3 Simulace 1.4 O modelech", " Úvod do statistiky Jan Schubert 2022-05-02 Kapitola 1 Úvod Tato kniha navazuje na kurz Základy logiky a matematiky (JSB536), ale jeho absolvování není nutné k pochopení látky. Statistika je nástroj, který aplikuje matematiku na získání užitečných informací z dat. Roli statistiky je možné rozdělit na dva úkoly: Popisování většího množství dat (popisná statistika) Předpovídání nějakého fenomenu/určení míry nejistoty (inferenční statistika) 1.1 Příklad popisné statistiky Mnoho fenomenů v každodenním světě je možné vyjádřit pomocí dat. Fotka se dá vyjádřit jako 3D matice (red, green, blue) řádků a sloupců, která vyjadřuje jednotlivé pixely. Lidl Stiftung &amp; Co. KG, Public domain, via Wikimedia Commons obrazek &lt;- png::readPNG(&quot;../imgs/img1.1.svg.png&quot;) dim(obrazek) ## [1] 480 480 3 Můžeme například zobrazit hodnoty červené barvy prvních 5x5 pixelů. obrazek[1:5, 1:5, 1] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1.0000000 ## [2,] 1 1 1 1 1.0000000 ## [3,] 1 1 1 1 1.0000000 ## [4,] 1 1 1 1 1.0000000 ## [5,] 1 1 1 1 0.3215686 Můžeme zobrazit pouze nějaké řádky a sloupce: plot(0:1, 0:1, type = &quot;n&quot;, ann = FALSE, axes = FALSE) rasterImage(obrazek[125:325, 100:300, ], 0, 0, 1, 1) Můžeme vypočítat průměrnou nebo medianovou barvu. # vypocitame prumer pro kazdou barvu (cervena, zelena, modra) prumerna_barva &lt;- sapply(1:3, function(i) mean(obrazek[, , i])) # vypocitame barvu na rgb skale prumerna_barva &lt;- rgb(prumerna_barva[1], prumerna_barva[2], prumerna_barva[3]) # vypocitame median pro kazdou barvu (cervena, zelena, modra) median_barva &lt;- sapply(1:3, function(i) median(obrazek[, , i])) # vypocitame barvu na rgb skale median_barva &lt;- rgb(median_barva[1], median_barva[2], median_barva[3]) # zobrazime v grafu plot(1,1, type = &quot;n&quot;, axes = FALSE, ann = FALSE, xlim = c(0, 2), ylim = c(0, 1)) rect(0, 0, 1, 0.9, col = prumerna_barva) text(0.45, 1, labels = paste0(&quot;Průměrná barva: &quot;, prumerna_barva)) rect(1, 0, 2, 0.9, col = median_barva) text(1.45, 1,labels = paste0(&quot;Mediánová barva: &quot;, median_barva)) Protože obrázek je matice dat, můžeme na ni uplatnit různé statistické metody. Zajímá nás například, jaké odstiny barev jsou použité v tomto logu. Vidíme, že logo se skládá ze tří barev a můžeme extrahovat 3 barevy pomocí shlukovacího algoritmu. set.seed(42) k &lt;- 3 cervena &lt;- as.vector(obrazek[, , 1]) zelena &lt;- as.vector(obrazek[, , 2]) modra &lt;- as.vector(obrazek[, , 3]) m &lt;- kmeans(cbind(cervena, zelena, modra), centers = k) barvy &lt;- m$centers barvy_rgb &lt;- rep(NA, k) plot(seq(1, k * 10 + 10, length.out = 10), seq(1, 10, length.out = 10), axes = FALSE, ann = FALSE, type = &quot;n&quot; ) for (i in 1:k) { barvy_rgb[i] &lt;- rgb(barvy[i, 1], barvy[i, 2], barvy[i, 3]) rect(i * 10, 1, i * 10 + 10, 8, col = barvy_rgb[i]) text(i * 10 + 5, 9, labels = barvy_rgb[i]) } 1.2 Příklad inferenční statistiky Většina jevů okolo nás je ovlivněna náhodou, ať už z důvodu náhodného výběru nebo protože jsou součástí nějakého komplikovaného systému, který ovliňuje hodnoty jevu, který nás zajímá. Počet aut, které projedou na mostě, délka toaletního paríru, který je vyrobený v továrně, to jsou některé příklady jevů, které jsou ovlivněny náhodou. Statistika nám poskytuje soubor nástrojů, jak tuto náhodu (nejistotu) kvantifikovat. Pohlaví dětí je určeno náhodou a nově narozené dítě má zhruba stejnou pravděpodobnost, že bude děvče nebo chlapec. Řekněme, že z důvodu kapacitního plánování nás zajímá, kolik chlapců se narodí, pokud se v porodnoci denně narodí 25 dětí (sám počet narozených dětí by se dal modelovat jako náhodná proměnná). n &lt;- 25 p &lt;- 0.5 x &lt;- c(0:25) pmf &lt;- dbinom(x = x, size = n, p) plot(x, pmf, type = &quot;h&quot;, xlab = &quot;Počet chlapců z 25 narozených dětí&quot;, ylab = &quot;Pravděpodobnost&quot;, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 1.1: Pravděpodobnostní rozložení počtu chlapců z 25 narozených dětí Vidíme, že nejpravděpodobněji se narodí 12 chlapců (v 31% případů). Můžeme také z grafu vypočítat, že více než 15 chlapců se narodí zhruba v 11% případů, tedy zhruba 42 dní v roce. 1.3 Simulace K pochopení statistiky budeme používat programovací jazyk. Ten nám umožní, abychom si statistické koncepty osahali detailně. Budeme simulovat data, u kterých budeme vědět pravé hodnoty a sledovat, jak (ne)úspěšně různé statistické postupy pravé hodnoty odhadují. Cílem je, abychom statistiku pochopili tak, že ji budeme moci použít na konkrétní problém. Chceme dosáhnout toho, aby statistika byla jazykem, který můžeme použít na různé datové problémy. K pochopení statistiky budeme používat programovací jazyk R. Našim cílem je ale koncepty vysvětlovat a kódovat obecně tak, aby postupy byly lehko přenositelné do jiného programovacího jazyka. Vždy si tedy vysvětlíme konkétní výpočet nebo proceduru do podrobna a to i když existuje balíček nebo funkce, která by daný výpočet provedla za nás. 1.4 O modelech Modely jsou reprezentaci reality. Jsou (někdy) užitečné, protože zjednodušeně ukazují vlastnosti toho, co nás zajímá. Glóbus je příkladem modelu planety země. Glóbus nevystihuje přesně to, jak planeta vypadá. Nejsou na něm zaznamenány všechny ostrovy, jeho tvar neodpovídá přesně tvaru naší planety. Přesto jsou glóbusy užitečné k pochopení toho, jak planeta vypadá. Dalším příkladem modelu je mapa. Mapa je ještě více zkresleným modelem terénu než glóbus (mapa musí vněstnat 3D svět do 2D modelu). Mnoho map dokonce velmi nepřesně reprezentuje terén, přesto jsou ale mapy nesmírně užitečné když se potřebujeme dostat z bodu A do bodu B. Stejně je tomu s modely statistickými. Nejsou přesným vyjádřením reality, ale mohou být užitečným vyjádřením reality. Jejich užitečnost bude záviset na činnosti, pro který jsme tento model stvořili. Důležité je dodat, že naše modely (ne)fungují na datech, které jsme jim dodali. Model, který je užitečný na jedněch datech může být bezcenný na jiných datech. Je tedy vždy potřeba přemýšlet o tom, zda je náš model vhodný pro data a situaci, na kterou se ho snažíme použít. "],["desc-stats.html", "Kapitola 2 Popisná statistika 2.1 Míry centrální tendence 2.2 Míry rozptýlenosti 2.3 Míry polohy", " Kapitola 2 Popisná statistika Jak jsme zmínili v úvodu popisná statistika je jeden z hlavních cílů statistiky. Úkolem popisné statistiky je shrnout informace o našem výběru do pár čísel, které nám pomohou pochopit jaké má náš výběr vlastnosti. Hlavními vlastnostmi, které nás zajímají je: Jaká je typická hodnota měřené proměnné (míra centrální tendence) Na kolik se liší hodnoty jendotlivých pozorování (míra rozptýlenosti) Abychom si popisnou statistiku představili, budeme používat Novoroční/Vánoční projev prezidenta republiky. Nejdříve si data načteme. Text můžeme načíst různými způsoby, my použijeme funkci readLines, která vrátí zpět vektor. prezident &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/prezident.txt&quot;, encoding = &quot;UTF-8&quot;) Nejdříve text očistíme o mezery a čárky, přeneseme vše do malých písmen, rozdělíme na věty a potom na slova. Protože tuto proceduru budeme dělat vícekrát, uděláme si na to funkci. Výstupem této funkce bude list, jehož každý element reprezentuje jednu větu a v rámci této věty jaká obsahuje slova. vycistit_text &lt;- function(.text) { # vse malym pismem .text &lt;- tolower(.text) # odstranime prazdne radky .text &lt;- .text[.text != &quot;&quot;] # odstranime carky .text &lt;- gsub(pattern = &quot;,&quot;, replacement = &quot;&quot;, x = .text) # rozdelit na vety, pokud najdeme &quot;.&quot; nebo &quot;!&quot; nebo &quot;?&quot; .text &lt;- unlist(strsplit(.text, split = &quot;\\\\.|\\\\!|\\\\?&quot;)) # vymazeme mezery na zacatku a konci .text &lt;- trimws(.text, which = c(&quot;left&quot;)) .text &lt;- trimws(.text, which = c(&quot;right&quot;)) # odstranime prazdne prvky, ktere vznikly protoze po tecce neni zadny text .text &lt;- .text[.text != &quot;&quot;] # rozdelime na slova slova &lt;- list() for (i in 1:length(.text)) { slova[[i]] &lt;- unlist(strsplit(.text[i], split = &quot; &quot;)) } return(slova) } prezident_clean &lt;- vycistit_text(prezident) 2.1 Míry centrální tendence Míry centrální tendence se snaží popsat nějakou typickou hodnotu proměnné. My si představíme modus, medián, průměr, absolutní a relativní četnost. Jaké míry centrální tendence můžeme na proměnné vypočítat se liší podle typu proměnné. Nominální proměnná je taková proměnná, u které nemůžeme hodnoty seřadit od nejmenšího po největší a nemůžeme ani určit o kolik je jedna hodnota větší než jiná. Tou nejzákladnější popisnou statistikou je četnost nějakého jevu a z ní odvozená míra centrální tendence modus. Modus proměnné \\(x_i\\), který označujeme \\(\\hat{x}\\), je tedy nejčastější hodnota proměnné. Když se zamyslíte, tak u proměnné u které nemůžeme hodnoty seřadit ani jinak matematicky porovnat je nejčastější hodnota nejvíce vypovídající o typické hodnotě proměnné. Pojďme si jako příklad vypočítat nejčastější slovo z projevu. # nejprve prevedeme list na vekor slova &lt;- unlist(prezident_clean) # vypocitame cetnost hodnot tabulka_slov &lt;- table(slova) Naše tabulka četností je velká, obsahuje 651 hodnot. To mimo jiné znamená, že v projevu bylo použito 651 unikátních slov. Abychom získali nejčastější hodnotu musíme si tabulku seřadit od největší četnost po nejmenší a zobrazit první hodnotu. Jméno nejčastější hodnoty nám prozradí, modus této proměnné. tabulka_slov_serazena &lt;- sort(tabulka_slov, decreasing = TRUE) modus &lt;- names(tabulka_slov_serazena)[1] Modus této proměnné je hodnota “a”, která se vyskytla 45x. To je v textové analýze typické a tato slova se označují jako “stopwords” a jsou zpravidla a textové analýzy vyřazena. Další mírou centrální tendence je medián. Medián nám značí prostřední hodnotu nějaké proměnné. Můžeme si jeho výpočet představit tak, že hodnoty proměnné seřadíme od nejmenší po největší a vyberete hodnotu, která bude přesně uprostřed. Tato hodnota je medián. Matematicky se medián u proměnné \\(x_i\\) vypočítá jako \\[\\tilde{x} = x_{(n + 1)/2}\\] Pokud má naše proměnná sudý počet čísel, vypočítá se medián zpravidla jako průměr dvou prostředních hodnot, tedy \\[\\tilde{x} = \\frac{x_{n/2} + x_{n/2+1}}{2}\\]Řekněme, že bychom chtěli vědět medián počtu slov ve větě. Nejdříve si musíme pro každou větu (prvek listu prezident_clean) vypočítat počet slov. pocet_slov &lt;- sapply(prezident_clean, length) Velmi dobrým zvykem je si rozdělení hodnot proměnné zobrazit graficky. Grafické zobrzení nám vždy poví nejenom o typické hodnotě proměnné, ale i o tom, jak se hodnoty liší (viz Míry rozptýlenosti 2.2). Pokud máme číselnou proměnnou je nejčastějším způsobem, jak zobrazit hodnoty proměnné histogram. Histogram je: …grafické znázornění distribuce dat pomocí sloupcového grafu se sloupci stejné šířky, vyjadřující šířku intervalů (tříd), přičemž výška sloupců vyjadřuje četnost sledované veličiny v daném intervalu. Zdroj: Wikipedia Můžeme ho vytvořit pomocí funkce hist. Na grafu 2.1 vidíme, že nejčastěji věta obsahuje mezi 5 až 25 slovy, ale některé věty obsahují i více než 35 slov. hist(pocet_slov, col = &quot;#1f77b4&quot;, xlab = &quot;Počet slov ve větě&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram počtu slov ve větě&quot; ) Graf 2.1: Příklad histogramu na číselné (kardinální) proměnné Funkce hist automaticky zvolí vhodné intervaly pro sloupce v grafu. Pokud bychom je chtěli změnit, můžeme tak udělat pomocí argumentu breaks. hist(pocet_slov, col = &quot;#1f77b4&quot;, xlab = &quot;Počet slov ve větě&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram počtu slov ve větě&quot;, breaks = 15 ) Graf 2.2: Histogram s 15 intervaly n &lt;- length(pocet_slov) # protoze mame sudy pocet slov vypocitame prumer dvou prostrednich hodnot i1 &lt;- n / 2 i2 &lt;- i1 + 1 .median &lt;- mean(pocet_slov[c(i1, i2)]) # nebo pomoci funkce median(pocet_slov) Medián můžeme také vypočítat pomocí funkce median. Mediánový počet slov ve větě je 15.5. Pojďme si výpočet zobrazit graficky a seřadit si počet slov od nejmenšího po největší a zobrazit si medián na grafu. plot(1:n, sort(pocet_slov), col = &quot;#1f77b4&quot;, ylab = &quot;Počet slov ve větě&quot;, xlab = &quot;Pořadí&quot;, main = &quot;Počet slov ve větě seřazený podle velikosti&quot;, type = &quot;h&quot;, lwd = 5 ) # pridame prostredni hodnoty lines(i1, pocet_slov[i1], col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 5) lines(i2, pocet_slov[i2], col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 5) # pridame median lines(mean(c(i1, i2)), .median, col = &quot;red&quot;, type = &quot;h&quot;, lwd = 1, lty = 2) lines(c(-2, mean(c(i1, i2))), c(.median, .median), col = &quot;red&quot;, type = &quot;l&quot;, lwd = 1, lty = 2) legend(&quot;topleft&quot;, col = c(&quot;grey&quot;, &quot;red&quot;), lwd = c(4, 4), legend = c(&quot;Prostřední hodnoty&quot;, &quot;Median&quot;), cex = 0.7 ) Graf 2.3: Seřazení proměnné pro ilustraci výpočtu mediánu Asi nejčastější mírou centrální tendence, která se používá je průměr. Průměr je možné vypočítat pouze pro kardinální proměnné. Technicky kardinální proměnné rozlišujeme na diskrétní a spojitou. Diskrétní nabývá celých čísel (1,2,3,4 etc., například počet dětí), tedy \\(\\in Z\\). Spojitá proměnná pak teoreticky nebývá nekonečně mnoho hodnot, prakticky je ale omezena tím, jak přesně dokážeme danou metriku měřit. Platí ale, že spojité proměnné nabývají racionálních čísel, tedy \\(\\in R\\). Průměr proměnné \\(x_i\\) vypočítáme jako \\[\\overline{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\] # prumer x_prumer &lt;- sum(pocet_slov) / length(pocet_slov) # nebo pomci funkce mean(pocet_slov) # vazeny prumer w &lt;- length(pocet_slov):1 x_prumer_w &lt;- sum(w * pocet_slov) / sum(w) # nebo pomci funkce weighted.mean(pocet_slov, w) Průměrný počet slov ve větě je 16.56. Někdy nechceme všem pozorováním při výpočtu průměru dát stejnou váhu. V takovém případě vypočítáme vážený průměr. Jeho vzorec je \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} w_ix_i}{\\sum_{i=1}^{n}w_i}\\] Řekněme například, že bychom průměrný počet slov chtěli vážit pozicí v textu a dát slovům v první větě větší váhu, než slovům, které se objevily později v textu. Takovýto vážený průměr je 17. Důležitý rozdíl mezi mediánem a průměr nastává pokud nejsou data symetricky rozdělena. Symetricky rozdělená data jsou taková, která mají podobný počet hodnot nalevo a napravo od průměru. V našem případě rozdělení počtu slov ve větě je medián menší než průměr. To je typické pro rozdělení, která mají delší konec napravo od průměru. Průměr je totiž náchylný na extrémní pozorování. Medián je založený na pořadí, takže ho extrémní pozorování tolik neovlivní. Většina proměnných, která je ohraničená zleva (nemůže mít menší hodnotu než nějaká hranice) má asymetrické rozdělení s více hodnotami napravo od průměru, např. příjem. V takových případech může být medián lepší mírou centrální tendence, ale výběr bude vždy záležet na otázce, kterou daty chceme zodpovědět. I pomocí měr centrální tendence se dají dělat zajívé analýzy. Řekněme, že nás zajímá rozdíl v relativní četnosti slov v projevu prezidenta a premiéra1. Graf 2.4 zobrazuje 20 slov2, které mají největší relativní četnost. Jak je vidět projev premiéra se zaměřoval častěji na problémy ČR, kdežto projev prezidenta se častěji zaměřoval na rozpočet a očkování3. # nacteme text projevu premiera premier &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/vlada.txt&quot;) premier_clean &lt;- vycistit_text(premier) # extrahujeme slova z vet do vektoru slova_premier &lt;- unlist(premier_clean) # nacteme a odstranime stopwords stopwords &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/stopwords.txt&quot;, encoding = &quot;UTF-8&quot;) # vybereme pouze slova, ktera nejsou ve stopwords slova_premier &lt;- slova_premier[!slova_premier %in% stopwords] slova &lt;- slova[!slova %in% stopwords] # vypocitame absolutni cetnost slov cetnost_premier &lt;- table(slova_premier) cetnost_prezident &lt;- table(slova) # vypocitame relativni cetnost cetnost_premier &lt;- cetnost_premier / sum(cetnost_premier) cetnost_prezident &lt;- cetnost_prezident / sum(cetnost_prezident) # vypocitame relativni cetnost # ted slouzime obe tabulky rel_cetnost &lt;- merge(data.frame(cetnost_premier), data.frame(cetnost_prezident), by.x = &quot;slova_premier&quot;, by.y = &quot;slova&quot; ) # prejmenuje sloupce, aby davaly vice smysl colnames(rel_cetnost) &lt;- c(&quot;slova&quot;, &quot;premier&quot;, &quot;prezident&quot;) head(rel_cetnost) ## slova premier prezident ## 1 alespoň 0.001686341 0.001547988 ## 2 bezpečnostní 0.001686341 0.001547988 ## 3 bojovat 0.001686341 0.001547988 ## 4 ceny 0.003372681 0.001547988 ## 5 české 0.006745363 0.001547988 ## 6 cesta 0.005059022 0.003095975 # vybereme nejcastejsich 20 slov pro každého top_premier &lt;- order(rel_cetnost$premier, decreasing = TRUE)[1:20] top_prezident &lt;- order(rel_cetnost$prezident, decreasing = TRUE)[1:20] # vyfiltrujeme top_index &lt;- unique(c(top_premier, top_prezident)) rel_cetnost_top &lt;- rel_cetnost[top_index, ] x &lt;- 1:nrow(rel_cetnost_top) plot(x, rel_cetnost_top$premier, type = &quot;h&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;Relativní četnost&quot;, col = &quot;black&quot;, lwd = 3 ) lines(x + 0.3, rel_cetnost_top$prezident, type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 3 ) axis(1, at = x, labels = rel_cetnost_top$slova, las = 3, cex.axis = 0.8) legend(&quot;topright&quot;, legend = c(&quot;Premiér&quot;, &quot;Prezident&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(3, 3), cex = 0.8) Graf 2.4: Top 20 slov podle relativní četnosti v projevu prezidenta a premiéra 2.2 Míry rozptýlenosti Míry centrální tendence nám udávájí hodnotu typického pozorování proměnné. Míry rozptýlenosti nám říkají, jak jsou hodnoty rozptýleny okolo nějaké typické hodnoty. Nejjednodušší mírou rozptýlenosti je variační rozptyl, kdy nás zajímá rozdíl mezi největší a nejmenší hodnotou proměnní \\(x_i\\), tedy \\(VR = max(x_i) - min(x_i)\\). vr &lt;- max(pocet_slov) - min(pocet_slov) Minimální počet slov ve větě je 3, nejdelší věta má 43 slov. Variační rozpětí je tedy 40. Pro kardinální proměnné se nejčastěji používá rozptyl, který vypočítáme tak, že každou hodnotu proměnné odečteme od průměru a umocníme. Tyto hodnoty sečteme a vydělíme počtem pozorování. Matematicky bychom rozptyl \\(s^2\\) proměnné \\(x_i\\) vypočítali jako \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\overline{x})^2\\]4Ve statistice se také používá pro výpočet rozptýlenosti směrodatná odchylka \\(s\\), které se vypočítá jako \\[s = \\sqrt{s^2}\\]Protože umocněné vzdálenosti od průměru opět odmocníme, vrátíme je tím na stejnou škálu jako \\(x_i\\). Směrodatná odchylka tedy představuje typickou vzdálenost pozorování od průměru. Ukažme si příklad na počtu slov. n &lt;- length(pocet_slov) rozptyl &lt;- sum((pocet_slov - mean(pocet_slov))^2) / n # nebo v R pomoci var smerodatna_odchylka &lt;- sqrt(rozptyl) # nebo v R pomoci sd Rozptyl počtu slov ve větě je tedy 93.67 a směrodatná odchylka je 9.68. Typická vzdálenost od průměrného počtu slov ve větě, který je 16.56, je tedy zhruba 10 slov. Ukažme si princip rozptylu/směrodatné odchylky na imaginárních datech. Na ukázku si vytvoříme proměnnou, která má 10 pozorování a zobrazíme je do grafu 2.5 jako body. Červená čára označuje průměr těchto bodů. Horizontální čáry potom označují vzdálenost každého pozorování od průměrné hodnoty. Nejdříve si ukážeme příklad s menším rozptylem hodnot a pod ním příklad rozdělení s větším rozptylem hodnot. Protože mají oba příklady stejný počet pozorování (10), můžete si rozdíl v jejich směrodatné odhylce představit jako rozdíl vertikálních úseček, které vedou od průměru. par(mfrow = c(2, 1)) x &lt;- rnorm(1e4, mean = 5, sd = 1) prumer &lt;- mean(x) n &lt;- seq(1, 10) smerodatna_odchylka &lt;- sd(x) # prvni graf plot(x[n], n, main = paste0(&quot;Směrodatná odchylka: &quot;, round(smerodatna_odchylka, 2)), xlim = c(0, 10), xlab = &quot;&quot;, ylab = &quot;Číslo pozorovaní&quot;, pch = 19, col = &quot;#1f77b4&quot; ) abline(v = prumer, col = &quot;black&quot;, lwd = 2) for (i in n) { lines(c(prumer, x[i]), c(i, i), col = &quot;#1f77b4&quot;, lwd = 2) } legend(&quot;topright&quot;, legend = c(&quot;Průměr&quot;, &quot;Vzdal. od průměru&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) # druhy graf x2 &lt;- rnorm(1e4, mean = 5, sd = 2.5) prumer &lt;- mean(x2) smerodatna_odchylka &lt;- sd(x2) plot(x2[n], n, main = paste0(&quot;Směrodatná odchylka: &quot;, round(smerodatna_odchylka, 2)), xlim = c(0, 10), xlab = &quot;&quot;, ylab = &quot;Číslo pozorovaní&quot;, pch = 19, col = &quot;#1f77b4&quot; ) abline(v = prumer, col = &quot;black&quot;, lwd = 2) for (i in n) { lines(c(prumer, x2[i]), c(i, i), col = &quot;#1f77b4&quot;, lwd = 2) } legend(&quot;topright&quot;, legend = c(&quot;Průměr&quot;, &quot;Vzdal. od průměru&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) Graf 2.5: Ukázka výpočtu směrodatné odchylky Pro úplnost, jak by vypadal histogram rozdělení obou proměnných. V tomto případě jsme data generovali z normálního rozdělení (viz kapitola pravděpodobnostní rozdělení 3). hist(x, col = adjustcolor(&quot;#1f77b4&quot;, 0.9), breaks = 20, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;, xlim = c(0, 10) ) hist(x2, col = adjustcolor(&quot;black&quot;, 0.4), breaks = 40, add = TRUE ) Graf 2.6: Histogram bodů z normálního rozdělení o průměru 5 a směrodatné odchylce 1 (modrá) a 2.5 (černá) 2.3 Míry polohy Poslední popisnou statistikou, kterou si představíme, jsou míry polohy. Ty nám udávají polohy nějakého pozorování proměnné vůči ostatním pozorováním. První mírou polohy, kterou si představíme je kvantil. Kvantil proměnné \\(x_i\\) se značí jako \\(x_q\\) a je to číslo, které nám říká podíl případů proměnné \\(x_i\\), které leží pod danou hodnotou proměnné \\(x_q\\). Medián proměnné je tedy vlastně specifická hodnota kvantilu \\(q=0.5\\), tedy \\(x_{0.5}\\). Pokud chceme data rozdělit na stejně velké podíly, můžeme použít jeden ze standardně používaných způsobů dělení, které jsou: Medián Kvantil rozdělující statistický soubor na dvě stejně početné množiny se nazývá medián, tzn. jedná se o kvantil \\(Q_{0.5}\\). Tercil Dva tercily rozdělují statistický soubor na třetiny. 1/3 prvků má hodnoty menší nebo rovné hodnotě prvního tercilu \\(Q_{1/3}\\), 2/3 prvků mají hodnoty menší nebo rovné hodnotě tercilu druhého \\(Q_{2/3}\\). Kvartil Tři kvartily rozdělují statistický soubor na čtvrtiny. 25 % prvků má hodnoty menší než dolní kvartil \\(Q_{0.25}\\) a 75 % prvků hodnoty menší než horní kvartil \\(Q_{0.75}\\); někdy se označují \\(Q_1\\) a \\(Q_3\\). Kvintil Čtyři kvintily dělí statistický soubor na pět stejných dílů. 20 % prvků souboru má hodnoty menší (nebo rovné) hodnotě prvního kvintilu, 80 % hodnoty větší (nebo &gt;rovné). Decil Decil dělí statistický soubor na desetiny. Jako k-tý decil označujeme \\(Q_{10/k}\\). Percentil Percentil dělí statistický soubor na setiny. Jako k-tý percentil označujeme \\(Q_{100/k}\\). Používá se například při vyhodnocení testů: Pokud má účastník umístění na 85. percentilu, znamená to, že 85 % účastníků mělo horší výsledek (a 15 % účastníků je lepších nebo stejných jako on [včetně jeho samého]). Znamená to, že účastník s nejlepším umístěním nebude mít percentil 100 %, ale nižší (o část vyjadřující procento jeho vlastního &gt;podílu na výsledku). Percentil tak vypočteme: \\(PR = \\frac{CF - (0.5 * F)}{N} * 100\\). Kde PR je hodnota percentilu, CF je kumulativní počet výsledků a F je počet výskytů počítaného výsledku (percentilu). Zdroj: Wikipedia V R můžeme empirický kvantil vypočítat pomocí funkce quantile. V argumentu prob stanovíme jakou pravděpodobnost (podíl hodnot menších) chceme vypočítat. Například vypočítejme kvartily počtu slov. kvartil &lt;- quantile(pocet_slov, prob = c(0.25, 0.5, 0.75)) kvartil ## 25% 50% 75% ## 9.00 15.50 22.75 Vidíme, že 25% vět má méně než 9 slov a že 25% vět má více než 23 slov (nebo že 75% vět má méně než 23 slov). Někdy se hodnoty kvantilů používají k výpočtu mezikvantilových rozpětí tak, že se honodty kvantilů odečtou mezi sebou. Například mazikvartilové rozpětí se vypočítá jako \\(Q_{0.75} - Q_{0.25}\\). Pro relativní četnost slov v dokumentu se používá termín term-frequency nebo také “tf”. Více informací zde.↩︎ Očištěných o stopwords.↩︎ Při textové analýze bychom potom dále využili nástroje jako bag-of-words abychom získali pouze kořeny slov apod.↩︎ V případě populačního rozptylu \\(\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^n (x_i - \\mu_x)^2\\).↩︎ "],["prob-dist.html", "Kapitola 3 Pravděpodobnostní rozdělení 3.1 Pravděpodobnostní rozdělení jako jazyk statistiky 3.2 Binomické 3.3 Normální 3.4 T 3.5 Uniformní 3.6 Poisson 3.7 Chi-kvadrát 3.8 Jak vybrat správný model pro data", " Kapitola 3 Pravděpodobnostní rozdělení Tuto kapitolu začneme videm. Matemateca (IME USP) / name of the photographer when stated, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons Proč kuličky v tomto videu skončí v určitém množství v určitém sloupci? A proč skončí podobné množství kuliček v každém sloupci, pokud bychom kuličky pustili znova? Jak by se počet kuliček ve sloupcích lišil? Pokud si z této knihy máte odnést jednu kapitolu, pak by to měla být tato :). Jak jsme zmínili v úvodu knihy, většina proměnných okolo nás se chová náhodně. To znamená, že může nabývat náhodných hodnot podle nějakého klíče (procesu). Náhodnost může vycházet z faktu, že měříme jenom nějakou část populace (např. výběrové šetření), z fyzikálních vlastností (např. váha součástky vyrobená v továrně nebude vždy stejná) nebo z chyby měření (např. teploměr nezměří stejnou teplotu vždy stejně, ale hodnoty měření budou kolísat okolo nějakého čísla). Pravděpodobnostní rozdělení nám pomáhají kvantifikovat a predikovat míru nahodilosti. Typ pravděpodobnostního rozdělení, který na popsání náhodnosti uplatníme vychází z našeho porozumění vlastnostní rozdělení a jeho vhodnosti na daný problém. A vlastnosti pravděpodobnostních rozdělení je to, co si v této kapitole ukážeme. Na začátku jsme řekli, že budeme používat statistické modely a že tyto modely nejsou přesným vyjádřením reality, ale mohou být užitečným popsáním reality. V této kapitole začneme právě takové modely používat. 3.1 Pravděpodobnostní rozdělení jako jazyk statistiky V této kapitole si ukážeme spoustu nových značení, která se stanou našim jazykem, kterým budeme ve statistice komunikovat. Budeme je používat k tomu, abychom popsali očekávané chování náhodné proměnné. Každé rozdělení má svoje parametry, pomocí kterých ho můžeme popsat. Každé rozdělení má také svoji očekávanou hodnotu (typická hodnota rozdělení) a svůj roztyl (rozptýlenost okolo typické hodnoty). To, jakých hodnot a s jakou pravděpodobností náhodná proměnná nabývá, popisujeme pomocí pravděpodobnostní funkce (u diskrétních proměnných nabývajích celých čísel) nebo pomocí hustoty pravděpodobnosti (u spojitých proměnných nabývajích reálných čísel). V angličtině se pro hustotu pravděpodobnosti využívá pojem probability density function (PDF) a pro pravděděpodobnostní funkci pojem probability mass function (PMF). Integrál hustoty pravděpodobnosti/pravděpodobnostní funkce je vždy rovný jedné. To tedy znamá, že kdybychom sečetli všechny hodnoty PMF/PDF (pro spojité proměnné integrovali), tak by se výsledek rovnal jedné. To plyne z toho, že pravděpodobnost jevu nemůže být vyšší než jedna. Nakonec ještě trocha terminologie, které budeme používat. Náhodnou proměnnou nabývajících konkrétních hodnot budeme označovat jako \\(x_i\\). Budeme používat malá písmena, pokud výsledkem procesu bude vektor a velká písmena, pokud výsledkem bude matice. \\(i\\) označuje jednotlivá pozorování. Teda první hodnota náhodné proměnné, by se označovala jako \\(x_1\\). \\(n\\) označuje zpravidla počet pozorování proměnné. 3.2 Binomické Vraťme se k videu ze začátku kapitoly. Tomuto přístroji se říká Galton Box. Kuličky jsou puštěny do přístroje z jednoho stejného bodu a procházejí několika vrstvami (ve videu 10 vrstvami). V každé vrstvě míček narazí na bod, který ho může poslat na levou nebo na pravou stranu (zhruba se stejnou pravděpodobností 0.5). To, na jakou stranu se míček vydá je určeno náhodou. Kuličky potom spadnou do jednoho ze sloupců (ve videu 13 sloupců). Kdybysme nechali všechny kuličky spadnout, posbírali je a znovu spustili, tak skoro stejné množství kuliček skončí v každém sloupci. Jak je možné, že když spustíme tisíce kuliček, každý se může 10x odrazit nalevo nebo napravo, tak výsledný počet kuliček v každém sloupci je skoro stejný? Příčinou je pravděpodovnoství rozdělení. Prvním rozdělením, které si ukážeme je binomické rozdělení. Jevy mohou být generovany procesem, který vede k binomickému rozdělení, pokud máme \\(n\\) pokusů, jejichž výsledkem je úspěch nebo neúspěch, pokusy jsou nezávislé a mají konstantní pravděpodobnost úspěchu \\(p\\) (konstantní pro všechny pokusy). Obecně platí, že proměnná \\(x_i\\) pochází z binomického rozdělení, kde \\[x_i \\sim B(n, p)\\] a kde \\(n\\) značí počet pokusů a \\(p\\) pravděpodobnost úspěchu. Proměnná \\(x_i\\) je potom diskrétní. Pojďme si ukázat, jak se binomické rozdělení vztahuje ke Galtonově boxu. Každý míček prochází nejméně 10 pokusy, kde výsledek může být buď úspěch (řekněme, že míček spadne napravo) nebo neúspěch (řeknemě, že míček spadne nalevo). Výsledkem je potom zařazení do jednoho ze 13 sloupců. V extrému může náš proces skončit tak, že všechny kuličky budou nalevo nebo že všechny kuličky budou napravo. Kuličky jsou na sobě relativně nezávislé a všechny mají při každém pokusu relativně konstatní pravděpodobnost. Opět je nutné si uvědomit, že náš model dat (binomické rozdělení) není přesnou reprezentací procesu, který se snažíme popsat. Kuličky nejsou ve skutečnosti kompletně nezávislé, mohou do sebe narazit a tím se ovlivnit. Protože ale prochazí zhruba 10 pokusy (i když to se může lišit, některé kuličky se mohou odrazit po nárazu znovu nahoru), bereme proces jako dostatečně náhodný a nezávislý. Důležité je, jestli jsou předpoklady našeho modelu splněny dostatečně na to, aby jeho matematické vyjádření bylo validní pro reálný process, který se snažíme popsat. Pojďme si nyní ukázat, jak se Galtonův box dá připodobnit binomickým rozdělením. Náhodný process z binomického rozdělení můžeme generovat pomocí funkce rbinom. Ta jako argumenty předpokládá n počet pozorování, která chceme generovat, size počet pokusů (způsobů, jak může proces skončit) a p pravděpodobnost úspěchu. Budeme uvaživat, že spustíme 10 000 kuliček, které mohou skončit v 13 sloupcích, tedy \\[x_i \\sim B(12, 0.5)\\] set.seed(4) # pocet micku n &lt;- 10000 # pravdepodobnost uspechu (napravo) p &lt;- 0.5 # 0 by znamenalo všechny kulicky nalevo, 12 všechny kulicky napravo, # celkem 13 možností, jak mohou kulicky skončit s &lt;- 12 vysledek &lt;- rbinom(n = n, size = s, prob = p) cetnost &lt;- table(vysledek) plot(cetnost, xlab = &quot;x&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;Galtonův box s &quot;, n, &quot; kuličkami&quot;), col = &quot;#1f77b4&quot;, type = &quot;h&quot;, lwd = 15, xlim = c(0, 12) ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) Graf 3.1: Simulace Galtonova boxu s 13 sloupci Očekávanou hodnotu proměnné pocházející z binomické proměnné můžeme vypočítat jako \\[E(x_i) = np\\] rozptyl jako \\[Var(x_i) = npq\\] a směrodatbou odchylku jako \\[Sd(x_i) = \\sqrt{Var(X_i)}\\]V našem případě, kdy máme 12 možností, jak mohou kuličky skončit a očekávaná hodnota je tedy 6 a směrodatná odchylka 1.7320508. A ještě ukázka, že počet kuliček v každém sloupci je zhruba stejný, pokud bychom je pustili znova, řekněme 11x. N &lt;- 11 for (i in 1:N) { vysledek &lt;- rbinom(n = n, size = s, prob = p) micek &lt;- vysledek[1] cetnosti &lt;- table(vysledek) # pouzijeme funkci plot, abychom mohli pridat micek plot(cetnosti, xlab = &quot;x&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;Galtonův box s &quot;, n, &quot; kuličkami; pokus #&quot;, i), col = &quot;#1f77b4&quot;, lwd = 15, ylim = c(0, 2500), xlim = c(0, 12) ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) points(x = micek, y = 2, pch = 19, col = &quot;red&quot;) } Graf 3.2: Opakované spuštění kuliček v Galtonově boxu Jak jsme zmínili v 3.1 frekvenci hodnot diskrétní náhodné proměnné můžeme popsat matematicky pomocí pravděpodobnostní funkce (PMF). U binomického rozdělení můžeme PMF vypočítat jako \\[PMF(x_i) = \\binom{n}{k}p^kq^{n-k}\\] kde \\(k\\) je počet úspěšných pokusů a \\(q=1-p\\). V praxi můžeme využít funkce dbinom, která PMF vypočítá. Tato funkce argument x kam dosadíme hodnoty proměnné \\(x_i\\), pro které chceme PMF vypočítat. Argumenty size a prob jsou stejné jako u rbinom. Protože PMF je vypočítáno deterministicky (není tam žádná náhoda jako u generování náhodných čísel nahoře, čísla pouze dosadíme do vzorce), bude jeho výpočet stejný pokaždé, když zadáme stejné argumenty funkce. # muze padnou minimalne 0 (uplne nalevo) a maximalne 12 (uplne napravo) x &lt;- 0:12 s &lt;- 12 p &lt;- 0.5 pmf &lt;- dbinom(x = x, size = s, prob = p) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;Galtonův box&quot;), col = &quot;#1f77b4&quot; ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) Graf 3.3: PMF pro Galtonův box s 13 sloupci Protože hodnoty \\(x_i\\) mohou nabývat pouze celých čísel, je PMF vyjádřením pravděpodobnosti toho, že náhodná proměnná \\(x_i\\) nabyde nějaké hodnoty. Nejpravděpodobněji skončí kulička ve sloupci 6 s pravděpodobností 0.23, nebo matematickým zápisem \\(P(x_i=\\) 6 \\()=\\) 0.23. Pravděpodobnost, že kulička skončí v 4 sloupci zleva je \\(P(x_i = 3)\\)=0.0537109. To, že dokážeme predikovat, kolik kuliček skončí v jakém sloupci, ještě neznamená, že víme, kde skončí jedna konkrétní kulička. V animaci 3.2 si všimněte červené tečky. Ta reprezentuje první kuličku, kterou jsme vhodili. Jak je vidět kulička cestuje mezi sloupci. To s jakou pravděpobností skončí v daném sloupci nám říká právě PMF. Stejně tak můžeme spočítat pravděpodobnost, že skončí v 7 sloupci a více napravo, tedy \\(P(x \\ge 7)\\)=0.39. Tomuto typu úlohy, kdy nás zajímá, zda náhodná proměnná \\(x_i\\) bude mít hodnotu menší/větší než nějaká hodnota kvantilu \\(q\\), říkáme kumulativní pravděpodobnosti (anglicky cumulative distribution function CDF). Matematicky bychom tento typ úlohy označili jako \\(P(x_i \\le q)\\) pokud by nás zajímala pravděpodobnost, že náhodná proměnná \\(x_i\\) bude menší nebo rovna než nějaká hodnota \\(q\\) a \\(P(x_i &gt; q)\\), pokud by nás zajímala pravděpodobnost, že hodnota náhodné proměnné \\(x_i\\) bude větší než nějaká hodnota \\(q\\). R můžeme na výpočet CDF u binomického rozdělení použít funkci pbinom. Jako argumenty očekává tato funkce q, hodnota náhodné proměnné vůči které porovnáváme (tedy kvantil), size a prob mají stejný význam jako u funkcí nahoře a argument lower.tail vyjadřující, zda chceme zjistit pravděpodobnost, že \\(x_i\\) bude menší rovno nebo větší než \\(q\\). Pokud má argument lower.tail hodnotu TRUE, pak počítáme \\(P(x_i \\le q)\\), pokud má hodnotu FALSE, pak počítáme \\(P(x_i &gt; q)\\). Pojďme si ukázat různé výpočty a jejich zobrazení na grafu. Začneme \\(P(x &gt; 7)\\). # P(x &gt; 7) p_x7 &lt;- pbinom(q = 7, size = s, prob = p, lower.tail = FALSE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &gt; 7)=&quot;, round(p_x7, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &gt; 7], pmf[x &gt; 7], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.4: Ukázka výpočtu P(x &gt; 7) pomocí PMF Dále si ukažme jak vypočítat \\(P(x \\le 5)\\). # P(x &lt;= 5) p_x5 &lt;- pbinom(q = 5, size = s, prob = p, lower.tail = TRUE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &lt;= 5)=&quot;, round(p_x5, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &lt;= 5], pmf[x &lt;= 5], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.5: Ukázka výpočtu P(x &lt;= 5) pomocí PMF A nakonec \\(P(x_i \\ge 6)\\). # P(x &gt;= 6) # pouzijeme FALSE na vypocet upper tail a za 1 dosadime 5, protoze... # ...lower.tail = FALSE pocita P(x_i &gt; q) p_x6 &lt;- pbinom(q = 5, size = s, prob = p, lower.tail = FALSE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &gt;= 6)=&quot;, round(p_x6, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &gt;= 6], pmf[x &gt;= 6], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.6: Ukázka výpočtu P(x &gt;= 6) pomocí PMF 3.3 Normální Normální rozdělení popisuje náhodný proces spojitých proměnných. Za určitých podmínek (velký výběr a \\(p\\) neblížící se 0 nebo 1) se binomické rozdělení bude podobat normálnímu. Normální rozdělení popisuje proces, kterým jsou generované náhodné spojité proměnné, které nejsou ohraničené zleva ani zprava. Jednotlivá pozorování jsou potom opět na sobě nezávislá. Protože mnoho fyzikálních jevů je spojitých5 a nejsou ohraničeny6, normální rozdělení se často vyskuteje v přírodě. Normální rozdělení má dva parametry, které určují jeho tvar, průměr \\(\\mu\\) a směrodatnou odchylku \\(\\sigma\\). Náhodná proměnná \\(x_i\\) pocházející z normálního rozdělení se zapíše jako \\[x_i \\sim N(\\mu, \\sigma)\\]Parameter \\(\\mu\\) ovlivňuje polohu rozdělení a parameter \\(\\sigma\\) jeho roztaženost nalevo a napravo od \\(\\mu\\). Protože náhodná proměnná \\(x_i\\) je spojitá, nabývá reálných čísel, může mít nekonečně mnoho hodnot. To představuje problém, pokud chceme vypočítat hustotu pravděpodobnosti (PDF). Aby byly zachovány vlastnosti pravděpodobnosti (\\(p \\in [0, 1]\\)), je integrál PDF rovný jedné. To ale znamená, že hodnota PDF už nevyjadřuje pravděpodobnost nějakého jevu. V angličtině se proto používá pojem density (odtud probability density function PDF). Vzorec pro PDF je \\[PDF(x_i) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\] v R můžeme pro výpočet PDF normálného rozdělení použít funkci dnorm, které má argumenty x hodnoty náhodné proměnné \\(x_i\\), pro které chceme PDF vypočítat, mean průměr a sd směrodatnou odchylku. Pojďme si ukázat, jak by vypadala PDF pro tyto náhodné proměnné: \\[x_i \\sim N(0, 1)\\] \\[y_i \\sim N(3, 2)\\] \\[z_i \\sim N(-2, 3)\\] # definujeme si parametry mu &lt;- c(0, 3, -2) s &lt;- c(1, 2, 3) # definuje hodnoty nahodne promenne, pro kterou budeme chtit vypocitat pdf x &lt;- seq(-12, 12, length.out = 1000) # vypocitame pdf pdf_x &lt;- dnorm(x = x, mean = mu[1], sd = s[1]) pdf_y &lt;- dnorm(x = x, mean = mu[2], sd = s[2]) pdf_z &lt;- dnorm(x = x, mean = mu[3], sd = s[3]) # zobrazime plot(x, pdf_x, col = &quot;#1f77b4&quot;, lwd = 2, type = &quot;l&quot;, xlab = &quot;&quot;, ylab = &quot;PDF&quot;, xlim = c(-12, 12) ) lines(x, pdf_y, col = &quot;black&quot;, lwd = 2) lines(x, pdf_z, col = &quot;orange&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;x ~ N(0, 1)&quot;, &quot;y ~ N(3, 2)&quot;, &quot;z ~ N(-2, 3)&quot;), lwd = rep(2, 3), col = c(&quot;#1f77b4&quot;, &quot;black&quot;, &quot;orange&quot;), cex = 0.7 ) Graf 3.7: Ukázka vlivu průměru a směrodatné odchylky na tvar normálního rozdělení Normální rozdělení je symetrické. To znamená, že se nachází stejně hodnot nalevo a napravo od průměru. Podle hodnoty směrodatné odchylky dokážeme určit kolik případů bychom a jak daleko bychom očekávali, že budou ležet od průměru. Obrázek 3.8 ukazuje, že zhruba 68% hodnot náhodné proměnné bude ležet +/- jednu směrodatnou odchylku od průměru a zhruba 95% hodnot bude ležet +/- dvě směrodatnou odchylku od průměru a 99% hodnot +/- tři směrodatné odchylky od průměru. Graf 3.8: M. W. Toews, CC BY 2.5 https://creativecommons.org/licenses/by/2.5, via Wikimedia Commons Očekávanou hodnotou normálního rozdělení je průměr, tedy \\(E(x_i)=\\mu\\) a rozptylem je \\(var(x_i) = \\sigma^2\\), tedy směrodatnou odchylkou je \\(\\sigma = \\sqrt{\\sigma^2}\\). V dalších několika řádcích si představíme některé základní početní operace s náhodnými proměnnými pocházejícími z normálního rozdělení. Pojďme se nejdříve podívat, co se stane s funkcí normálního rozdělení, pokud k náhodné proměnné \\(x_i\\) přičteme skalár. Podíváme se na následující proměnnou \\(h_i \\sim N(178, 8)\\), tedy proměnnou, která pochází z normálního rozdělení s průměrem 178 a směrodatnou odchylkou 8. K této náhodné proměnné přičteme 20, tedy \\(h&#39;_i = h_i + 20\\) a spočítáme průměr a směrodatnou odchylku. Ke generování náhodných čísel z normálního rozdělení použijeme funkci rnorm, který má argumenty n počet pozorování, které chceme generovat, mean průměr rozdělení, z kterého chceme generovat a sd směrodatnou odchylku rozdělení, z kterého chceme generovat. n &lt;- 1e5 h &lt;- rnorm(n, mean = 178, sd = 8) h. &lt;- h + 20 par(mfrow = c(2, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(130, 240), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(h.), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(130, 240), main = paste0( &quot;h&#39;~N(&quot;, round(mean(h.), 0), &quot;, &quot;, round(sd(h.), 0), &quot;)&quot; ) ) Graf 3.9: Efekt přičtení skalaru na rozdělení proměnné Jak je vidět z 3.9 přičtení (nebo odečtění) má vliv pouze na průměr a tedy polohu rozdělení. V případě přičtení čísla \\(a\\) se celé rozdělení posune doprava o \\(a\\), v případě odečtení se potom posune celé rozdělení doleva. Přičtení/odečtení nemá vliv na směrodatnou odchylku, tedy na šířku rozdělení od průměru. Tento fakt můžeme zapsat jako \\[h&#39;_i \\sim N(\\mu_h \\pm a, \\sigma_h)\\] Dále se podíváme na to, co se stane s rozdělením stejné náhodné proměnné \\(h_i \\sim N(178, 8)\\) pokud ji vynásobíme skalarem. Náhodnou proměnnou \\(h_i\\) vynásobíme číslem 0.66, tedy \\(h&#39;_i = h_i * 0.66\\) a spočítáme průměr a směrodatnou odchylku. Jak je vidět z grafu 3.10 vynásobení (nebo jako v našem případě dělení) má vliv na průměr i směrodatnou odchylku. Oba parametry normálního rozdělení se změní o faktor \\(a\\) , kterým původní náhodnou proměnnou násobíme, tedy v našem případě se zmenší o třetinu. Tento fakt můžeme zapsat jako \\[h&#39;_i \\sim N(\\mu_h * a, \\sigma_h * a)\\] h. &lt;- h * 2 / 3 par(mfrow = c(2, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(80, 230), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(h.), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(80, 230), main = paste0( &quot;h&#39;~N(&quot;, round(mean(h.), 0), &quot;, &quot;, round(sd(h.), 0), &quot;)&quot; ) ) Graf 3.10: Efekt vynásobení skalarem na rozdělení proměnné Dále se podíváme, co se stane s průměrem a směrodatnou odchylkou, pokud k sobě přičteme dvě náhodné proměnné pocházející z normálního rozdělení. Budeme mít dvě náhodné proměnné, které pocházejí z normálního rozdělení \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(80, 10)\\). Budeme sledovat, co se stane s průměrem a směrodatnou odchylkou nové proměnné \\(k_i\\), která vznikne sečtením \\(h_i\\) a \\(l_i\\), tedy \\(k_i = h_i + l_i\\). Jak vidíme na grafu 3.11, nová proměnná má průměr rovný \\(\\mu_k = \\mu_h + \\mu_l\\) a \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\), tedy \\[k_i \\sim N(\\mu_h + \\mu_l, \\sqrt{\\sigma_h^2 + \\sigma_l^2})\\] l &lt;- rnorm(n = n, mean = 80, sd = 10) k &lt;- h + l par(mfrow = c(3, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(l), col = &quot;#1f77b4&quot;,lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) plot(density(k), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;k&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.11: Efekt přičtení dvou normálně rozdělených náhodných proměnných na rozdělení nové proměnné Jako další výpočetní operaci náhodných proměnných pocházejících z normálního rozdělení si ukážeme, co se stane pokud od sebe odečteme dvě náhodné proměnné pocházející z normálního rozdělení. Budeme opět uvažovat náhodné proměnné \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(80, 10)\\) a budeme počítat \\(k_i = h_i - l_i\\). Jak je vidět z grafu 3.12, nový průměr má hodnotu \\(\\mu_k = \\mu_h - \\mu_l\\), ale směrodatná odchylka je stále rovna \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\). To je důležitý poznatek o odečtu dvou normálně rozdělených proměnných. Směrodatná odchylka nové proměnné tedy bude vždy součtem směrodatných odchylek původních proměnných, protože rozptyl v datech se sčítá (jinak bychom mohli dosáhnout negativní \\(\\sigma\\), což není definováno, protože \\(\\sigma \\in [0, \\infty)\\). Tento jev se dá zapsat jako \\[k_i \\sim N(\\mu_h - \\mu_l, \\sqrt{\\sigma_h^2 + \\sigma_l^2})\\] k &lt;- h - l par(mfrow = c(3, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(l), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) plot(density(k), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;k&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.12: Efekt odečtení dvou normálně rozdělených náhodných proměnných na rozdělení nové proměnné Je důležité zdůraznit, že zmíněná pravidla o rozptylu platí pouze za předpokladu, že jsou obě proměnné nezávislé. V grafu 3.13 ukazujeme příklad odečtení proměnné \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(h_i, 10)\\). \\(l_i\\) je tedy na \\(h_i\\) zavislá, protože její průměr je roven \\(h_i\\). Vypočítáme novou proměnnou \\(k_i=h_i - l_i\\). Jak jsme ukázali nahoře, pokud by byly proměnné nezávislé, byla by směrodatná odchylka nové proměnné rovna \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\). Protože jsou ale proměnné nezávislé, bude směrodatná odchylka nové proměnné rovna \\(\\sigma_k = \\sqrt{\\frac{(d_i - \\overline{d_i})^2}{n}}\\), kde \\(d_i = h_i - l_i\\). # abychom dosahli l~N(h_i, 10)m pak pro rnorm zvolime sd=6... # ...protoze sd nove promenne bude rovne sqrt(sd(h) + 6^2) l &lt;- rnorm(n = n, mean = h, sd = 6) k &lt;- h - l par(mfrow = c(3, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(-30, 230), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(l), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(-30, 230), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) plot(density(k), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;k&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(-30, 230), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.13: Efekt odečtení dvou závislých normálně rozdělených náhodných proměnných na rozdělení nové proměnné Poslední výpočetní operaci, kterou si ukážeme náhodnou proměnnou standardizuje tak, že její průměr bude rovný nule a směrodatná odchylka bude rovná jedné. Tento typ transformace je možné provést s proměnnou pocházející z jakéhokoliv rozdělení, ale my si ho zmiňujeme v kontextu normálního rozdělení, protože zde jej budem vídat nejčastěji. Tuto transformaci lze provést následujícím způsobem: \\[z_i = \\frac{(x_i - \\mu_x)}{sd(x_i)}\\]přičemž tedy platí, že pokud pochází \\(x_i\\) z normálního rozdělení, tak \\(z_i \\sim N(0, 1)\\)7. Pojďme si ukázat, jak by tato transofrmace vypadala pro náhodnou proměnnou \\(x_i \\sim N(178, 8)\\). Vzhledem k vlastnostem normálního rozdělení je tedy pro standardizovanou náhodnou proměnnou pocházející z normálního rozdělení snadné vypočítat kvantily (viz 3.8): \\(z_{0.023} = -2\\) \\(z_{0.159} = -1\\) \\(z_{0.5} = 0\\) \\(z_{0.841} = 1\\) \\(z_{0.977} = 2\\) z &lt;- (h - mean(h)) / sd(h) par(mfrow = c(2, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, # urcime intrval od - 4 sd do + 4 sd xlim = c(178 - 4 * 8, 178 + 4 * 8), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(z), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, # urcime intrval od - 4 sd do + 4 sd xlim = c(-4, 4), main = paste0( &quot;z~N(&quot;, round(mean(z), 0), &quot;, &quot;, round(sd(z), 0), &quot;)&quot; ) ) Graf 3.14: Standardizovaná proměnná pocházející z normálního rozdělení Jak jsme zmínili, protože spojitá proměnná může nabývat nekonečně mnoho hodnot, nevyjadřuje PDF v nějakém konkrétním bodě pravděpodobnost tak, jako tomu je u PMF (např. u binomického rozdělení). Platí ale, že integrál PDF je rovný jedné. To znamená, že můžeme použít kumulativní pravděpodobnost (CDF) k výpočtu \\(P(x_i &gt; q)\\) nebo \\(P(x_i \\le q)\\). Funkce na výpočet CDF se jmenuje pnorm a stejně jako pbinom má argument q, který vyjadřuje hodnotu náhodné proměnné vůči které porovnáváme (kvantil), parametr lower.tail, který opět vyjadřuje pro jaký konec rozdělení chceme pravděpodobnost vypočítat (TRUE \\(P(x_i \\le q)\\), FALSE \\(P(x_i &gt; q)\\)) a potom parametry normálního rozdělení, tedy mean a sd, které udávají tvar rozdělení a mají stejný význam jako ve funkci rnorm a dnorm. Vraťme se zpátky k náhodné proměnné \\(h_i \\sim N(178, 8)\\). Ta představuje rozdělení výšky mužů v dospělé populaci. Řekněme, že by nás zajímalo, jaká je pravděpodobnost, že muž pocházející z této populace bude 185cm a vyšší, tedy \\(P(x_i \\ge 185)\\). Pro úplnost si opět tuto pravděpodobnost8 zobrazíme v grafu. x &lt;- seq(145, 210, length.out = 1000) q &lt;- 185 # vypoceteme si PDF, abychom ji mohli zobrazit v grafu pdf &lt;- dnorm(x = x, mean = 178, sd = 8) cdf &lt;- pnorm(q = q, mean = 178, sd = 8, lower.tail = FALSE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;h&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &gt;= &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &gt;= q], pdf[x &gt;= q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.15: Ukázka výpočtu P(x &gt;= 185) pomocí PDF Pro úplnost, ještě vypočítáme příklad kdy nás zajímá pravděpodobnost, že muž pocházející z této populace bude menší než 164cm, tedy \\(P(x_i &lt; 164)\\). q &lt;- 164 cdf &lt;- pnorm(q = q, mean = 178, sd = 8, lower.tail = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;h&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &lt; &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &lt; q], pdf[x &lt; q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.16: Ukázka výpočtu P(x &lt; 164) pomocí PDF 3.4 T Dalším z rozdělení, které si představíme je tzv. t-rozdělení, nebo také někdy nazývané studentovo. Toto rozdělení vychází z normálního rozdělení, ale má větší varibilitu. PDF normálního rozdělení rapidně klesá se vzdáleností od průměru (viz 3.8). Pokud potřebujeme popsat jev, který má více hodnot vzdálených od průměru, než bychom čekali u normálního rozdělení, je toho možné dosáhnout pomocí t-rozdělení. T-rozdělení je automaticky standardizované na \\(\\mu = 0\\) a má pouze jeden paramater - stupně volnosti (degrees of freedom) \\(\\nu\\), který určuje jak pravděpodobné jsou hodnoty více vzdálené od průměru. Čím je \\(\\nu\\) menší, tím je rozdělení více široké a naopak. Pokud je \\(\\nu\\) větší než 50, je t-rozdělení skoro totožné se standardizovaným normálním rozdělením \\(N(0, 1)\\). Pojďme si nyní ukázat PDF při různých stupních volnosti \\(\\nu\\) a jejich podobnost k \\(N(0, 1)\\). Ukážeme si \\(T(2)\\), \\(T(4)\\), \\(T(8)\\), \\(T(16)\\), \\(T(34)\\) a \\(T(64)\\). Jak je vidět z grafu 3.17, s rostoucím počtem stupňů volnosti se t-rozdělení přibližuje \\(N(1,0)\\), \\(T(64)\\) už skoro překrývá křivku PDF pro \\(N(1,0)\\) # stupne volnost sv &lt;- 2^c(1:6) # x pro ktere pocitame pdf x &lt;- seq(-5, 5, length.out = 1000) pdf_n &lt;- dnorm(x, mean = 0, sd = 1) pdf_t &lt;- lapply(sv, dt, x = x) # vybereme barvy pouzijeme nejsvetlejsi pro vetsi sv... # ...proto funkce rev, ktera obrati hodnoty cols &lt;- rev(RColorBrewer::brewer.pal(n = length(sv), name = &quot;Blues&quot;)) # zobrazime pdf N(1,0) plot(x, pdf_n, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, type = &quot;l&quot;, lwd = 3, col = &quot;black&quot; ) # zobrazime pdf vsech T for (i in 1:length(sv)) { lines(x, pdf_t[[i]], type = &quot;l&quot;, lwd = 2, col = cols[i]) } legend(&quot;topright&quot;, legend = c(&quot;N(0,1)&quot;, paste0(&quot;T(&quot;, sv, &quot;)&quot;)), col = c(&quot;black&quot;, cols), lwd = rep(2, length(sv) + 1), cex = 0.7 ) Graf 3.17: Podobnost t-rozdělení k N(1,0) K výpočtu PDF t-rozdělení jsme použili funkci dt, která má argument x hodnoty proměnné x, pro které chci PDF vypočítat a df, počet stupňů volnosti. Pokud bychom chtěli generovat náhodnou proměnnou z t-rozdělení, můžeme tak učinit pomocí funkce rt. Stejně tak CDF t-rozdělení můžeme vypočítat pomocí funkce pt. Očekávaná hodnota t-rozdělení je 0, tedy \\(E(x_i) = 0\\) a rozptyl je roven \\(var(x_i) = \\frac{\\nu}{\\nu-2}\\). R obsahuje pouze tradiční t-rozdělení, které je definováno pouze stupni volnosti \\(\\nu\\). Pokud bychom chtěli pomocí t-rozdělení popsat rozdělení, která mají jiný průměr a rozptyl, museli bychom t-rozdělení zobecnit. Pokud bychom tedy chtěli stanovit t-rozdělení s průměrem \\(\\mu\\) a směrodatnou odchylkou \\(\\sigma\\) pro náhodnou proměnnou \\(x_i\\), pak bude platit \\[x_i = \\mu + \\sigma T\\]Očekávaná hodnota náhodné proměnné pocházející z zobecněného t-rozdělení je \\(E(x_i) = \\mu\\) a rozptyl \\(var(x_i) = \\sigma^2 \\frac{\\nu}{\\nu-2}\\). PDF tohoto rozdělení bychom vypočítali jako: \\[PDF(x_i) = \\frac{1}{\\sigma} PDF_T(\\frac{x_i - \\mu}{\\sigma}, \\nu)\\]Všimněme si, že vlastně počítáme PDF t-rozdělení pro standardizovanou proměnnou a toto PDF násobíme faktorem \\(\\frac{1}{\\sigma}\\). Na grafu 3.18 ukazuje příklad PDF zobecněného t-rozdělení pro rozdělení s průměrem 178, směrodatnou odchylkou 8 a 5 stupni volnosti. Náhodnou proměnnou \\(x_i\\) pocházející z tohoto rozdělení bychom mohli zapsat jako \\(x_i \\sim T(178, 8, 5)\\). Jak je vidět menší stupně volnosti znamenají, že hodnoty více vzdálené od průměru jsou u takové náhodné proměnné více pravděpodobné, než u normálního rozdělení. # definujeme si parametry rozlozeni mu &lt;- 178 s &lt;- 8 sv &lt;- 5 # vytvorime x pro ktere chceme vypocitat pdf x &lt;- seq(178 - 5 * s, 178 + 5 * s, length.out = 1000) # vypocteme pdf N(178,8) pro porovnani pdf_n &lt;- dnorm(x, mean = mu, sd = s) # vypocteme pdf T(178,8,5) pdf_t &lt;- dt((x - mu) / s, df = 5) pdf_t. &lt;- 1 / s * pdf_t plot(x, pdf_n, type = &quot;l&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, col = &quot;grey&quot; ) lines(x, pdf_t., type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot; ) legend(&quot;topright&quot;, legend = c(&quot;N(178,8)&quot;, &quot;T(178,8,5)&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) Graf 3.18: Zobecněné t-rozdělení Pro úplnost ještě dodáme, že CDF náhodné proměnné \\(x_i\\) zobecněného t-rozdělení se vypočítá jako \\[CDF(x_i) = CDF_T(\\frac{x_i - \\mu}{\\sigma}, \\nu)\\]Zajímá nás jaká je pravdědobnost, zda hodnota náhodné proměnné pocházející z \\(x_i \\sim T(178,8,5)\\) bude mít hodnotu větší nebo rovnou než 185, tedy \\(P(x_i \\ge 185)\\). Porovnejte tuto pravděpodobnost se stejným výpočtem pro náhodnou proměnnou pocházející z \\(N(178,8)\\). q &lt;- 185 cdf_t. &lt;- pt((q - mu) / s, df = sv, lower.tail = FALSE) plot(x, pdf_t., type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &gt;= &quot;, q, &quot;)=&quot;, round(cdf_t., 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &gt;= q], pdf_t.[x &gt;= q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.19: CDF zobecněného t-rozdělení pro P(x &gt;= 185) 3.5 Uniformní Uniformní rozdělení popisuje náhodný proces, v kterém mají všechny hodnoty v nějakém intervalu stejnou pravděpodobnost, že budou vybrány. Uniformní rozdělení je definováno jak pro diskrétní, tak pro spojité proměnné. S diskrétním uniformním rozdělením jsme se setkali minulý rok při hodu kostkou nebo hodu mincí. Pokud náhodná proměnná \\(x_i\\) pochází z uniformního rozdělení, pak platí, že \\[x_i \\sim U(a,b)\\]Uniformní rozdělení je tedy možná vyjádřit pomocí dvou parametrů \\(a\\) a \\(b\\), které vyjadřují minimální a maximální možné hodnoty proměnné. PMF diskrétního uniformního rozdělení rozdělení lze vyjádřit jako \\[PMF(x_i) = \\frac{1}{n}\\]U spojitého uniformního rozdělení lze PDF vyjádřit jako \\[PDF(x_i) = \\frac{1}{b-a}; x_i \\in [a,b]\\]Pokud je hodnota \\(x_i\\) mimo \\([a,b]\\), pak je jeho PDF rovná nule (nebo jinak taková hodnota není možná). Očekávaná hodnota diskrétního i spojitéhouniformního rozdělení se spočítá jako \\(E(x_i) = \\frac{1}{2}(a+b)\\). Rozptyl u spojitého uniformního rozdělení se spočítá jako \\(Var(x_i) = \\frac{1}{12}(b-a)^2\\) a u diskrétního jako \\(Var(x_i) = \\frac{n^2-1}{12}\\). Protože generování čísel z diskrétního uniformního rozdělení můžeme použít funkci sample, kterou jsme používali při simulacích některých klasických pravděpodobnostních problémů. Pro generování hodnot náhodné proměnné pocházející ze spojitého uniformního rozdělení můžeme použít funkci runif. PDF poté vypočítáme pomocí funkce dunif a CDF pomocí punif. Jako příklad uniformního rozdělení si ukážeme čekání na tramvaj. Řekněme, že přijdeme na zastávku a máme se rozhodnout, zda se nám vyplatí čekat nebo jít pěšky. Čekání na tramvaj v takovém případě můžeme modelovat (připodobnit) uniformním rozdělením. Nevíme, kdy tramvaj přijede (v našem příkladu nejsou jízdní řády, nebo alespoň nejsou spolehlivé jízdní řády :). Může přijet hned (tedy za 0min), může přijet za 8min. Pravděpodobnost, za jak dlouho přijede (respektive, v jakém momentu jsme my přisli na zastávku) je rovnoměrně rozdělena v celém intervalu. Pojďme si tento proces vyjádřit v animaci. Červená tečka reprezentuje nás čekající na zastávce. Modrý čtverec reprezentuje tramvaj blížící se k zastávce. Jak je vidět, někdy přijdeme na zastávku a tramvaj je blízko. Někdy přijdeme a tramvaj je stále daleko. # muzeme cekat 0 minut az 8 minut a &lt;- 0 b &lt;- 8 # pocet simulaci N &lt;- 5 # kde bude tram, kdyz prijdeme na zastavku x &lt;- seq(0, 8, by = 0.5) # pro kazdy pokus for (n in 1:N) { # vybereme nahodne jak daleko od nas tram bude ... # ...-0.5 protoze pricitame +0.5 v prvnim kroku tramvaj &lt;- runif(1, min = a, max = b) - 0.5 my &lt;- 8 # dokud neni tramvaj u nas while (tramvaj &lt; my) { # zobrazime kazdych 0.5 minuty tramvaj &lt;- tramvaj + 0.5 # protoze pridavame kazdych 0.5 min ... # ...abychom zobrazili tramvaj u nas na zastavce if (tramvaj &gt; my) tramvaj &lt;- my plot(tramvaj, 1, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, xlim = c(-1, 9), ylim = c(0, 2), pch = 22, col = &quot;#1f77b4&quot;, main = paste0(&quot;Pokus: &quot;, n), sub = paste0(&quot;Zbývající doba čekání: &quot;, round(my - tramvaj, 1), &quot;min&quot;) ) # zobrazime nas points(my, 1, pch = 19, col = &quot;red&quot;) # otocime labely cekani 8-0 axis(1, at = x, labels = rev(x)) # pridame popisek text(4, 0, &quot;Vdálenost tramvaje od naší zastávky&quot;) legend(&quot;topright&quot;, legend = c(&quot;my&quot;, &quot;tramvaj&quot;), col = c(&quot;red&quot;, &quot;#1f77b4&quot;), pch = c(19, 22), cex = 0.7 ) } } Graf 3.20: Ukázka čekání na tramvaj jako náhodný proces z uniformního rozdělení Pěšky je to do našeho cíle 6min. Tramvají 1min. Tramvaj jezdí jednou za 8min, tedy \\(t_i \\sim U(0, 8)\\). Vyplatí se nám čekat? ocekavana_hodnota &lt;- (a + b) / 2 Očekávané hodnota našeho čekání je 4. Když k tomu přičteme 1min cesty tramvají, pak očekávaná hodnota naší cesty tramvají je 5min. Řekněme, že je velmi důležité, abychom nedorazili pozdě. Jaká je pravděpodobnost, že tramvají do cíle dorazíme za dobu delší než 6min (tedy, že nám bude cesta trvat déle, než pěšky)? x &lt;- seq(-1, 9, length.out = 1000) q &lt;- 5 pdf &lt;- dunif(x = x, min = a, max = b) cdf &lt;- punif(q, min = a, max = b, lower.tail = FALSE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;t&quot;, ylab = &quot;PDF&quot;, xlim = c(-1, 9), main = paste0(&quot;P(x &gt; &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # type &quot;h&quot;, abychom vyjadrili integral lines(x[x &gt; q], pdf[x &gt; q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.21: Příklad PDF pro t ~ U(0, 8) a CDF pro P(t &gt; 5) Jak je vidět, pravděpodobnost, že tramvaj přijede za 5 a více minut (musíme si nechat 1 minutu na cestu tramvají, proto chceme vědět \\(P(t_i &gt; 5)\\)) je 0.38, tedy docela vysoká. Tato situace by nastala zhruba 1 ze 3 případů. Pravděděpodobně bychom tedy v takovém případě šli pěšky. 3.6 Poisson Poissonovo rozdělení popisuje chování diskrétních náhodných proměnných v nějakém daném časovém intervalu. Od binomického rozdělení se liší tím, že popisuje proměnné, které nemají pevně stanovený počet pozorování, tedy \\(x_i \\in [0, \\infty)\\). Toto rozdělení má jediný parametr \\(\\lambda\\), který je zároveň očekávanou hodnotou a rozptylem. Stejně jako u binomického, normálního, t a uniformního rozdělení předpokládáme, že jednotlivé hodnoty náhodné proměnné jsou na sobě nezávislé. Náhodnou proměnnou \\(x_i\\) pocházející z tohoto rozdělení značíme jako \\(x_i \\sim Poisson(\\lambda)\\). Mnoho každodenních jevů, jejichž hodnoty jsou celé čísla se dají popsat poissonových rozdělením. U mnoha jevů totiže neznáme \\(n\\) a tedy nedokážeme říct, jaká je největší očekávaná hodnota. Jako příklady jevů, které je možné approximovat (připodobnit) poissonových rozdělením je například počet telefonátů za den, počet aut, které projedou silnicí za hodinu, počet gólů za zápas apod. Vrátíme se zpět do kapitoly popisné statistiky 2.1, kde jsme vypočítali počet slov ve větě v projevu prezideta republiky. Pro připomenutí ukážeme četnosti počtu slov ve větě. Graf 3.22: Četnost počtu slov ve větě v projevu prezidenta republiky Řekněme, že bychom chtěli predikovat počet slov ve větě v nějaké prezidentově dalším projevu a chtěli bychom to udělat tak, že bychom se snažili zjistit parametry procesu, který počet slov vyprodukoval. Víme, že minimálně můžeme mít ve větě jedno slovo. Maximální počet slov je sice prakticky ohraničen, ale my neznáme žádnou hranici, kterou bychom mohli uplatnit. Budeme tedy předpokládat, že ohraničený není9. Zároveň předpokládáme, že hodnoty náhodné proměnné vygenerované z poissonova rozdělení jsou nezávislé. V našem případě jsou hodnoty spíše silně závislé. Počet slov v jedné větě bude určitě ovlivňovat počet slov v následující(ch) větách. Jak je vidět na 3.23 po hodně dlouhých větách, častěji následují kratší věty. Málokdy je více krátkých vět za sebou a naopak. plot(pocet_slov, type = &quot;l&quot;, lwd = 2, ylab = &quot;Počet slov&quot;, xlab = &quot;Pořadí&quot;, col = &quot;#1f77b4&quot; ) Graf 3.23: Zkoumání závislosti jednotlivých hodnot proměnné počet slov lambda &lt;- mean(pocet_slov) Průměrný počet slov ve větě je 16.56, což je \\(\\lambda\\) našeho rozdělení. Modelujeme tedy počet slov \\(s_i\\) ve větě \\(s_i \\sim Poisson(\\) 16.56 \\()\\).Graf 3.24 vyjadřuje PMF tohoto rozdělení. Jak je vidět tento model na naše data moc nesedí. Náš model má moc malý rozptyl v porovnání s daty. Protože poissonovo rozdělení má pouze jeden parametr \\(\\lambda\\), který vyjadřuje očekávanou hodnotu i rozptyl, nemůžeme rozptyl nijak kontrolovat pomocí parametru rozdělení. Tento fakt je koneckonců zřejmý už z chrakteristik našich dat. Pokud by tato data opravdu byla vygenerována z poissonova rozdělení jejich průměr by se zhruba rovnal rozptylu Jak je ale vidět, tak tomu není protože průměr počtu slov je 16.56 a rozptyl je 95.11. x &lt;- 1:50 n &lt;- length(pocet_slov) pmf &lt;- dpois(x, lambda) plot(x, pmf, type = &quot;h&quot;, lwd = 5, col = &quot;#1f77b4&quot;, xlab = &quot;s&quot;, ylab = &quot;PMF&quot;, main = paste0(&quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot;) ) Graf 3.24: PMF poissonova rozdělení I když je náš model jednoduchý s pouze jedním parametrem \\(\\lambda\\), je to model, který můžeme použít pro predikci. Výsledky jakéhokoliv modelu je vždy dobré si zobrazit graficky, abychom si dokázali lépe představit, co predikuje. 3.25 ukazuje, kolik by náš model predikoval počtet slov ve větě. Protože PMF vyjadřuje pravděpodobnost, můžeme očekávaný (predikovaný) četnost počtu slov podle našeho modelu vypočítat jako \\(PMF * n\\). V našem případě máme 66 vět, tedy predikovaná četnost počtu slov \\(\\hat{s_i} = PMF * n\\). # zaokrouhlime, protoze musime mit cela cisla s_hat &lt;- round(pmf * n) # pridame data .x &lt;- as.numeric(names(cetnost)) # zobrazime nase data plot(.x, cetnost, type = &quot;h&quot;, lwd = 5, col = &quot;grey&quot;, xlab = &quot;Počet slov&quot;, ylab = &quot;Četnost&quot;, xlim = c(0, 50), ylim = c(0, 6), main = &quot;Porovnání modelu poissonova rozdělení s daty&quot; ) # pridame ocekavane hodnoty lines(x, s_hat, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.4), type = &quot;h&quot;, lwd = 5 ) # aby y osa zdy vyjadrovala vsechna cisla axis(2, at = c(0:6), labels = c(0:6)) # legenda legend(&quot;topright&quot;, legend = c( &quot;Data&quot;, paste0( &quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot; ) ), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) Graf 3.25: Očekávaná četnost počtu slov ve větě pro n=66 vět Toto se v reálném světě často. Tento jev se v angličtině označuje jako overdispersion. Jednou z možností, jak se s overdispersion vypořádat je, že modelujeme náhodnou proměnnou jako pocházející z jiného rozdělení. Pro náš jednoduchý příklad bychom mohli využít častého triku, který převede pozitivní diskrétní data s větším rozptylem na data spojitá. Tento trik využívá transformace proměnné a to konkrétně jejího logaritmu. Jak je známo logaritmus je definován pro definiční obor \\(\\in (0, \\infty)\\). Pro zopakovaní ukazujeme funkci přirozeného logaritmu v grafu 3.26. Jak je vidno, tak funkce logaritmu převede hodnoty do oboru hodnot \\(\\in (-\\infty, \\infty)\\) a zároveň budou její hodnoty spojité. Normální rozdělení je takto definováno a proto zkusíme modelovat počet slov ve větě \\(s_i\\) jako \\(log(s_i) \\sim T(\\mu_{log(s)}, \\sigma_{log(s)}, n)\\). curve(log(x), from = 0.01, to = 100, col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;Log(x)&quot;, main = &quot;Přirozený logaritmus&quot; ) abline(h = 0, v = 0, lwd = 1) Graf 3.26: Ukázka přirozeného logaritmu Graf 3.27 zobrazuje histogram logaritmu početu slov ve větě a teoretické PDF \\(T(2.61, 0.66, 65)\\). Jak je vidět takovýto model lépe vystihuje rozptyl v počtu slov. Vzhledem k tomu, že máme pouze 66 pozorování nebude taková náhodná proměnná přesně vystihovat teoretické rozdělení. Graf 3.28 ukazuje odhadnutou hustotu pravděpodobnosti pro 50 výběrů z \\(T(2.61, 0.66, 66)\\), každý o velikosti \\(n=\\) 66. Modrá křivka ukazuje naše data, šedé křivky potom simulované výběry. Je vidět, že několiktakových výběrů, které jsou podobně nahnuté doprava nastalo, což znamená, že taková data nejsou tak nepravděpodobná (pokud pocházejí z \\(log(s_i) \\sim T(2.61, 0.66, 66)\\)). # transfromujeme promennou s_log &lt;- log(pocet_slov) # vypocitame prumer mu &lt;- mean(s_log) # vypocitame rozptyl s &lt;- sd(s_log) # stanovime x pro ktere vypocitame pdf x &lt;- seq(0.01, 5, length.out = 1000) # vypocitame pdf zobecneneho t rozdeleni pdf &lt;- dt((x - mu) / s, df = n) * 1 / s hist(s_log, xlab = &quot;log(s)&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram logaritmu počtu slov&quot;, col = &quot;grey&quot;, breaks = 10, xlim = c(0, 5) ) # pridame na druhou osu par(new = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, bty = &quot;n&quot; ) legend(&quot;topright&quot;, legend = c( &quot;log(s)&quot;, paste0(&quot;T(&quot;, round(mu, 2), &quot;,&quot;, round(s, 2), &quot;,&quot;, n,&quot;)&quot;) ), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) thicks &lt;- round(seq(min(pdf), max(pdf), by = 0.05), 2) axis(4, at = thicks, labels = thicks) Graf 3.27: rozdělení logaritmu dat a teoretická PDF T(2.61, 0.66, 66) # odhadneme hustotu pravdepodobnosti dat d_data &lt;- density(s_log) S &lt;- 50 plot(d_data, type = &quot;l&quot;, xlab = &quot;log(s)&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, xlim = c(0, 5), ylim = c(0, 0.85), lwd = 2 ) for (i in 1:S) { # provedeme vybery z t rozdeleni o stejne velikosti vyberu vyber &lt;- rt(n, df = n) * s + mu lines(density(vyber), col = adjustcolor(&quot;black&quot;, alpha.f = 0.2)) } Graf 3.28: Příklad hustoty pravděpodobnosti 50 výběrů o velikosti n=66 z T(2.61, 0.66, 66) Nakonec se tedy vrátíme k naší původní otázce - predikce počtu slov ve větě. Jak jsme si ukázali, logaritmus počtu slov ve větě můžeme dobře approximovat normálním rozdělením. Na tomto rozdělení můžeme počítat CDF stejně tak, jak jsme si to ukázali v 3.3. Řekněme, že z důvodu srozumitelnosti nedoporučejeme věty delší než 20 slov. Zajímá nás, jaká je pravděpodonost, že libovolně zvolená věta bude delší než 20 slov, pak vlastně počítáme \\(P(log(s_i) &gt; log(20))\\) z našeho \\(log(s_i) \\sim N(2.61, 0.66)\\)10. q &lt;- log(20) cdf &lt;- pt((q - mu) / s, df = n, lower.tail = FALSE) Tato pravděpodobnost je rovna 0.28. Nebo jinak zhruba 28% vět bude delších než 20 slov. 3.7 Chi-kvadrát \\(\\chi^2\\) rozdělení popisuje náhodný proces, kterého nabývá součet umocněných stadandardizovaných normálně rozdělených proměnných. Jak jsme uvedli v 3.3, standardizovaná náhodná proměnná pocházející z normálního rozdělení \\(z_i\\) má \\(\\mu_z = 0\\) a \\(\\sigma_z = 1\\). Pokud máme \\(k\\) standardizovaných náhodných proměnných, pak můžeme zapsat \\(Q = \\sum_{i=1}^k z_i^2\\). Toto rozdělení se nepoužívá tak často k popsání jevů z fyzického světa jako spíše pro popsání chování různých parametrů, které se ve statistice odhadují. Protože mnoho odhadovaných paramtrů (jako např. průměr) mají normální rozdělení, můžeme jejich pravděpodobnostní rozdělení popsat právě pomocí \\(\\chi^2\\), což se hodí při posuzování významnosti různých procedur. \\(\\chi^2\\) má jediný parameter \\(k\\), který vyjadřuje počet stupňů volnsti (degrees of freedom). V 3.29 ukazujeme, jak \\(\\chi^2\\) rozdělení vzniká. Děláme výběr o velikosti \\(n=200\\) ze standardizovaného normálního rozdělení pro \\(k=2\\) proměnné, \\(k=4\\) a \\(k=16\\), tedy \\(z \\sim \\chi^2(2)\\), \\(z \\sim \\chi^2(4)\\) a \\(z \\sim \\chi^2(16)\\). Proměnné umocníme, sečteme a zobrazíme histogram rozdělení této nově vzniklé náhodné proměnné. # velikost vyber n &lt;- 1000 # pocet standardizovanych promennych K &lt;- c(2, 4, 16) # zobrazit grafy vedle sebe par(mfrow = c(3, 1)) for (k in K) { Z &lt;- sapply(1:k, function(x) rnorm(n, 0, 1)) X2 &lt;- rowSums(Z^2) hist(X2, xlab = &quot;z&quot;, breaks = 20, ylab = &quot;Četnost&quot;, main = paste0(&quot;X2(&quot;, k, &quot;)&quot;), col = &quot;#1f77b4&quot;, ) } Graf 3.29: Ukázka X2 rozdělení pro k=2, k=4 a k=16 Jak je vidět \\(\\chi^2\\) je definováno pouze pro \\([0, \\infty)\\), tedy pro pozitivní hodnoty spojité proměnné. To je proto že počítáme s mocninou, tedy žádné negativní hodnoty v našem definičním oboru být nemohou. Očekávanou hodnotou je \\(k\\), tedy \\(E(z_i) = k\\) a rozptyl je \\(2k\\), tedy \\(Var(z_i)=2k\\). S rostoucím počtem stupňů volnosti \\(k\\) se rozdělení tvarem podobá normálnímu rozdělení s velkým rozptylem (protože rozptyl je roven \\(2k\\)). Graf 3.30 zobrazuje PDF pro \\(z \\sim \\chi^2(2)\\), \\(z \\sim \\chi^2(4)\\), \\(z \\sim \\chi^2(8)\\), \\(z \\sim \\chi^2(16)\\) a \\(z \\sim \\chi^2(32)\\). x &lt;- seq(0, 50, length.out = 1000) K &lt;- 2^c(1:5) PDF &lt;- sapply(K, function(k) dchisq(x, k)) cols &lt;- rev(RColorBrewer::brewer.pal(n = length(K) + 1, name = &quot;Blues&quot;)) plot(x, PDF[, 1], type = &quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;PDF&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, lwd = 2 ) for (j in 2:length(K)) { lines(x, PDF[, j], type = &quot;l&quot;, lwd = 2, col = cols[j] ) } legend(&quot;topright&quot;, legend = paste0(&quot;X2(&quot;, K, &quot;)&quot;), col = c(&quot;#1f77b4&quot;, cols[2:length(K)]), lwd = rep(2, length(K)), cex = 0.7 ) Graf 3.30: PDF X2 rozdělení s k=2, k=4, k = 8, k=16 a k=32 3.8 Jak vybrat správný model pro data My jsme si v této kapitole ukázali některé nejčastější rozdělení, které popisují náhodné procesy vyskutující se ve fyzikálním světě okolo nás. Při analýze máte zpravidla nějaká data (náhodné proměnné), jejichž chování se nažíte popsat pomocí nějakého modelu. To jaký model na vaše data vybrat záleží především na povaze dat, tedy zda jsou data spojitá nebo diskrétní. Dále pak na tom, jaké hodnoty byste u vašich dat čekali, především, zda májí data nějakou hranici, za kterou data nejsou možná (např. data musí být pozitivní apod.). V poslední řadě pak záleží na tom, zda zvolený model věrohodně vystihuje pravděpodobnost, s kterou data nabývají různých hodnot. Znovu zopakujeme, že model představuje připodobnění reality a jeho užitečnost závisí na otázce, kterou se snažíme modelem zodpovědět. Vždy je dobrým zvykem si data zobrazit. Ze zvažovaného modelu si vygenerujte data a porovnejte je s reálnými daty. Uvažujte jakých extrémních hodnot by váš model mohl dosáhnout a zda jsou takové hodnoty pro problém který zkoumáte vůbec reálné. Zvažte, do jaké míry váše data (nebo jejich sběr) porušují předpoklady vašeho modelu11. Právě díky grafickému zobrazení různých simulací můžete pochopit, jak by mohly různé předpoklady váš model rozbít, nebo za jakých podmínek přestane věrohodně vystihovat vaše data. Nakonec představíme aplikaci, pomocí které si můžete vyzkoušet, jak různé parametry ovlivňují tvar různých rozdělení nebo vypočítat pravděpodobnost, že náhodná proměnná \\(x_i\\) nabyde určitých hodnot \\(a\\). Graf 3.31: Zdroj: https://github.com/ShinyEd/intro-stats/tree/master/dist_calc Např. teplota, hmotnost, výška. Naše schonost tyto jevy měřit je ale často diskrétní, hmotnost můžeme měřit na mg apod. Protože jsou ale jednotky míry často malé, bereme je jako spojitou proměnnou.↩︎ Teoreticky jsou ohraničené, ale prakticky se v přírodě extrémy často nevyskutují. Například teplota neklesá k absolutní nule. Váha zvířat je zpravidla větší než 0 mg apod.↩︎ Pokud by \\(x_i\\) pocházela z jiného rozdělení byl by průměr stále rovný 0 a směrodatná odchylka 1, ale použitá notace by odpovídala rozdělení, z kterého pochází (např. U pro uniformní rozdělení apod.).↩︎ Jedná se o pravděpodobnost, protože počítáme PDF v nějakém intervalu CDF.↩︎ PMF poissonova rozdělení klesá s rostoucí hodnotou \\(x_i\\) rychle k zanedbatelné hodnotě, takže bude nepravděpodobné, že bychom generovali věty s nadpřirozenými počty slov.↩︎ Pokud bychom chtěli věrohodně zachytit nejistotu ohledně počtu slov v našich datech, měli bychom počítat s nejistotou odhadu průměru a směrodatné odchylky rozdělení. Něco, co si ukážeme v příští kapitole.↩︎ Skoro všechna data porušují do nějaké míry alespoň jedne z předpokladů modelu :).↩︎ "],["dist-params.html", "Kapitola 4 Výběrové rozdělení parametrů 4.1 Relativní četnost 4.2 Interval spolehlivosti 4.3 Bootstrap 4.4 Průměr 4.5 Rozptyl", " Kapitola 4 Výběrové rozdělení parametrů Během této kapitoly se dostaneme dále do základů statistiky a představíme si jeden z nejpoužívanějších principů statistiky, který nás bude provázat po zbytku semestru. Opět to bude náročnější kapitola a možná se k ní budete chtít vrátit zpět, jak budeme centrální limitní větu (CLV) aplikovat na další statistické problémy. V této kapitole si představíme její princip. V kapitole o pravděpodobnostních rozděleních 3 jsme si ukázali, že existují různé typy rozdělení, které je možné použít k popsání (připodobnění) toho, jakých hodnot bude proměnná nabývat a v jaké četnosti. Často nás zajímají nejenom hodnoty náhodné proměnné, ale také některé její parametry, jako například průměr nebo procenta. Už víme, že hodnoty náhodné proměnné jsou ovlivněny náhodou, které může mít různé důvody. Je asi intuitivní, že i parametry této proměnné budou touto náhodou ovlivněny a že nebudou vždy stejné. No a protože tyto parametry budou nabývat různých hodnot, pak můžeme jejich rozdělení zkoumat jako výběrové rozdělení parametrů. A jako u každého jiného pravděpodobnostního rozdělení můžeme popsat jaké hodnoty a s jakou frekvencí bychom u proměnné očekávali. Než se pustíme do principů CLV a výběrového rozdělení paramerů, zmíníme pár vět o termilogii, kterou budeme používat. Populační parametry, se zpravidla označují písmeny řecké abecedy. Výběrové parametry pak označujeme písmeny latinské abecedy. Parametr Výběr Populace průměr \\(\\overline{x}\\) \\(\\mu\\) směrodatná odchylka \\(s\\) \\(\\sigma\\) procento \\(p\\) \\(\\pi\\) 4.1 Relativní četnost Vraťme se zpět ke Galtonově boxu. My jsme tento proces simulovali jako \\(x_i \\sim B(12, 0.5)\\). Řekněme, že nás zajímá zjistit jaké procento míčků skončí v prostředním (sedmém) sloupci \\(p\\). Procento je nějaký parametr naší proměnné, který jsme zvolili, ale mohli bychom si zvolit například medián. Toto procento bude pokaždé když kuličky spustíme znova trochu jiné. Centrální limitní věta nám pomůže kvantifikovat, jak jiné toto procento bude. set.seed(42) # pocet micku n &lt;- 10000 # pravdepodobnost uspechu (napravo) p &lt;- 0.5 # 0 by znamenalo všechny kulicky nalevo, 12 všechny kulicky napravo, # celkem 13 možností, jak mohou kulicky skončit s &lt;- 12 vysledek &lt;- rbinom(n = n, size = s, prob = p) cetnost &lt;- table(vysledek) # pouzijeme nazev 6 v tabulce, protoze 0 je prvni sloupec p_7 &lt;- cetnost[names(cetnost) == &quot;6&quot;] / n Například v simulaci, kterou jsme provedli v kodu nahoře je procento kuliček v sedmém sloupci \\(p_7\\) rovné 0.2227. Když jsme simulovali hodnoty náhodné proměnné, tak každá kulička představovala jedno pozorování. V simulaci, kterou si teď předvedeme, vždy spustíme kuličky, spočítáme procento v sedmém sloupci, kuličky pustíme znova, opět spočítáme procento kuliček v sedmém sloupci a tak dále. Každé procento, které takto vypočítáme, představuje jednu hodnotu nějaké nové náhodné proměnné. Nakonec si ukážeme jak vypadá histogram těchto procent. Předtím, než se na animaci podíváte zkuste pomocí vašich znalostí pravděpodobnostních rozdělení odhadnout, jaké bude mít rozdělení průměrů tvar. # funkce na vypocet p_7 rel_cetnost &lt;- function(x, k) { cetnost &lt;- table(x) p &lt;- cetnost[names(cetnost) == k] / sum(cetnost) return(p) } # pocet simulaci S &lt;- 1:90 p_7 &lt;- rep(NA, length(S)) for (i in S) { # spustime kulicky vysledek &lt;- rbinom(n = n, size = s, prob = p) # vypocitame procento kulicek v kazdem sloupci p_7[i] &lt;- rel_cetnost(vysledek, &quot;6&quot;) hist(p_7, breaks = seq(0.21, 0.24, by = 0.002), xlim = c(0.21, 0.24), main = paste0(&quot;Počet opakových spuštění kuliček: &quot;, S[i]), col = &quot;#1f77b4&quot;, xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;Četnost&quot; ) } Graf 4.1: rozdělení procenta kuliček v sedmém sloupci při opakovaném spuštění kuliček Jak je vidět z histogramu v 4.1, rozdělení procent z opakovaných spuštění kuliček je zhruba normálně rozloženo s průměrem 0.23 a směrodatnou odchylkou 0.004, nebo jinak \\(p \\sim N(\\) 0.23 \\(,\\) 0.004 \\()\\). To není žádné překvapení. Jak víme z 3.3 spojité proměnné, které se shlukují okolo nějakého bodu a mohou nabývat hodnot na levo a na pravo od tohoto bodu bývají často normálně rozděleny. Centrální limitní věta nám popisuje tento fenomen a říká nám, jak vypočítat parametry tohoto normálního rozdělení (tedy průměr a směrodatnou odchylku). V našem případě bychom parametry takového rozdělení mohli vypočítat jako \\[p \\sim N(\\pi, \\frac{\\sigma_{\\pi}}{\\sqrt{n}})\\]kde \\(n\\) je velikost výběru (10000 v našem případě), pričemž z 3.2 víme, že rozptyl u procent vypočítáme jako \\(\\sigma^2 = \\pi*(1-\\pi)\\) a směrodatnou odchylku jako \\(\\sigma = \\sqrt{\\pi*(1-\\pi)}\\). \\(\\frac{\\sigma_{\\pi}}{\\sqrt{n}}\\) se nazývá směrodatná chyba odhadu a označuje se jako \\(\\sigma_{\\pi}\\)12. Protože teoretická rozdělení parametru využívá k aproximaci normální rozdělení13, platí pro PDF stejné překlady, jaké jsme si ukazovali v 3.3, tedy, že jednotlivé hodnoty jsou na sobě nezávislé. Graf 4.2 ukazuje porovnání histogramu z 90 opakovaných puštění kuliček a jejich procent v sedmém sloupci s teoretickým rozdělením tohoto parametru podle CLV14. # vypocitame p p_hat &lt;- dbinom(6, size = 12, prob = 0.5) # vypocitame smerodatnou odchylku dat s_hat &lt;- sqrt(p_hat * (1 - p_hat)) # vypocitame pdf rozlozeni procent x &lt;- seq(0.21, 0.24, length.out = 1000) pdf &lt;- dnorm(x, mean = p_hat, sd = s_hat/sqrt(n)) hist(p_7, breaks = seq(0.21, 0.24, by = 0.002), xlim = c(0.21, 0.24), col = &quot;grey&quot;, main = &quot;&quot;, xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;Četnost&quot; ) # pridame na druhou osu par(new = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, bty = &quot;n&quot; ) legend(&quot;topright&quot;, legend = c( &quot;rozdělení procent&quot;, paste0(&quot;N(&quot;, round(p_hat, 2), &quot;,&quot;, round(s_hat/sqrt(n), 3), &quot;)&quot;) ), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) thicks &lt;- round(seq(0, max(pdf), by = 10), 2) axis(4, at = thicks, labels = thicks) Graf 4.2: Porovnání rozdělení 90 opakování procent kuliček v sedmém sloupci s teoretickým rozdělením procent kuliček v sedmém sloupci podle CLV Jak je zřejmé s parametrů rozdělení, poloha tohoto rozdělení určena průměrem a šířka rozdělení bude ovlivněna směrodatnou chybou odhadu \\(\\frac{\\sigma_{\\pi}}{\\sqrt{n}}\\), tedy mírou variability v datech a velikostí našeho výběru. Čím větší velikost výběru, tím menší je směrodatná odchylka rozdělení parametru15. Aby nedocházelo k záměně směrodatné odchylky rozdělení parametru a směrodatné odchylky výběru, používá se pro směrodatnou odchylku rozdělení parametru pojem směrodatná chyba odhadu. Doposud jsme pro průměr rozdělení parametru a směrodatnou odchylku používali populační parametry, tedy věděli jsme hodnotu \\(\\pi\\) v populaci a také rozptyl dat \\(\\pi*(1-\\pi)\\). To vychází z toho, že u Galtonova boxu víme, jaký náhodný proces data generuje. Většinou ale tento náhodný proces neznáme a nemáme tedy informace o populačním průměru a směrodatné odchylce. V takovém případě odhadneme obě hodnoty z dat (výběru). Je asi intuitivní, že pokud bude velikost našeho výběru \\(n\\) malá, bude existovat větší nejistota o odhadu populačního průměru a směrodatné odchylky z dat. Abychom tuto dodatečnou nejistotu zachytili v teoretickém pravděpodobnostním rozdělení parametru, použijeme k jeho aproximaci t-rozdělení s \\(n-1\\) stupni volnosti, tedy \\[p \\sim T(p, \\frac{s_p}{\\sqrt{n}}, n-1)\\]16Jak víme z 3.4, t-rozdělení dává větší pravděpodobnost hodnotám více vzdáleným od průměru. Graf 4.3 ukazuje teoretické rozdělení procenta kuliček v sedmém sloupci \\(p_7\\) při \\(n=20\\), \\(n=50\\) a \\(n=100\\). Tedy místo 10000, pustíme pouze 20, 50 a 100 kuliček. Abychom ukázali použití t-rozdělení a odhadu průměru a směrodatné odchylky z výběru, budeme chvilku předstírat, že nevíme jejich teoretické hodnoty a proto \\(p_7\\) a \\(s_7\\) odhadneme z výběru Jak je vidět při puštění pouze 20 kuliček je náš odhad velmi nepřesný. Procento kuliček, které skončí v sedmém sloupci může klidně být od 0 do 0.417. To dává smysl, představte si, jaká míra nejistoty existuje, pokud do Galtonova boxu nasypeme pouze 20 kuliček. S tím, jak se zvětšuje velikost výběru \\(n\\), klesá míra naší nejistoty o odhadu procenta kuliček v sedmém sloupci, klesá směrodatná chyba odhadu a tím pádem o šířka teoretického rozdělení parametru. # definujeme velikost vyberu N &lt;- c(20, 50, 100) # definujeme hodnoty pro ktere budeme pocitat pdf x &lt;- seq(0.00,1, length.out = 1000) # vytvorime prazdný graf, do ktereho budeme pridavat plot(x, type= &quot;n&quot;, xlim = c(0, 0.8), ylim = c(0, 10), xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;PDF&quot;) cols &lt;- rev(RColorBrewer::brewer.pal(length(N) + 1, name = &quot;Blues&quot;) ) for(i in 1:length(N)) { # provedeme vyber a odhadneme p a sd n &lt;- N[i] .vyber &lt;- rbinom(n = n, size = s, prob = p) p_hat &lt;- rel_cetnost(.vyber, &quot;6&quot;) s_hat &lt;- sqrt(p_hat*(1-p_hat)) # vypocteme pdf pro T(p, s/sqrt(n), n-1) podle zobecneneho t-rozlozeni pdf_t &lt;- 1/(s_hat / sqrt(n)) * dt((x - p_hat)/(s_hat / sqrt(n)), df = n-1) # pridame t-rozlozeni lines(x, pdf_t, type = &quot;l&quot;, lwd = 2, col = cols[i]) } # pridame legendu legend(&quot;topright&quot;, legend = c(paste0(&quot;T(p, s/sqrt(n), &quot;, N-1, &quot;)&quot;)), col = cols[1:length(N)], cex = 0.7, lwd = c(2, 2) ) Graf 4.3: Porovnání vlivu velikosti výběru n na směrodatnou chybu odhadu 4.2 Interval spolehlivosti Pokud dokážeme pomocí PDF popsat předpokládáné rozdělení výběrového parametru, dokážeme pomocí CDF spočítat pravděpodobnost, že by výběrový parametr nabyl nějaké hodnoty nebo vyšší/menší. Tedy v případě výběrového parametru procent kuliček v sedmém sloupci, dokážeme spočítat \\(P(p \\ge q)\\) nebo \\(P(p &lt; q)\\). Například, můžeme vypočítat kvantily, které pokryjí 95% hodnot rozdělení výběrového parametru okolo průměru, tedy \\(p_{0.025}\\) a \\(p_{0.975}\\). Procento hodnot rozdělení výběrového parametru pokryté našim kvantilem označujeme jako \\(1-\\alpha\\) a tedy procento hodnot rozdělení výběrového parametru nepokryté našim kvantilem jako \\(\\alpha\\). Graf 4.4 ukazuje teoretické rozdělení výběrového parametru (procenta kuliček v sedmém sloupci) a modrá oblast ukazuje hodnoty p, které jsou menší než \\(p_{0.025}\\) nebo větší než \\(p_{0.975}\\). K vypočítání kvantilů rozdělení použijeme funkci qt. probs &lt;- c(0.025, 0.975) # vypocitame quantily zobecneneho t-rozlozeni pro probs q &lt;- qt(probs, df = n-1) q &lt;- q * s_hat/sqrt(n) + p_hat # vytvorime prazdný graf, do ktereho budeme pridavat plot(x, pdf_t, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlim = c(0, 0.8), ylim = c(0, 10), xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;PDF&quot;) # naznacime kvantily a hodnoty vetsi nez nebo mensi nez # pridame jako &quot;h&quot; abychom nazanacili integral lines(x[x &lt; q[1]], pdf_t[x &lt; q[1]], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) lines(x[x &gt; q[2]], pdf_t[x &gt; q[2]], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) legend(&quot;topright&quot;, legend = c(&quot;p_0.025 | p_0.975&quot;, paste0(&quot;T(&quot;, round(p_hat, 2), &quot;,&quot;, round(s_hat / sqrt(n), 2), &quot;,&quot;, tail(N, 1)-1,&quot;)&quot;) ), col = c(&quot;#1f77b4&quot;, &quot;grey&quot;), lwd = c(2, 2), cex = 0.7) Graf 4.4: Kvantily p_0.025 a p_0.975 pro rozdělení T(p, s/sqrt(n), 99) Pojďme si ještě jednou shrnout, co jsme vypočítali. Pomocí CLV jsme vypočítali pravděpodobnostní rozdělení výběrového parametru (procenta kuliček v sedmém sloupci). Toto rozdělení nám říká, s jakou pravděpodobností bychom získali další parametry, pokud bychom provedli nové výběry. Představme si opět na chvilku, že neznáme proces, který procenta kuliček v sedmém sloupci generuje (tedy neznáme populační \\(\\pi_7\\)). Pomocí pravděpodobnostního rozdělení parametru výběru bychom tedy mohli usoudit něco o populačním parametru, konkrétně to, že pokud bychom výběry z populace opakovali, pak by 95% hodnot výběrového parametru bylo v rozmezí \\(p_{0.025}\\) a \\(p_{0.975}\\), tedy konkrétně mezi \\(p_{0.025}=\\) 0.16 a \\(p_{0.975}\\)= 0.34. To je nesmírně užitečná informace, která nám říká něco o populační hodnotě, kterou vůbec neznáme (v našem předstíraném případě). Skoro pokaždé budete při analýze pracovat s výběrem18 a pomocí CLV tedy můžete odhadnout populačního parametru. Gratuluji, právě jste sestrojili interval spolehlivosti! Formálně se interval spolehlivosti vypočítá jako \\[IS_{1-\\alpha} = \\overline{x} +/- t_{\\alpha/2; 1-\\alpha/2} \\frac{s}{\\sqrt{n}}\\]kde \\(t\\) je hodnota kvantilu t-rozdělení s \\(n-1\\) stupni volnosti pro \\(\\alpha/2\\) a \\(1-\\alpha/2\\). t_q &lt;- qt(probs, n-1) i_s &lt;- p_hat + t_q * s_hat/sqrt(n) Pokud bychom vypočítali interval spolehlivosti podle výše zmíněného vzorce pro \\(\\alpha=0.05\\), pak bychom měli hodnoty mezi [0.16, 0.34], tedy pokud bychom výběry z populace opakovali, pak by 95% z nich bylo v rozmezí [0.16, 0.34]. Ještě ukážeme to, co jsme zmínili nahoře, tedy, že interval spolehlivosti nám říká v jakém rozmezí bychom očekávali dané procento hodnot (toto procento je dané námi zvolenou hladinou \\(1-\\alpha\\)) výběrového parametru při opakovaných výběrech. Budeme simulovat 100 výběrů, u každého výběru spočítáme interval spolehlivosti a zobrazíme ho šedě, pokud nepokrývá populační hodnotu \\(\\pi_7\\) a modře pokud ji pokrývá. Protože naše hladina spolehlivosti je zvolena na 90%, očekáváme, že ze 100 intervalů spolehlivosti, jich asi 90 pokryje populační hodnotu a asi 10 ji nepokryje19. # vytvorime si funkci na vypocet is interval_spolehlivosti &lt;- function(p_hat, alpha, n) { # spocitame populacni sd s_hat &lt;- sqrt(p_hat *(1 - p_hat)) # vypocitame pravdepodobnosti 1-alpha/2 probs &lt;- c(alpha / 2, 1 - (alpha / 2)) # spocitame kvantily pro 1/alpha/2 t_q &lt;- qt(probs, n-1) # spocitame interval spolehlivosti i_s &lt;- p_hat + t_q * s_hat/sqrt(n) return(i_s) } # pocet simulaci S &lt;- 100 # velikost vyberu n &lt;- 100 # urcime hladinu spolehlivosti alpha &lt;- 0.1 # populacni hodnota pi &lt;- dbinom(x = 6, size = 12, prob = 0.5) # vyvorime prazdny graf plot(1:S, type= &quot;n&quot;, xlab = &quot;Pořadí&quot;, ylab = &quot;Procento kuliček v sedmém sloupci&quot;, ylim = c(0.05, 0.45)) for(x in 1:S) { # udelame vyber a spocitame ... # ...vyberovy prumer a smerodatnou odchylku vyber &lt;- rbinom(n, size = 12, prob = 0.5) p_hat &lt;- rel_cetnost(vyber, &quot;6&quot;) s_hat &lt;- sqrt(p_hat * (1 - p_hat)) # spocitame interval spolehlivosti i_s &lt;-interval_spolehlivosti(p_hat, alpha, n) # zvolime barvu podletoho, zda is pokryva populacni parametr .col &lt;- &quot;#1f77b4&quot; if(i_s[1] &gt; pi | i_s[2] &lt; pi) { .col &lt;- &quot;grey&quot; } # zobrazime v grafu bodovy odhad points(x, p_hat, pch = 19, col = .col) lines(c(x, x), i_s, lwd = 1, col = .col) } # pridame populacni prumer abline(h = pi, col = &quot;black&quot;, lwd = 2) legend(&quot;topright&quot;, legend = &quot;populační parametr&quot;, col = &quot;black&quot;, lwd = 2) Graf 4.5: Ukázka opakovaných výběrů z populace a jak hladina alpha ovlivňuje podíl intervalů pokrytých opakovaných výběrem Šířku intervalu spolehlivosti (nejistoty ohledně hodnoty populačního parametru tedy ovlivňuje): směrodatná chyba odhadu parametru \\(\\frac{s}{\\sqrt{n}}\\) (a tedy jaká je variabilita v našich výběrových datech, měřeno směrodatnou odchylkou a velikostí výběru) hladina spolehlivosti \\(\\alpha\\). Čím menší je alpha, tím je interval spolehlivosti užší, ale tím více procent opakovaných výběrů by hodnotu populačního parametru nepokrylo. Standardně tedy zvolíme \\(\\alpha\\) relativně malé, aby náš interval spolehlivosti pokryl populační parametr ve velké většině případů, a to i za cenu toho, že intrval spolehlivosti bude širší. 4.3 Bootstrap Pojďme se na chvilku vrátit ke grafu 4.1. Na tomto grafu jsme ukázali hypotetickou situaci, kdy jsme věděli jak vypadá rozdělení proměnné v celé populaci. Dělali jsme opakovené výběry, abychom ukázali, jak by vypadalo rozdělení parametru, kdybychom u každého výběru vypočítali parametr, který nás zajímá. Ukazuje se, že pomocí podobné metody můžeme vypočítat rozdělení parametru a to i bez toho, abychom měli data za celou populaci. Místo toho, abychom dělali náhodné výběry z populačních dat, tak budeme dělat náhodné výběry s opakováním z výběru o velikosti \\(n\\). Výstupem tohoto výpočtu je vektor parametru, který nás zajímá, o velikosti rovnající se počtu výběrů s opakováním. Je důležité zdůraznit, že přestože počítáme rozdělení parametru empiricky, některé ze předpokladů, které jsme zmínili u výpočtu teoretického rozdělení, jsou stále platné. Především předpoklad o nezávislosti jednotlivých pozorování a o náhodném výběru z populace (pokud chceme dělat úsudky o celé populaci). # pocet bootstrap vyberu S &lt;- 1e4 p_hat &lt;- rep(NA, S) for(i in 1:S) { # udelame vyber s opakovanim boostrap_vyber &lt;- sample(.vyber, size = n, replace = TRUE) # vypocitame parametr vyberu p_hat[i] &lt;- rel_cetnost(boostrap_vyber, &quot;6&quot;) } hist(p_hat, col = &quot;#1f77b4&quot;, xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;) Graf 4.6: Boostrap výpočet procenta kuliček v sedmém sloupci # vypocitale kvantil 0.025 a 0.975 alpha &lt;- 0.05 i_s &lt;- quantile(p_hat, probs = c(alpha/2, 1-alpha/2)) Jako příklad se vrátíme k našemu výběru z 4.5, kdy jsme na chvilku předstírali, že neznáme populační data, respektive náhodný proces, který data generuje. Graf 4.6 ukazuje empirické rozdělení procenta kuliček, které skončí v sedmého sloupci. Pokud bychom chtěli vypočítat interval spolehlivosti, vypočítáme emprirický kvantil, tak jako jsme dělali v kapitole o mírách polohy 2.3. 95% interval spolehlivosti je [0.17, 0.34]. Pokud můžeme vypočítat rozdělení parametru analyticky, proč bychom měli používat bootstrap, který je více náročný z hlediska potřeby provádět opakované výběry. Je to proto, že pomocí bootsrapu můžeme vypočítat empirické rozdělení i parametrů, pro které neexistuje jednoduché teoretické rozdělení. Pracujeme opět se stejným hypotetickým výběrem o velikosti \\(n=\\) 100. Pokud bychom například chtěli vypočítat interval spolehlivosti poměru kuliček v pátém a sedmém sloupci, tedy \\(\\frac{p_{5}}{p_{7}}\\), nebude jednoduché vypočítat směrodatnou chybu20. S pomocí bootstrapu můžeme udělat \\(S\\) výběrů s opakováním, u každého výběru vypočítat \\(\\frac{p_{5}}{p_{7}}\\). Získáme tak vektor o velikosti \\(S\\), který můžeme použít k vypočítání empirického rozdělení parametru \\(\\frac{p_{5}}{p_{7}}\\). # vytvorime funkci bootstrap &lt;- function(x, func, S) { n &lt;- length(x) parameter &lt;- rep(NA, S) # provedeme vybery s opakovanim for(i in 1:S) { # udelame vyber s opakovanim boostrap_vyber &lt;- sample(x, size = n, replace = TRUE) # vypocitame parametr vyberu parameter[i] &lt;- func(boostrap_vyber) } return(parameter) } # funkce na vypocet pomeru pomer &lt;- function(x) { ans &lt;- rel_cetnost(x, &quot;4&quot;) / rel_cetnost(x, &quot;6&quot;) # pokud nebudou zadne kulicky v jednom ze sloupcu ... # ...vratit NA if(length(ans) == 0) { return(NA) } else { return(ans) } } # vypocitame bootstrap parametru p_hat &lt;- bootstrap(.vyber, pomer, S) hist(p_hat, col = &quot;#1f77b4&quot;, xlab = &quot;Poměr procenta kuliček v pátém a sedmém sloupci&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;) # vypocitale kvantil 0.025 a 0.975 alpha &lt;- 0.05 i_s &lt;- quantile(p_hat, probs = c(alpha/2, 1-alpha/2), na.rm = TRUE) 95% interval spolehlivosti pro \\(\\frac{p_{5}}{p_{7}}\\) je [0.11, 0.67]. 4.4 Průměr Jak jsme uvedli v 4.1, CLV říká, že průměr nezávislých proměnných \\(\\theta\\) bude normálně rozložen se směrodatnou chybou rovnou \\(\\frac{s}{\\sqrt(n)}\\)21, tedy \\(\\theta \\sim N(\\theta, \\frac{s}{\\sqrt(n)})\\). U procent \\(p\\) jsme směrodatnou odchylku počítali jako \\(\\sqrt{p(1-p)}\\). U průměru počítáme směrodatnou odchylku jako \\(s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\overline{x})^2}\\), tedy jak jsme si ukazovali v 2.2. rozdělení výběrového průměru náhodné proměnné \\(x\\), tedy bude \\[\\overline{x} \\sim T(\\overline{x}, \\frac{s}{\\sqrt{n}}, n-1)\\]Platí přitom, že nezáleží na tom, zda náhodné proměnná pochází z normálního rozdělení. CLV se týká průměru této proměnné. Jako příklad si ukážeme hypotetickou situaci, v které budeme vědět populační rozdělení náhodné proměnné \\(x_i\\), která pochází z uniformního rozdělení \\(x_i \\sim U(-10, 10)\\). Provedeme 5000 výběrů o velikosti \\(n=100\\) z této populace, u každého spočítáme průměr a poté zobrazíme rozdělení těchto průměrů histogramem. # pocet simulaci S &lt;- 5000 # vektor, kam budeme ukladat prumery prumer &lt;- rep(NA, S) # velikost vyberu n &lt;- 100 # parametry uniformniho rozlozeni a &lt;- -10 b &lt;- 10 for(i in 1:S) { vyber &lt;- runif(n, min = -10, max = 10) prumer[i] &lt;- mean(vyber) } hist(prumer, col = &quot;#1f77b4&quot;, breaks = 10, xlim = c(-3, 3), main = &quot;&quot;, xlab = &quot;Průměr x~U(-10, 10)&quot;, ylab = &quot;Četnost&quot; ) legend(&quot;topright&quot;, legend = paste0(&quot;průměr: &quot;, round(mean(prumer), 2), &quot;\\n&quot;, &quot;směr.odchylka: &quot;, round(sd(prumer), 2)), lwd = 10, col = &quot;#1f77b4&quot;, cex = 0.7 ) Graf 4.7: Průměr 5000 výběrů o velikosti n=100 z x~U(-10, 10) Jak je vidět z grafu 4.7 rozdělení průměrů z 5000 výběrů je normálně rozloženo. Průměr těchto 5000 průměrů je 0.01 a směrodatná odchylka je 0.57. Porovnejme tyto hodnoty s teoretickým rozdělením podle CLV. Z kapitoly o uniformním rozdělení 3.5 víme, že průměr se vypočítá jako \\(\\mu = \\frac{1}{2}(a+b)\\), tedy pro \\(U(-10 ,10)\\) je to hodnota 0. Rozptyl je \\(\\sigma^2 = \\frac{1}{12}(b-a)^2\\), tedy pro \\(U(-10 ,10)\\) hodnota 33.33. Směrodatná chyba odhadu je \\(\\frac{s}{\\sqrt{n}}\\), tedy 0.58. Jak je vidět, parametry normálního rozdělení průměru podle CLV se shodují s údaji, které jsme získali při simulaci naší hypotetické situace. Pokud bychom chtěli vypočítat interval spolehlivosti, můžeme postupovat stejně jako jsme uvedli v kapitole o intervalu spolehlivosti 4.2. Pokud máme k dispozici výběr proměnné \\(x_i\\), pak platí \\[IS_{1-\\alpha} = \\overline{x} +/- t_{\\alpha/2; 1-\\alpha/2} \\frac{s}{\\sqrt{n}}\\]tedy, že k výběrovému průměru přičteme/odečteme hodnotu směrodatné chyby odhadu vynásobenou kvantilem t-rozdělení (podle zvolené hladiny \\(\\alpha\\)). # zvolime hladinu spolehlivosti alpha &lt;- 0.05 # vypocitame vyberovy prumer x_hat &lt;- mean(vyber) # vypocitame vyberovou smer. odchylku s_hat &lt;- sd(vyber) # vypocitame kvantil t rozdeleni pro n-1 t_q &lt;- qt(c(alpha/2, 1-alpha/2), df = n-1) # vypocitame interval spolehlivosti i_s &lt;- x_hat + t_q * s_hat/sqrt(n) # zaokrouhlime pro prehlednost i_s &lt;- round(i_s, 2) Abychom si ukázali výpočet intervalu spolehlivosti, použijeme poslední výběr z naší simulace. Průměr tohoto výběru o velikosti \\(n=\\) 100 je -0.18 a směrodatná odchylka je 5.57. 95% interval spolehlivosti je [-1.28, 0.93]. Tedy, pokud bychom výběry opakovali, pak by 95% výběrů mělo průměr v rozmezí [-1.28, 0.93]. Protože se jedná o hypotetickou situaci, tak my víme, že populační výběr pro \\(U(-10, 10)\\) je roven 0. Jak je vidět, přestože náš výběr je pouze o velikosti \\(n=\\) 100, pak dokážeme pomocí intervalu spolehlivosti získat informaci o populačním průměru. 4.5 Rozptyl Poslední z parametrů, jehož výběrové rozdělění si představíme je rozptyl, respektive směrodatná odchylka. Stejně jako platí u průměru, i rozptyl různých náhodných proměnných se bude lišit v závislosti na výběru. Interval spolehlivosti pro výběrový rozptyl \\(s^2\\) je možné zapsat analytickou formou jako \\[\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}\\]kde \\(\\chi^2\\) je kvantil chi-kvadrát rozdělení s \\(n-1\\) stupni volnosti. K výpočtu výběrového rozdělení rozptylu bychom také mohli použít bootstrap. Vraťme se zpátky k výpočtu počtu slov ve větě z kapitoly 3.6, kde jsme počet slov ve větě \\(s_i\\) modelovali jako \\(log(s_i) \\sim T(2.61, 0.66, 66)\\). Zkoumaný proslov můžeme chápat jako výběr z populace proslovů. Tedy vypočítaný výběrový průměr \\(log(\\overline{s}) = 2.61\\) a výběrová směrodatná odchylka rovná \\(0.66\\) se budou lišit v závislosti na výběru. Tuto nejistotu bychom měli vzít v potaz i při predikci procenta vět delších než 20 slov. Graf 4.8 ukazuje výběrové rozdělení průměru a výběrové rozdělení směrodatné odchylky. n &lt;- 66 vyberovy_prumer &lt;- 2.61 vyberova_sd &lt;- 0.66 smerodatna_chyba &lt;- vyberova_sd / sqrt(n) # vypocitame pdf vyberoveho rozdeleni prumery x &lt;- seq(2.2, 3, length.out = 1000) pdf_prumer &lt;- dt((x - vyberovy_prumer) / smerodatna_chyba, df = n-1) * 1/smerodatna_chyba par(mfrow = c(1, 2)) # vyberove rozdeleni prumeru plot(x, pdf_prumer, xlab = &quot;Logaritmus počtu slov ve větě&quot;, ylab = &quot;PDF&quot;, col = &quot;#1f77b4&quot;, main = &quot;Výběrové rozdělení průměru&quot;, type = &quot;l&quot;, lwd = 2, cex.main = 0.7) # vypocitame pdf vyberove směrodatné odchylky r_s &lt;- sqrt((n-1)*vyberova_sd^2 / rchisq(1e5, n-1)) pdf_s &lt;- density(r_s, n=1e5) # vyberove rozdeleni rozptylu plot(pdf_s, xlab = &quot;Logaritmus počtu slov ve větě&quot;, ylab = &quot;PDF&quot;, col = &quot;#1f77b4&quot;, main = &quot;Výběrové rozdělení směr.odchylky&quot;, type = &quot;l&quot;, lwd = 2, cex.main = 0.7) Graf 4.8: PDF výběrového průměru a směrodatné odchylky # vypocitame jeste interval spolehlivosti alpha &lt;- 0.05 q &lt;- c(alpha/2, 1-alpha/2) is_prumer &lt;- vyberovy_prumer + smerodatna_chyba * qt(q, df = n-1) is_sd &lt;- quantile(r_s, probs = q) 95% interval spolehlivosti pro průměr je [2.45, 2.77] a 95% interval spolehlivosti pro směrodatnou odchylku je [0.56, 0.8]. Pokud bychom převedli hodnoty zpět na původní škálu, pak by byl 95% interval spolehlivosti pro průměr počtu slov ve větě [11.56, 15.99] a 95% interval spolehlivosti pro směrodatnou odchylku je [1.76, 2.22]. Abychom tuto nejistotu vzali v potaz při naší predikci budeme simulovat výběry z t-rozdělení, kde zohledníme vypočítanou nejistotu ve výběrovém průměru a směrodatné odchylce. # pocet simulaci s &lt;- 1e5 # matrix na ukladani simulaci S &lt;- matrix(NA, nrow = s, ncol = n) for(i in 1:s) { S[i, ] &lt;- rt(n, df = n-1) * sqrt((n-1)*vyberova_sd^2 / rchisq(1, n-1)) + # vyberovy rozptyl rnorm(1, vyberovy_prumer, smerodatna_chyba) # vyberovy prumer } # zobrazime prvnich 100 plot(s, type = &quot;l&quot;, xlab = &quot;log(s)&quot;, ylab = &quot;Hustota pravděpodobnosti (PDF)&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, xlim = c(0, 5), ylim = c(0, 0.85), lwd = 2 ) for (i in 1:50) { # provedeme vybery z t rozdeleni o stejne velikosti vyberu lines(density(S[i, ]), col = adjustcolor(&quot;black&quot;, alpha.f = 0.2)) } Graf 4.9: Prvních 50 simulací výběrů z t-rozdělení při n=66 a zohlednění nejistoty výběrového průměru a směrodatné odchylky # vypocitame (P &gt; log(20)) q &lt;- log(20) cdf &lt;- apply(S, 1, function(x) sum(x &gt; q) / n) i_s &lt;- quantile(cdf, probs = c(alpha/2, 1-alpha/2)) Provedeme 10^{5} simulaci, v každé simulaci provedeme výběr ze zobezněného t rozdělení, o velikosti \\(n=\\) 66. U každého výběru vezmeme v potaz nejistotu o odhadu výběrového průměru a výběrové směrodatné odchylky. Graf 4.9 ukazuje prvních 50 simulací počtu slov ve větě \\(log(s_i)\\). Abychom spočítali pravděpodobnost, že v libovolném budoucím proslovu o 66 větách bude věta delší než 20 slov, tedy \\(P(log(s_i) &gt; log(20))\\), pak u každé simulace spočítáme % hodnot větších než \\(log(20)\\). Protože existuje nejistota ohledně každého výběru, pak je i ohledně odhadu \\(P(log(s_i) &gt; log(20))\\) nejistota. Průměr odhadu \\(P(log(s_i) &gt; log(20))\\) je 0.28 a směrodatná odchylka 0.07. Můžeme použít empirický kvantil a výpočítat, že 95% interval spolehlivosti pro podíl vět s počet slov větších než 20 je [15.15, 42.42]22. Jak je tedy vidět interval spolehlivosti je široký. To plyne z toho, že v samotných datech je velký rozptyl v počtu slov ve větě a také z toho, že naše data obsahují pouze 66 vět. V případě, že neznáme populační rozptyl \\(\\sigma^2\\) a populační průměr \\(\\pi\\), tak se směrodatná chyba odhadu označuje jako \\(s_{p}\\).↩︎ Respektive t-rozdělení pokud je velikost našeho výběru malá a populační parametry neznámé.↩︎ \\(p_7 \\sim N(0.19, \\sqrt {\\frac{0.19*0.81}{n}})\\)↩︎ Tento vztah není lineární, tedy 2x větší výběr neznamená 2x menší směrodatnou odchylku rozdělení parametru, směrodatná odchylka rozdělení klesá \\(\\frac{1}{\\sqrt{n}}\\).↩︎ Směrodatná odchylka odhadnutá z dat se označuje \\(s\\).↩︎ Velikost tohoto výběru je dokonce tak malá, že aproximace t-rozdělením nemusí být přesná.↩︎ Ať už výběrem z populace jedniců, nebo výběrem výrobků z nějakého výrobního procesu nebo výběrem teplot měřených v nějaký čas dne apod.↩︎ Protože náš výběr je pouze 100, nebude přesně 10 a 90.↩︎ Pro představu zde je analytické řešení.↩︎ Směrodatnou odchylku parametru nazýváme směrodatná chyba, aby nedocházelo k záměně se směrodatnou odchylkou výběru.↩︎ Při libovolném budoucím proslovu o 66 větách \\(n=66\\). Pokud by byl budoucí proslov delší, byl by interval spolehlivosti pro odhad \\(P(log(s_i) &gt; log(20))\\) menší, protože bychom měli více vět a tím pádem stabilnější odhad.↩︎ "],["stats-test-single.html", "Kapitola 5 Statistické testování 5.1 Postup statistického testování 5.2 Velikost účinku 5.3 Souvislost s intervalem spolehlivosti 5.4 Síla testu 5.5 Velikost výběru 5.6 Mnoho testů", " Kapitola 5 Statistické testování Statistické testy tvoří základní kámen moderní vědy. Jsou používány v klinických studiích na určení efektu léku, v A/B testech na určení toho, zda nějaká intervence funguje, nebo v analýze výběrových šetření na určení toho, zda je nějaký vztah důsledkem náhody. V Popperově filozofii vědeckého poznání jsou statistické testy používány k falzifikaci vědeckých teorií23. Statistické testování zahrnuje soubor různých testů podle problému, který se snažíme vyřešit a podle proměnné, kterou používáme. My si v této kapitole na příkladu jednovýběrového t-testu pro průměr ukážeme principy statistického testování, které platí i pro další typy statistických testů. Výběr testu záleží na problému a hypotéze, kterou jsme pro daný výzkumný problém stanovili. Z matematického hlediska nebude v této kapitole mnoho nového. Budeme využívat znalostí o výběrovém rozdělení parametru (viz 4) a pouze ho uvedme do kontextu statistického testu. 5.1 Postup statistického testování Každý statistický test má čtyři fáze: zvolení testu podle výzkumné otázky a typu dat, formulace nulové \\(H_0\\) a alternativní \\(H_1\\) hypotézy, výpočet testovací statistiky interpretace výsledku 5.1.1 Zvolení testu Pro toto cvičení budeme používat jednovýběrový t-test pro průměr. Tedy, budeme testovat hodnotu průměru jednoho výběru. Bude nás zajímat průměrná doba, kterou dospělí jedinci v České republice stráví denně na internetu (v minutách). Následující data pocházejí z European Social Study, která byla provedena v roce 2018. První sloupec obsahuje id respondenta a druhý počet minut kolik strávil denně na internetu. Pojďme se podívat na data. ess &lt;- read.csv(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/int_use.csv&quot;) summary(ess) ## idno int_use_day_mins ## Min. : 8403 Min. : 60.0 ## 1st Qu.:15605 1st Qu.: 97.5 ## Median :26020 Median :127.5 ## Mean :32457 Mean :147.1 ## 3rd Qu.:50716 3rd Qu.:172.5 ## Max. :63736 Max. :300.0 ## NA&#39;s :6 # odstranit chybejici hodnoty ess_bez_na &lt;- ess[!is.na(ess$int_use_day_mins), ] hist(ess_bez_na$int_use_day_mins, col = &quot;#1f77b4&quot;, main = &quot;&quot;, xlab = &quot;Počet min na internetu (týdně)&quot;, ylab = &quot;Četnost&quot; ) Graf 5.1: Histogram týdenného počtu minut na internetu Jak je vidět z 5.1, většina lidí tráví na internetu mezi 100 a 150 minutami týdně. Průměrný počet minut na internetu je 147.14. Jak je obvyklé u proměnných, které mají hranici na levé straně24, rozložení náhodné proměnné je zešikmené pozitivně, tedy má více odlehlých hodnot na pravo od průměru. Jak ale víme z CLV 4.4 výběrové rozdělení parametru (v našem případě budeme zkoumat průměr) náhodné proměnné bude normálně rozděleno, i když samotná náhodná proměnná normálně rozložená není. 5.1.2 Formulace nulové a alternativní hypotézy V této fázi formulujeme nulovou a alternativní hypotézu na základě naší výzkumné otázky. Tyto hypotézy formulujeme vždy před výpočtem testovací statistiky a vybíráme je tak, aby nám posloužili k možnému vyvrácení vědecké teorie. Technicky nám nic nebrání počítat různé testovací statistiky a poté vybrat tu, která vychází zajímavě, ale tímto způsobem se rychle dostaneme do nebezpečné situace. Statistické testy neslouží k objevování vztahů a proto by se tak neměly používat (viz 5.6)! My si ukážeme tři různé nulové hypotézy, abychom si ukázali rozdíl mezi dvojstranným, levostranným a pravostranným testem. Ve reálném výzkumu si ale vybereme pouze takovou hypotézu, které odpovídá výzkumné otázce (a která vychází z vědecké teorie). Dvojstranná hypotéza \\(H_0 = 180\\) \\(H_1 \\neq 180\\) Levostranná hypotéza \\(H_0 \\ge 180\\) \\(H_1 &lt; 180\\) Pravostranná hypotéza \\(H_0 \\le 180\\) \\(H_1 &gt; 180\\) 5.1.3 Výpočet testovací statistiky Výpočet testovací statistiky bude záležet na tom, který test zvolíme. Různé testy májí různé testovací statistiky a proto je důležité u zvoleného testu porozumět tomu, jakou testovací statistiku počítáme. Obecně ale platí, že každá testovací statistika se vypočítá jako \\(testovaci\\;statistika = \\frac{bodovy\\;odhad - nulova\\;hypoteza}{smerodatna\\;chyba\\;odhadu}\\) V našem případě jednovýběrového t-testu pro průměr je naší testovací statistikou t-skór. T-skór vypočítáme jako \\(t = \\frac{\\overline{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\), kde \\(\\overline{x}\\) je náš výběrový průměr, \\(\\mu_0\\) je hodnota z nulové hypotézy a \\(\\frac{s}{\\sqrt{n}}\\) je směrodatná chyba odhadu. Výběrový průměr spočítáme snadno a směrodatnou chybu odhadu výběrového průměru známe z centrální limitní věty a intervalů spolehlivosti. Testovací statistiku \\(t\\) budeme počítat s \\(n-1\\) stupni volnosti. # testovaci_statistika = bodovy_odhad - nulova_hypoteza / smerodatna_chyba_odhadu bodovy_odhad &lt;- mean(ess_bez_na$int_use_day_mins) nulova_hypoteza &lt;- 180 n &lt;- nrow(ess_bez_na) smerodatna_chyba_odhadu &lt;- sd(ess_bez_na$int_use_day_mins) / sqrt(n) testovaci_statistika &lt;- (bodovy_odhad - nulova_hypoteza) / smerodatna_chyba_odhadu Bodový odhad je výběrov průměr \\(\\overline{x}=\\) 147.14. Směrodatná chyba odhadu \\(s_{\\overline{x}}=\\) 19.33. Testovací statistika je \\(t=\\) -1.7. U každého statistického testu si zvolíme nějakou kritickou mez, která je určena chybou I. druhu \\(\\alpha\\). \\(\\alpha\\) nám udává pravděpodobnost, že nesprávně zamítneme nulovou hypotézu. Konvenčně se udává hladina \\(\\alpha = 0.05\\), ale my si zvolíme nějakou jinou, například \\(\\alpha=0.11\\), abychom si ukázali, že tato hladina je do jisté míry arbitrární a záleží na více věcech (jako je síla testu, typ chyby, kterou chcete akceptovat - I.druhu vs II.druhu. Více viz 5.4). Protože t-skór pochází ze studentova t standardního rozdělení s \\(n-1\\) stupni volnosti, můžeme ho vypočítat pomocí funkcí, které již známe (stejně jako jsme určovali u intervalů spolehlivosti). Kritická mez bude záležet na typu nulové a alternativní hypotézy. Pojďme si ukázat, jak bude kritická mez vypadat na grafu rozdělení testovací statistiky za předpokladu \\(H_0\\). Zároveň si do grafu zakreslíme hodnotu naší testovací statistiky. 5.1.4 Interpretace výsledku Modrá vertikální přímka značí kritickou mez, za kterou bychom zamítali \\(H_0\\). Modrý region pak značí region, kde zamítáme \\(H_0\\). Pokud se testovací statistika nachází v tomto regionu, zamítáme \\(H_0\\) ve prospěch \\(H_1\\). V opačném případě \\(H_0\\) zamítnout nemůžeme. Graf 5.2 ukazuje výběrové rozdělení průměru za předpokladu \\(H_0\\) pro levostranný test, graf 5.3 ukazuje výběrové rozdělení průměru za předpokladu platnosti \\(H_0\\) pro pravostranný test a 5.4 ukazuje výběrové rozdělení průměru za předpokladu platnosti \\(H_0\\) pro oboustranný test. alpha &lt;- 0.11 kritickou_mez_levostranny &lt;- qt(alpha, df = n - 1) x &lt;- seq(-4, 4, by = 0.001) pdf &lt;- dt(x, df = n - 1) # levostranna plot(x, pdf, xlab = &quot;Výběrový průměr (standardizovaný)&quot;, ylab = &quot;PDF&quot;, main = &quot;&quot;, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot; ) lines(x[x &lt; kritickou_mez_levostranny], pdf[x &lt; kritickou_mez_levostranny], type = &quot;h&quot;, col = &quot;#1f77b4&quot; ) abline(v = kritickou_mez_levostranny, col = &quot;#1f77b4&quot;) abline(v = testovaci_statistika, lty = 2) Graf 5.2: Rozdělení výběrového průměru za platnosti H0: levostranný # pravostrannou kritickou_mez_pravostranny &lt;- qt(1 - alpha, df = n - 1) plot(x, pdf, xlab = &quot;Výběrový průměr (standardizovaný)&quot;, ylab = &quot;PDF&quot;, main = &quot;&quot;, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot; ) lines(x[x &gt; kritickou_mez_pravostranny], pdf[x &gt; kritickou_mez_pravostranny], type = &quot;h&quot;, col = &quot;#1f77b4&quot; ) abline(v = kritickou_mez_pravostranny, col = &quot;#1f77b4&quot;) abline(v = testovaci_statistika, lty = 2) Graf 5.3: Rozdělení výběrového průměru za platnosti H0: pravostranný # oboustranny kritickou_mez_oboustranny &lt;- qt(c(alpha / 2, 1 - alpha / 2), df = n - 1) plot(x, pdf, xlab = &quot;Výběrový průměr (standardizovaný)&quot;, ylab = &quot;PDF&quot;, main = &quot;&quot;, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot; ) lines(x[x &lt; kritickou_mez_oboustranny[1] | x &gt; kritickou_mez_oboustranny[2]], pdf[x &lt; kritickou_mez_oboustranny[1] | x &gt; kritickou_mez_oboustranny[2]], type = &quot;h&quot;, col = &quot;#1f77b4&quot; ) abline(v = kritickou_mez_oboustranny, col = &quot;#1f77b4&quot;) abline(v = testovaci_statistika, lty = 2) Graf 5.4: Rozdělení výběrového průměru za platnosti H0: oboustranný Při oboustranné hypotéze: \\(H_0 = 180\\) a tedy \\(H_1 \\neq 180\\), je kritické mez +/- 1.715361 Naše testovací statistika má hodnotu -1.6994055 a není tedy menší než kritické mez. V takovém případě bychom nezamítali \\(H_0\\) ve prospěch \\(H_1\\). Při levostranné hypotéze: \\(H_0 \\ge 180\\) a \\(H_1 &lt; 180\\) je kritické mez -1.288595. Testovací statistika je menší než tato kritická hodnota a my proto zamítáme \\(H_0\\) ve prospěch \\(H_1\\). Při pravostranné hypotéze: \\(H_0 \\le 180\\) a \\(H_1 &gt; 180\\) je kritické mez 1.288595. Testovací statistika je menší než tato kritická hodnota a my proto nezamítáme \\(H_0\\) ve prospěch \\(H_1\\). Abychom nemuseli stále počítat hodnoty kritické meze, které se mohou lišit v závislosti na rozdělení výběrové statistiky a chybě prvního druhu (\\(alpha\\)), používá se tzv p-hodnota, což vyjadřuje pravděpodobnost, že uvidíme pozorovanou hodnotu nebo ještě extrémnější (za předpokladu \\(H_0\\)). P-hodnotu můžeme vypočítat jako distribuční funkci (CDF) podle toho, jaký typ \\(H_0\\) jsme zvolili. p_levostranny &lt;- pt(testovaci_statistika, df = n - 1) p_pravostranny &lt;- 1 - pt(testovaci_statistika, df = n - 1) p_oboustrany &lt;- (1 - pt(abs(testovaci_statistika), df = n - 1)) * 2 P-hodnota levostranného testu při alpha 0.11 je 0.06. P-hodnota pravostranného testu při alpha 0.11 je 0.94. P-hodnota oboustranného testu při alpha 0.11 je 0.11. Vidíme, že hodnota p je menší, než zvolená \\(\\alpha\\) pouze u levostranného testu, takže bychom zde zamítli \\(H_0\\) ve prospěch \\(H_1\\). Jak vidíme, výsledek se shoduje s porovnáním testovací statistiky k vypočtené kritické hladině. Výhodou je, že můžeme výpočet kritické hladiny přeskočit, rovnou vypočítat p-hodnotu a porovnat k námi zvolené \\(\\alpha\\). Ještě je důležité dodat, že pokud nemůžeme zamítnout \\(H_0\\), tak to neznamená, že hodnota parametru v populaci se rovná \\(H_0\\)25. Znamená to pouze, že v našich datech neexistuje dostatek evidence k tomu, abychom mohli na hladině spolehlivosti \\(1-\\alpha\\) řící, zda je výběrový parametr odlišný od \\(H_0\\). Pokud byste chtěli počítat jednovýběrový z-test pro průměr místo t-testu, pouze nahradíte rozdělení výběrového průměru za normální rozdělení (z-test ještě dále předpokládá, že známe populační rozptyl \\(\\sigma\\)). V praxi ale můžete použít t-test, protože víme, že při vetším počtu pozorování (\\(n&gt;50\\)) se t-rozdělení velmi podobá normálnímu. V R existuje na vypočítání t-testu funkce t.test, který má argumenty x náhodnou proměnnou, kterou testujeme, alternative určení typu \\(H_0\\) a \\(H_1\\), conf.level určení spolehlivosti testu a mu hodnotu \\(H_0\\), vůči které test provádíme. t_test_levostranna &lt;- t.test(ess_bez_na$int_use_day_mins, alternative = &quot;less&quot;, conf.level = 1 - alpha, mu = nulova_hypoteza ) t_test_pravostranny &lt;- t.test(ess_bez_na$int_use_day_mins, alternative = &quot;greater&quot;, conf.level = 1 - alpha, mu = nulova_hypoteza ) t_test_oboustranny &lt;- t.test(ess_bez_na$int_use_day_mins, alternative = &quot;two.sided&quot;, conf.level = 1 - alpha, mu = nulova_hypoteza ) 5.2 Velikost účinku Statistické testování nám prozradí pouze směr účinku naší hypotézy, nikoliv však její sílu/efekt. K tomu nám slouží Cohenovo d. Vypočítá se jako \\(d = \\frac{|(bodovy\\;odhad - H_0)|}{smerodatna\\;odchylka\\;mereni}\\). Všimněte si, že \\(d\\) je vlastně standardizovaným rozdílem mezi bodovým odhadem a \\(H_0\\). Obecně se udává následující síly účinku pro hodnoty \\(d\\): &lt; 0.2 0.2 - 0.5 0.5 - 0.8 0.8 - 1.2 &gt; 1.2 velmi malý malý efekt střední efekt velký efekt velmi velky Pro náši úlohu by se \\(d\\) vypočítalo jako: d &lt;- abs(bodovy_odhad - nulova_hypoteza) / sd(ess_bez_na$int_use_day_mins) Velikost účinku je 0.45, což odpovídá spíše malému efektu. 5.3 Souvislost s intervalem spolehlivosti Konstrukt intervalu spolehlivosti a statistických testů vychází ze stejné matematické teorie a to sice, že nejistota ohledně bodového výběrového odhadu se dá modelovat pomocí standardní chyby odhadu \\(\\frac{s}{\\sqrt{n}}\\). Graf 5.5 ukazuje výběrové rozdělení průměru, které jsme počítali v 4 a výběrové rozdělení za platnosti \\(H_0\\). Všimněte si, že p-hodnota je vlastně \\(P(x_i \\le \\overline{x})\\) za platnosti H0. Tedy, pokud platí rozdělení parametru za H0 (pokud je průměr roven naší \\(H_0\\)), jaká je pravděpodobnost, že bychom v našem výběru sledovali tuto hodnotu nebo extrémnější. Matematicky to je tento výpočet stejný, jako levostranný test, který jsme počítali nahoře26. Statistické testování tedy z matematického hlediska nepřináší žádné nové koncepty a používá teorii CLV 4 a pravděpodobnostních rozdělení 3.3. # vypocitame vyberove rozdeleni parametru... # ..pomoci zobecneneho t rozdeleni x &lt;- seq(80, 250, length.out = 1000) # stpne volnosti df &lt;- nrow(ess_bez_na) - 1 # vypocitame pdf pdf_vyber &lt;- dt((x - bodovy_odhad) / smerodatna_chyba_odhadu, df = df ) * 1 / smerodatna_chyba_odhadu h0 &lt;- 180 # spocitame vyberove rozdeleni za predpokladu H0 pdf_h0 &lt;- dt((x - h0) / smerodatna_chyba_odhadu, df = df ) * 1 / smerodatna_chyba_odhadu # vypocitame p hodnotu jako cdf za predpokladu h0 p_hodnota &lt;- pt((bodovy_odhad - h0) / smerodatna_chyba_odhadu, df = df) plot(x, pdf_vyber, xlab = &quot;Počet min na internetu (týdně)&quot;, ylab = &quot;PDF&quot;, col = &quot;#1f77b4&quot;, main = paste0(&quot;Za platnosti H0: P(x&lt;&quot;, round(bodovy_odhad, 2), &quot;)=&quot;, round(p_hodnota, 2)), type = &quot;l&quot;, lwd = 2 ) lines(x, pdf_h0, col = &quot;grey&quot;, lwd = 2) abline(v = bodovy_odhad, lty = 2) abline(v = h0, lty = 2, col = &quot;grey&quot;) legend(&quot;topright&quot;, legend = c(paste0(&quot;Výběr. rozděl. průměru; \\n T(&quot;, round(bodovy_odhad, 2), &quot;, &quot;, round(smerodatna_chyba_odhadu, 2), &quot;, &quot;, df, &quot;)&quot;), &quot;Bodový odhad&quot;, paste0(&quot;Výběr. rozděl. H0; \\n T(&quot;, h0, &quot;, &quot;, round(smerodatna_chyba_odhadu, 2), &quot;, &quot;, df, &quot;)&quot;), paste0(&quot;H0: &quot;, h0)), col = c(&quot;#1f77b4&quot;, &quot;black&quot;, &quot;grey&quot;, &quot;grey&quot;), lwd = c(2, 1, 2, 1), lty = c(1, 2, 1, 2), cex = 0.7 ) Graf 5.5: Výběrové rozdělení průměru a výběrové rozdělení za platnosti H0 5.4 Síla testu Proč nezvolit chybu I. druhu \\(\\alpha\\) vždy velmi malou, abychom si byli jisti, že pravděpodobnost našich dat při platnosti \\(H_0\\) je velmi malá? K tomu, abychom pochopili, jakou \\(\\alpha\\) zvolit si musíme představit i chybu II. druhu \\(\\beta\\). Chyba I. druhu \\(\\alpha\\) nám určuje pravděpodobnost, že zamítneme \\(H_0\\), pokud ve skutečnosti \\(H_0\\) je platná. Někdy se taková chyba označuje jako false positive (FP). Situace, kdy správně zamítneme \\(H_0\\) a ona ve skutečnosti neplatí, se někdy označuje jako tru positive (TP). Teď si představme situaci, v které \\(H_0\\) nezamítneme, ale ona ve skutečnosti neplatí (tedy platí \\(H_1\\)). Takové situace se někdy říká false negative (FN). A právě tuto chybu vyjadřuje chyba II. druhu \\(\\beta\\). Síla testu se vypočítá jako \\(1-\\beta\\) a označuje se tedy TP. Síla testu nám říká pravděpodobnost, s kterým test správně určí \\(H_1\\), pokud ve skutečnosti rozdíl existuje. Tabulka dole ukazuje všechny situace, které mohou nastat a jak se tyto situace vztahují na chyby I. a II. druhu27. Tabulka: Ukázka výsledků testů v zavislosti na skutečnosti. Závěr testu Skutečnost \\(H_0\\) platí \\(H_0\\) neplatí \\(H_0\\) platí TN (\\(1-\\alpha\\)) FP (\\(\\alpha\\)) \\(H_0\\) neplatí FN (\\(\\beta\\)) TP (\\(1-\\beta\\)) Velikost chyby I. druhu je úměrná chybě II. druhu. Chyba II. druhu nám udává pravděpodobnost, že nezamítneme \\(H_0\\), pokud ve skutečnosti platí \\(H_1\\). Aplikace 5.6 ukazuje jak spolu souvisí tyto dvě chyby v případě jednovýběrového z-testu28. Graf udává rozdělení parametru za platnosti \\(H_0\\) a rozdělení za platnosti \\(H_1\\). Červena plocha pod rozdělením \\(H_0\\) ukazuje oblast zamítnutí \\(H_0\\) při platnosti \\(H_0\\) (tedy \\(\\alpha\\)) a modrá plocha ukazuje oblast nezamítnutí \\(H_0\\) při platnosti \\(H_1\\). Jak si můžeme vyzkoušet v aplikaci, při snížení chyby I. druhu \\(\\alpha\\), vzroste chyba II. druhu \\(\\beta\\) a naopak. Zároveň si můžeme všimnout, že při zvolení oboustranného testu se chyba II. druhu \\(\\beta\\) při stejné \\(\\alpha\\) sníží a to z toho důvodu, že posuneme hranici, kde zamítáme \\(H_0\\). Dále si všimněme, že velikost \\(\\beta\\) závisí na standarní chybě odhadu, která určuje šířku obou rozdělení parametru29. Čím je standardní chyba odhadu menší, tím je menší \\(\\beta\\). Dále závisí chyba II. druhu \\(\\beta\\) na vzdálenosti průměru \\(H_0\\) od \\(H_1\\). Čím je tato vzdálenost větší, tím je chyba II. menší. Graf 5.6: Ukázka závislosti chyby I. a II. druhu. Zdroj: https://shiny.rit.albany.edu/stat/betaprob/ Jakým způsobem vybrat velikost \\(\\alpha\\) bude záležet na problému, v kterém test provádíme. Jaké důsledky má chyba I. druhu (FP) a chyba II.druhu (FN)? Co hrozí pokud na základě testu rozhodneme určíme špatně, že něco je pravda, pokud to ve skutečnosti pravda není (FP) a co hrozí, pokud určíme špatně, že něco pravda není, pokud to ve skutečnosti pravda je (FN)? To, jak závažné důsledky mají FP a FN závisí na situaci, v které test aplikujeme. Uvažujme příklad, kdy pacientovy odebereme vzorky krve a sledujeme hodnoty určitých látek. Na základě statistického testu potom vyhodnotíme, zda množství látek indikuje nějakou nemoc. Důsledek FP je, že pacientovi řekneme, že má nemoc, pokud ji ve skutečnosti nemá. Pokud léčení takové nemoci není nákladné a nemá žádné vedlejší účinky, pak se nic tak velkého nestalo. Důsledek FN ve stejném příkladu znamená, že pacientovi řekneme, že nemoc nemá, ale on ji ve skutečnosti má. Pokud má nemoc závažné důsledky, pak může taková chyba způsobit pacientovi závažné zdravotní komplikace. V takovém případě bychom chtěli, aby chyba FN (\\(\\beta\\)) byla poměrně malá. Nyní si představme situaci, v které používáme statistický test, abychom určili, zda někdo podvádí ve výkazu nákladů na pracovní cestu. Na kontrolu máme zhruba 14 dní, protože náklady musíme zaměstnanci uhradit v příští výplatě. Řekněme, že testujeme určitou kombinaci čísel, které se ve výkazu opakují. Pokud označíme nějaké daňové přiznání za podvodné, musíme podrobně zkontrolovat všechny faktury a účtenky v daném výkazu a to je velmi pracné. Protože máme jenom omezený počet dní, v kterých můžeme výkazy kontrolovat, budume chtít, abychom si byli docela jisti, že někdo podvádí. V takovém případě je důsledek FP velmi náročný na čas a budeme se proto snažit \\(\\alpha\\) určit tak, aby nebylo mnoho FP. 5.5 Velikost výběru Teorii statistického testování můžeme využít při designu výzkumu k tomu, abychom stanovili nutnou velikost výběru. Vycházíme přitom z toho, že velikost výběru je jednou z proměnných při výpočtu směrodatné chyby odhadu parametru \\(\\frac{s}{\\sqrt{n}}\\). Zpravidla stanovíme, jakou chybu odhadu jsme schopni akceptovat \\(\\delta\\) a podle toho určíme velikost výběru \\(n\\). Velikost výběru potom vypočítáme jako \\(n=(\\frac{z_{1-\\alpha} * \\sigma}{\\delta})^2\\), tedy jako hodnotu kvantilu \\(z\\) rozdělení na hladině spolehlivosti \\(1-\\alpha\\) vynásobenou populační směrodatnou odchylkou náhodné proměnné \\(\\sigma\\). Pokud populační směrodatnou odchylku neznáme, můžeme ji odhadnout pomocí dostupných znalostí o zkoumané proměnné, nebo simulovat různá rozdělení a vybrat takové \\(s\\), které nejvíce odpovídá našim předpokladům. delta &lt;- 0.01 smer_odchylka &lt;- sqrt(0.05 * (1 - 0.05)) z &lt;- qnorm(0.975) # vypocitame n n &lt;- (z * smer_odchylka / delta)^2 # zaokrouhlime n &lt;- round(n) Při odhadu volebních preferencí bychom chtěli, aby chyba odhadu u malých stran (preference okolo 5%) při \\(\\alpha=0.05\\) byla +/- 1 p.b. Pak bychom počítali \\(n=(\\frac{z_{0.975}*\\sqrt{0.05(1-0.05)}}{0.01})^2\\). K tomu, abychom u malých stran měli interval spolehlivosti pro hladinu spolehlivosti \\(1-\\alpha=\\) 0.95 s +/- 1 p.b, pak bychom potřebovali výběr o velikosti 1825. 5.6 Mnoho testů Jak jsme zmínili v 5.1.2, \\(H_0\\) a \\(H_1\\) je potřeba zvolit před použitím statistického testu. V případě, že používáme statistické testy pro nalezení vztahů mezi více proměnnými, narazíme na problém s FP. Abychom si tento případ ukázali, vytvoříme data, kde jsou na sobě všechny proměnné nezávislé. Každá proměnná pochází z rozdělení \\(N(0, 1)\\). U každé proměnné budeme testovat, zda platí \\(H_0: \\mu = 0\\) a \\(H_1: \\mu \\ne 0\\). Zajímá nás, kolik statisticky významných vztahů bychom v našich datech našli. # vytvorime nahodna data o 1000 pozorovanich a 200 promennych n &lt;- 1000 k &lt;- 200 M &lt;- sapply(1:k, function(x) rnorm(n)) alpha &lt;- 0.05 # budeme ukaladat p hodnotu p_hodnota &lt;- rep(NA, k) for (i in 1:k) { # provedeme test test t_test &lt;- t.test( x = M[, i], alternative = &quot;two.sided&quot;, conf.level = 1 - alpha, mu = 0 ) # ulozime p-hodnotu p_hodnota[i] &lt;- t_test$p.value } # pocet false positive FP &lt;- sum(p_hodnota &lt;= alpha) # bonferroniho korekce p_new &lt;- alpha / k FP_new &lt;- sum(p_hodnota &lt;= p_new) Při počtu proměnných 200 provádíme 200 statistických testů. V našich datech jsme našli 14 statisticky významných výsledků (0.07% ze všech provedených testů). Tedy 14 false positive testů. Samozřejmě, v našich datech pocházejí všechny proměnné z rozdělení s průměrem 0 a jakýkoliv vztah je pouze v důsledku náhody. Podíl FP testů bude zhruba odpovídat zvolené chybě \\(\\alpha\\). Jedním ze způsobů, jak se s tímto problémem vypořádat je pomocí Bonferroniho korekce, která bere v potaz počet provedených testů a upraví statisticky významnou p-hodnotu na novou hodnotu \\(p_i \\le \\frac{\\alpha}{m}\\), kde \\(m\\) je počet provedených testů. Pokud bychom bonferroniho korekci provedli na našem případu, pak by nová hodnota \\(p_i\\), za kterou zamítáme \\(H_0\\) byla 0. Počet statisticky významných testů by tak byl 0. Jak ale víme z kapitoly 5.4, snižování chyby I. druhu \\(\\alpha\\) vedek nárůstu FN, tedy chyby II. druhu \\(\\beta\\)30. Pokud je cílem pouze exploratorní analýza velkého počtu dat, je možné použít očekávanou hodnotu pozitivních testů jako kritérium pro určení toho, zda některé z pozitivních testů jsou TP. Počet pozitivních testů je náhodná proměnná a lze ji modelovat poissonových rozdělením 3.6. V našem případě při \\(\\alpha=\\) 0.05 bychom z 200 testů očekávali 10 pozitivních testů jenom vlivem náhody. Graf 5.7 ukazuje očekávané rozdělení pro \\(Poisson(\\) 10 \\()\\). Můžeme potom vyučít CDF, abychom vypočítali pravděpodobnost, že z 200 testů jich bude právě \\(q\\) pozitivních, tedy \\(P(x &gt; q)\\). x &lt;- 0:30 lambda &lt;- alpha * k pmf &lt;- dpois(x, lambda) plot(x, pmf, type = &quot;h&quot;, lwd = 5, col = &quot;#1f77b4&quot;, xlab = &quot;s&quot;, ylab = &quot;PMF&quot;, main = paste0(&quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot;) ) Graf 5.7: Očekávaný počet pozitivních testů z 200 cdf &lt;- ppois(FP, lambda, lower.tail = FALSE) V našem případě, kdy máme 14 pozitivních testů, je \\(P(x &gt;\\) 14 \\()=\\) 0.08. V případě, že by některé pozitivní testy byly TP, by tato pravděpodobnost byla malá. My bychom potom například mohli vybrat testy s nejmenší p-hodnotou a tyto proměnné dále prozkoumat. Více o tématu falzifikace např. zde.↩︎ Nelze trávit méně než 0 minut.↩︎ Velmi pravděpodobně hodnota parametru nebude rovna právě \\(H_0\\).↩︎ V případě pravostranného testu bychom počítali \\(P(x_i \\ge \\overline{x})\\). V případě oboustranného testu bychom počítali \\(P(x_i \\le \\overline{x})\\) &amp; \\(P(x_i \\ge \\overline{x})\\).↩︎ TP = true positive, TN = true negative, FP = false positive, FN = false negative.↩︎ Tedy jako t-test s větším počtem pozorování↩︎ Tedy za předpokladu \\(H_0\\) a za předpokladu \\(H_1\\).↩︎ Řekneme, že nelze vyvrátit \\(H_0\\), i když ve skutečnosti neplatí.↩︎ "],["porovnání-dvou-průměrů-a-relativních-četností.html", "Kapitola 6 Porovnání dvou průměrů a relativních četností 6.1 Průměry 6.2 Relativní četnosti", " Kapitola 6 Porovnání dvou průměrů a relativních četností V této kapitole si představíme, jak porovnat dva průměry nebo relativní četnosti náhodných proměnných. Porovnání dvou průměrů je častou metodou při experimentech, kdy jsou pozorování rozdělena náhodně do dvou skupin, jedna ze skupin je vystavena nějaké intervenci, kterou zkoumáme (tzv. experimentální skupina), druhá je vystavena placebo (tzv. kontrolní skupina) a potom jsou výsledky měřené proměnné porovnány za obě skupiny31. Výsledné měření je náhodnou proměnnou, protože jsme pozorování náhodně rozřadili do kontrolní a experimentální skupiny. Náhodné rozřazení je přitom důležité, protože měřená proměnná může být ovlivněna různými faktory, které jsou díky náhodnému rozdělení přiřazeny do kontrolní a experiemntální skupiny zhruba ve stejném počtu. Typickým příkladem náhodného experimentu jsou klinické testy, v kterých se posuzuje účinnost léků. Kromě náhodnosti rozdělení do obou skupin předpokládáme také, že pozorování jsou na sobě nezávislá. 6.1 Průměry 6.1.1 Dva nezávislé výběry Při zkoušce máme připravené dvě verze testu. Studentům náhodně přiřadíme jednu z verzí. Tabulka dole ukazuje popisné statistiky našich dat. dats &lt;- read.csv(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/test.csv&quot;, stringsAsFactors = TRUE) summary(dats) ## id verze hodnotitel body ## Min. : 1 verze_1:8 hodnotitel_1:8 Min. :15.00 ## 1st Qu.: 5 verze_2:9 hodnotitel_2:9 1st Qu.:28.50 ## Median : 9 Median :44.50 ## Mean : 9 Mean :41.62 ## 3rd Qu.:13 3rd Qu.:49.50 ## Max. :17 Max. :65.50 Testu se celkem zúčastnilo 17 studentů. 8 obrželo verzi 1 a 9 obdrželo verzi 2. Zajímá nás, zda byly obě verze testu stejně obtížné. Obtížnost testů budeme měřit průměrným počtem bodů pro každou verzi testu. Graf 6.1 ukazuje rozdělení počtu bodů podle verze testu. Z grafu je zřejmé, že verze 2 bude mít vyšší průměrný počet bodů, ale také větší rozptyl ve výsledcích. # vytvorime barvy cols &lt;- rev(RColorBrewer::brewer.pal(3, name = &quot;Blues&quot;)) plot(dats$body[dats$verze == &quot;verze_1&quot;], rep(1.5, sum(dats$verze == &quot;verze_1&quot;)), xlim = c(10, 70), ylim = c(0, 2), pch = 19, main = &quot;&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, col = adjustcolor(cols[1], alpha.f = 0.8) ) points(dats$body[dats$verze == &quot;verze_2&quot;], rep(1, sum(dats$verze == &quot;verze_2&quot;)), pch = 19, col = adjustcolor(cols[2], alpha.f = 0.8) ) legend(&quot;topright&quot;, legend = c(&quot;Verze 1&quot;, &quot;Verze 2&quot;), col = cols[1:2], pch = c(19, 19), cex = 0.7 ) Graf 6.1: Rozdělení počtu bodů podle verze testu Počet bodů je náhodná proměnná. Závisí na studentech, kterým byla verze náhodně přiřazena. Studenti mají různou úroveň znalostí, ale průměrný počet bodů bude ovlivněn i tím, že např. některý student ráno zaspal a byl ve stresu nebo tím, že se někdo špatně vyspal. Protože přiřazujeme verze testu náhodně jsou i tyto efekty náhodně rozptýleny do obou skupin. Pokud jsou obě verze stejně obtížné, očekávali bychom, že studenti v obou skupinách budou mít stejný průměrný počet bodů. Jak víme z 4, rozložení průměru náhodné proměnné \\(x_i\\) lze popsat pomocí \\(T(\\overline{x}, \\frac{s}{\\sqrt{n}}, n-1)\\). V našem případě máme \\(T(\\overline{x_1}, \\frac{s_1}{\\sqrt{n_1}}, n_1-1)\\) a \\(T(\\overline{x_2}, \\frac{s_2}{\\sqrt{n_2}}, n_2-1)\\). Pojďme se tedy podívat na výběrové rozdělení obou průměrů. Abychom nemuseli výběrové statistiky počítat samostatně pro každou skupinu, použijeme k výpčtu funkce sapply, která je zkrácenou formou for loop32 a funkci aggregate, která vypočítá zadanou funkci pro zvolené skupiny. # vyvorime si funkci na vypocet smerodatne chyby odhadu se &lt;- function(x) { n &lt;- length(x) sd(x) / sqrt(n) } # pro kazdou funkci vypocitame aggregate vyber_stats &lt;- sapply(c(length, mean, sd, se), function(f) aggregate(body ~ verze, FUN = f, data = dats)[, 2]) # pojmenujeme sloupce colnames(vyber_stats) &lt;- c(&quot;n&quot;, &quot;mu&quot;, &quot;sd&quot;, &quot;se&quot;) # zvolime x pro ktere budeme pocitat PDF x &lt;- seq(from = 20, to = 65, length.out = 1000) # vypocitame PDF vyberoveho rozdeleni prumeru 1 podle zobecneneho t-rozdeleni pdf_prumer_1 &lt;- dt((x - vyber_stats[1, &quot;mu&quot;]) / vyber_stats[1, &quot;se&quot;], df = vyber_stats[1, &quot;n&quot;] - 1 ) * 1 / vyber_stats[1, &quot;se&quot;] # vypocitame PDF vyberoveho rozdeleni prumeru 2 podle zobecneneho t-rozdeleni pdf_prumer_2 &lt;- dt((x - vyber_stats[2, &quot;mu&quot;]) / vyber_stats[2, &quot;se&quot;], df = vyber_stats[2, &quot;n&quot;] - 1 ) * 1 / vyber_stats[2, &quot;se&quot;] # vytvorime barvy cols &lt;- rev(RColorBrewer::brewer.pal(3, name = &quot;Blues&quot;)) # zobrazime PDF plot(x, pdf_prumer_1, type = &quot;l&quot;, lwd = 2, xlab = &quot;Body&quot;, ylab = &quot;PDF&quot;, col = cols[1], main = &quot;&quot; ) lines(x, pdf_prumer_2, type = &quot;l&quot;, lwd = 2, col = cols[2] ) # pridame legendu legend(&quot;topright&quot;, legend = c( paste0( &quot;Verze 1 ~ T(&quot;, round(vyber_stats[1, &quot;mu&quot;], 2), &quot;, &quot;, round(vyber_stats[1, &quot;se&quot;], 2), &quot;, &quot;, vyber_stats[1, &quot;n&quot;] - 1, &quot;)&quot; ), paste0( &quot;Verze 2 ~ T(&quot;, round(vyber_stats[2, &quot;mu&quot;], 2), &quot;, &quot;, round(vyber_stats[2, &quot;se&quot;], 2), &quot;, &quot;, vyber_stats[2, &quot;n&quot;] - 1, &quot;)&quot; ) ), col = cols[1:2], lwd = c(2, 2), cex = 0.7 ) Graf 6.2: Výběrové rozdělení průměrů pro každou skupinu Graf 6.2 zobrazuje výběrové rozdělení průměrů obou skupin. Průměrný počet bodů první verze je 36.88. Průměrný počet bodů druhé verze je 45.83, tedy rozdíl -8.95. Jak je ale zřejmé z grafu 6.2 ohledně obou výběrových průměrů panuje hodně nejistoty, což bychom očekávali vzhledem k tomu, že máme malý počet pozorování v každé skupině a vzhledem k tomu, že existuje velké množství faktorů, které počet bodů ovlivňují. My jsme díky náhodnému přiřazení verzí tyto faktory rozdělili do obou skupin a to se projevuje na velké směrodatné odchylce počtu bodů v obou skupinách. Abychom zjistili, zda jsou rozdíly v průměrech skutečné a ne pouze díky náhodě, musíme zjistit, zda je jejich populační rozdíl rovný nule33, tedy \\(\\mu_1 = \\mu_2\\) nebo \\(\\mu_1 - \\mu_2 = 0\\). Protože populační průměry obou verzí neznáme počítáme s výběrovým průměrným počtem bodů, tedy \\(N(\\overline{x_1}, \\frac{s_1}{\\sqrt{n_1}}) - N(\\overline{x_2}, \\frac{s_2}{\\sqrt{n_2}})\\). Z kapitoly 3.3 víme, že když odečítáme dvě normálně rozložené proměnné, pak bude nové proměnná mít normální rozložení s parametry \\(N(\\overline{x_1} - \\overline{x_2}, \\sqrt{(\\frac{s_1}{\\sqrt{n_1}})^2 + (\\frac{s_2}{\\sqrt{n_2}})^2})\\). Pokud je náhodná proměnná rozdělena podle t-rozdělení (jako v našem případě), pak bude počet stupňů volnosti \\(\\upsilon = n_1 + n_2 - 2\\). prumer_d &lt;- vyber_stats[1, &quot;mu&quot;] - vyber_stats[2, &quot;mu&quot;] se_d &lt;- sqrt(vyber_stats[1, &quot;se&quot;]^2 + vyber_stats[2, &quot;se&quot;]^2) df &lt;- vyber_stats[1, &quot;n&quot;] + vyber_stats[2, &quot;n&quot;] - 2 x &lt;- seq(-32, 15, length.out = 1000) pdf_d &lt;- dt((x - prumer_d) / se_d, df = df) * 1 / se_d q &lt;- 0 p_0 &lt;- pt((q - prumer_d) / se_d, df = df, lower.tail = FALSE) plot(x, pdf_d, type = &quot;l&quot;, lwd = 2, xlab = &quot;Rozdíl v průměru bodů&quot;, ylab = &quot;PDF&quot;, col = &quot;grey&quot;, main = paste0(&quot;P(x &gt; 0)=&quot;, round(p_0, 2)) ) # pridame integral plochy pro P(X &gt; 0) lines(x[x&gt;q], pdf_d[x&gt;q], type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 1) legend(&quot;topright&quot;, legend = paste0( &quot;T(&quot;, round(prumer_d, 2), &quot;, &quot;, round(se_d, 2), &quot;, &quot;, df, &quot;)&quot; ), col = &quot;grey&quot;, lwd = 2, cex = 0.7 ) Graf 6.3: Výběrové rozdělení rozdílů v průměre verze 1 a verze 2 Graf 6.3 ukazuje výběrové rozdělení rozdílu. Protože při odečetení dvou normálně rozdělených proměnných se jejich směrodatná odchylka sčítá, promítá se původní nejistota výběrových průměrů i do výběrověho rozdělení rozdílu obou průměrů. Směrodatná chyba odhadu je 6.11. Pravděpodobnost, že náš výběrový rozdíl nabyde hodnot větších než 0 je \\(P(x_i &gt; 0)=\\) 0.08. Přestože je tato pravděpodobnost malá, rozhodně není zanedbatelná. Je možné, že verze 1 byla těžší, ale na základě našich dat je těžké určit, zda tomu opravdu tak je, nebo zda je to pouze náhoda. alpha &lt;- 0.05 p &lt;- c(alpha / 2, 1 - alpha / 2) i_s &lt;- prumer_d + qt(p, df = df) * se_d Interval spolehlivosti pro rozdíl v průměrném počtu bodů obou verzí na hladině spolehlivosti 95% je -21.97, 4.06. Naši původní otázku o odlišnosti obou verzí bychom mohli formulovat pomocí statistického testu. Nulová hypotéza by v takovém případě byla, že oba průměrný počet bodů obou verzí je stejný \\(H_0: \\mu_1 = \\mu_2\\) a alternativní hypotéza by byla, že stejné nejsou \\(H_1: \\mu_1 \\ne \\mu_2\\). Testovací statistiku vypočítáme podle principu vzorce z 5.1.3, pčičemž našim výběrovým parametrem není průměr, ale rodíl průměrů \\(\\overline{d}\\). Směrodatná chyba pdhadu je stejná jako v textu nahoře, tedy \\(se = \\sqrt{(\\frac{s_1}{\\sqrt{n_1}})^2 + (\\frac{s_2}{\\sqrt{n_2}})^2}\\). Testovací statistiku tedy vypočítáme \\(t = \\frac{\\overline{d} - H_0}{se}\\). h0 &lt;- 0 t &lt;- (prumer_d - h0) / se_d p &lt;- pt(t, df = df) * 2 Testovací statistika je rovná -1.47, což při platnosti \\(H_0\\) a oboustranném testu vychází na p-hodnotu 0.16. Tedy, pravděpodobnost, že bychom zaplatnosti \\(H_0\\) (tedy žádného rozdílu mezi průměry) sledovali rozdíl -8.96 nebo extrémnější34 je 0.16. Stejně jako v kapitole 5 můžeme použít funkci t.test. Argumenty zůstavají stejné, pouze je potřeba definovat proměnné pro které počítáme průměrný rozdíl. To je možné pomocí formula. t.test(body ~ verze, data = dats) ## ## Welch Two Sample t-test ## ## data: body by verze ## t = -1.4671, df = 14.768, p-value = 0.1633 ## alternative hypothesis: true difference in means between group verze_1 and group verze_2 is not equal to 0 ## 95 percent confidence interval: ## -21.990993 4.074326 ## sample estimates: ## mean in group verze_1 mean in group verze_2 ## 36.87500 45.83333 Výpočet velikosti účinku zůstává stejný, tedy \\(d = \\frac{|(bodovy\\;odhad - H_0)|}{smerodatna\\;odchylka\\;mereni}\\). V případě porovnání dvou průměrů je směrodatnou odchylkou měření \\(\\sqrt{s_1^2 + s_2^2}\\). &lt; 0.2 0.2 - 0.5 0.5 - 0.8 0.8 - 1.2 &gt; 1.2 velmi malý malý efekt střední efekt velký efekt velmi velky d &lt;- abs(prumer_d - h0) / sqrt(vyber_stats[1, &quot;sd&quot;]^2 + vyber_stats[2, &quot;sd&quot;]^2) V našem případě je Cohenovo d rovno 0.5, což odpovídá malému efektu. 6.1.2 Dva závislé výběry Pokud jsou pozorování v prvním a druhém výběru závislá, tedy většinou to jsou stejná pozorování, bude výpočet společného směrodatné odchylky rozdílu jiný než v 6.1.1. Protože pozorování v obou výběrech jsou stejná, jsou jejich směrodatné odchylky závislé a proto směrodatná odchylka nové proměnné rozdílu nebude součetem směrodatných odchylek prvního a druhého výběru. Pokud máme dva výběry se stejnými pozorováními a s proměnnými \\(x_i\\) a \\(y_i\\) můžeme vypočítat u každého pozorování rozdíl \\(d_i\\) jako \\(d_i=x_i - y_i\\). Směrodatnou chybu rozdílu vypočít jako \\(s_\\overline{d} = \\frac{s_d}{\\sqrt{n}}\\), kde \\(s_d\\) je směrodatná odchylka \\(x_i - y_i\\). Počet stupňů volnosti vypočítáme jako \\(\\upsilon = n-1\\). Tedy výběrový průměrný rozdíl bude \\(T(\\overline{d}, \\frac{s_d}{\\sqrt{n}}, n-1)\\). Jako příklad si ukážeme data z výzkumu trhu. 20 respondentů bylo dotázáno, na jaké značky si vzpomenou. Poté jim bylo ukázáno několik reklam. Po 2 dnech byli stejní respondenti dotázáni opět na značky, které si pamatují. Sloupec pre označuje počet značek, které si respondent zapamatoval. post vyjadřuje počet značek, které si zapamatoval poté, co mu byly ukázány reklamy. Bude nás zajímat, zda reklamy, které respondent viděl měli pozitivní vliv na počet značek, na které si respondent vzpoměl. ads &lt;- read.csv(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/ads.csv&quot;) Průměrný počet značek, které si respendenti vybavili před shlédnutím reklam je 4.25, počet značek, které si respondenti vybavili po shlédnutí reklam je 5.8. Zajímá nás, zda je tento rozdíl díky náhodě, nebo zda bychom podobný rozdíl viděli i při opakování experimentu. Graf 6.4 zobrazuje výběrové rozdělení prům2rného rozdílu počtu značek, které respodenti zmínili před a po shlédnutí reklam. n &lt;- nrow(ads) # rozdil d_i &lt;- ads$pre - ads$post # prumerny rozdil d &lt;- mean(d_i) # smerodatna odchylka rozdilu sd_d &lt;- sd(d_i) # smerodatna chyba rozdilu se_d &lt;- sd_d / sqrt(n) # vypocitame pdf vyberového rozdeleni x &lt;- seq(-3, 0, length.out = 1000) pdf_d &lt;- dt((x-d)/se_d, df = n-1) * 1/se_d plot(x, pdf_d, type = &quot;l&quot;, lwd = 2, xlab = &quot;Rozdíl v počtu značek&quot;, ylab = &quot;PDF&quot;, col = &quot;#1f77b4&quot;, main = &quot;&quot; ) legend(&quot;topright&quot;, legend = paste0( &quot;T(&quot;, round(d, 2), &quot;, &quot;, round(se_d, 2), &quot;, &quot;, n-1, &quot;)&quot; ), col = &quot;#1f77b4&quot;, lwd = 2, cex = 0.7 ) Graf 6.4: Výběrové rozdělení průměru dvou závislých výběrů alpha &lt;- 0.05 q &lt;- qt(c(alpha/2, 1-alpha/2), df = n-1) i_s &lt;- d + q * se_d Interval spolehlivosti pro výběrový rozdíl na hladině spolehlivosti 95% je [-2.57, -0.53]. Je tedy menší než 0 a můžeme usuzovat, že pokud bychom experimenty opakovali dokola, pak v 95% případů by byl výběrový rozdíl v těchto mezích (a tedy zároveň menší než 0). Řekněme, že máme hypotézu, že průměrný počet značek se zvýší o 1 a více. Pak bychom mohli formulovat hypotézy: \\(H_0: \\mu_{pre} - \\mu_{post} \\ge -1\\) \\(H_1: \\mu_{pre} - \\mu_{post} &lt; -1\\) # stanovime H0 h0 &lt;- -1 # vypocitame t statistiku t &lt;- (d-h0) / se_d # vypocitame p-hodnotu p_0 &lt;- pt(t, df = n-1) Za předpokladu výběrového rozdělení \\(H_0=\\) -1 je pravděpodobnost, že bychom viděli hodnotu průměrného rozdílu -1.55 nebo extrémnější rovna 0.14 a tedy není menší než námi zvolená hladina 0.05 a nemohli bychom zamítnout \\(H_0\\) ve prospěch \\(H_1\\), že respondenti si po shlédnutí bloku reklam pamatují o jednu nebo více značek. Ještě si ukážeme, jak bychom tento příklad vypočítali pomocí funkce t.test. t.test(x = ads$pre, y = ads$post, conf.level = 1-alpha, mu = h0, paired = TRUE, alternative = &quot;less&quot;) ## ## Paired t-test ## ## data: ads$pre and ads$post ## t = -1.1242, df = 19, p-value = 0.1375 ## alternative hypothesis: true difference in means is less than -1 ## 95 percent confidence interval: ## -Inf -0.7040632 ## sample estimates: ## mean of the differences ## -1.55 6.2 Relativní četnosti Stejně jako je tomu u porovnání dvou průměrů, je porovnání dvou relativních četností rozšířením konceptů, které jsme si ukazovali v předchozích kapitolách. My si porování dvou relativních četností ukážeme na příkladu A/B testování. A/B testování je klasická marketingová metoda (v souvislosti s počítačovými programy také testovací metoda &gt;či metoda pro testování použitelnosti aplikací), jež má za cíl zvýšit konverze či konverzní poměry &gt;projektu prostřednictvím změny jednoho funkčního či designového prvku. Tato metoda byla původně vyvinuta &gt;pro účely testování direct mail, nicméně posléze byla přejata i pro účely testování např. bannerových &gt;reklam nebo cílových stránek. Zdrof: Wikipedia Následující obrázek ukazuje princip využití A/B testování na příkladu webové stránky. Máme dvě verze stránky, které se liší barvou a tvarem tlačítka “Learn more”. Příchozímu na webovou stránku náhodně nabídneme jednu z verzí stránky a měříme procento lidí, které klikne na dané tlačítko. Ať už jste si toho vědomi nebo ne, podobných A/B testů se účastníte pravidelně na stránkách jako Google nebo Facebook, které touto metodou testují nové prvky na jejich stránkách. Maxime Lorant, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons V našem případě se NGO snaží zjistit, jak nejlépe designovat tlačítko na darování peněz. Vedle současné verze tlačítka, které je hranaté vyvinuli verzi, kde má tlačítko oblé rohy. Po jeden den (od 4hod ráno dne 1 do 4hod ráno dne 2) náhodně příchozím na stránku nabízeli jednu z variant tlačítka na darování peněz. Nasbírali celkem stránku navštívilo za toto období 24519 návštěvníků. 12262 vidělo první variantu tlačítka (hranaté rohy), 12257 vidělo druhou variantu tlačítka (oblé rohy). 451 návštěvníků kliklo na tlačítko darovat v první verzi stránky a 479 kliklo na tlačítko darovat v druhé verzi stránky. # pocet uzivatelu ve skupinach n1 &lt;- 12262 n2 &lt;- 12257 # pocet kliknuti y1 &lt;- 451 y2 &lt;- 479 # relativni cetnost ve skupinach p1 &lt;- y1/n1 p2 &lt;- y2/n2 Očividně v našich datech kliklo více lidí na druhou verzi a to i pokud vezmem ev potaz počet lidi v každé skupině a vypočítáme relativní četnost. V první skupině kliklo na tlačítko 0.037, v druhé skupině kliklo na tlačítko 0.039. Zajímá nás, zda je tento rozdíl díky náhodě, nebo zda je pravděpodobné, že kdybyhom tento test opakovali, opět bychom uviděli vyšší podíl lidé v druhé skupině, který klik na tlačítko darovat. Rozdíl relativní četnosti bude mít rozložení \\(N(p_1-p_2, \\sqrt{s_1^2 + s_2^2})\\). Přičemž víme, že u relativní četnosti se směrodatná odchylka rovná \\(s = \\sqrt{p (1-p)}\\). Graf 6.5 ukazuje výběrové rozdělení rozdílu relativní četnosti. Protože náš výběr je hodné veliký, používáme normální rozdělení. # smerodatna odchylka s1 &lt;- sqrt(p1 * (1-p1)) s2 &lt;- sqrt(p2 * (1-p2)) # vyberove parametry rozdilu p_d &lt;- p1 - p2 # smerodatna chyba rozdilu se_d &lt;- sqrt(s1^2/n1 + s2^2/n2) x &lt;- seq(-0.009, 0.005, length.out = 1000) pdf_d &lt;- dnorm(x, mean = p_d, sd = se_d) h0 &lt;- 0 p_0 &lt;- pnorm(h0, mean = p_d, sd = se_d, lower.tail = FALSE) plot(x, pdf_d, type = &quot;l&quot;, lwd = 2, xlab = &quot;Rozdíl v relativní četnosti&quot;, ylab = &quot;PDF&quot;, col = &quot;grey&quot;, main = paste0(&quot;P(x &gt; 0)=&quot;, round(p_0, 2)) ) # pridame integral plochy pro P(X &gt; 0) lines(x[x&gt;h0], pdf_d[x&gt;h0], type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 1) legend(&quot;topright&quot;, legend = paste0( &quot;N(&quot;, round(p_d, 3), &quot;, &quot;, round(se_d, 3), &quot;)&quot; ), col = &quot;grey&quot;, lwd = 2, cex = 0.7 ) Graf 6.5: Výběrové rozdělení rozdílu relativní četnosti # vypocitame inteval spolehlivosti alpha &lt;- 0.05 q &lt;- qnorm(c(alpha/2, 1-alpha/2)) i_s &lt;- round(p_d + q * se_d, 3) Výběrový rozdíl v relativních četnost je -0.002. Interval spolehlivosti na hladině spolehlivosti 95% je [-0.007, 0.002]. Nemůžeme tedy vyloučit, že pokud bychom experiment opakovali, rozdíl by mohl být pozitivní, což by znamenalo, že \\(p_1\\) je větší než \\(p_2\\). Z hlediska statistické hypotézy nás zajímá zda \\(H_1: \\pi_1 &lt; \\pi_2\\), tedy zda \\(H_1: \\pi_1 - \\pi_2 &lt; 0\\) a tedy nulová hypotéza je \\(H_0: \\pi_1 - \\pi_2 \\ge 0\\). Graf 6.6 ukazuje výběrové rozdělení rozdílu relativních četností za předpokladu \\(H_0=0\\). Modrá oblast potom ukazuje oblast zamítnutí \\(H_0\\). x &lt;- seq(-0.007, 0.007, length.out = 1000) h0 &lt;- 0 pdf_d &lt;- dnorm(x, mean = h0, sd = se_d) p_0 &lt;- pnorm(p_d, mean = h0, sd = se_d, lower.tail = TRUE) plot(x, pdf_d, type = &quot;l&quot;, lwd = 2, xlab = &quot;Rozdíl v relativní četnosti&quot;, ylab = &quot;PDF&quot;, col = &quot;grey&quot;, main = paste0(&quot;P(x &lt; &quot;, round(p_d, 3), &quot;0)=&quot;, round(p_0, 2)) ) # pridame integral plochy pro P(X &gt; 0) lines(x[x&lt;p_d], pdf_d[x&lt;p_d], type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 1) legend(&quot;topright&quot;, legend = paste0( &quot;N(&quot;, h0, &quot;, &quot;, round(se_d, 3), &quot;)&quot; ), col = &quot;grey&quot;, lwd = 2, cex = 0.7 ) Graf 6.6: Výběrové rozdělení rozdílu relativní četnosti za předpokladu H0 Pravděpodobnost, že za předpokladu \\(H_0\\) bychom sledovali rozdíl -0.002 nebo více extrémní je 0.17. Pokud zvolíme chybu I. druhu \\(\\alpha=\\) 0.05, pak je tato pravděpodobnost větší a proto nemůžeme zamítnou \\(H_0: \\pi_1 - \\pi_2 \\ge 0\\). Ukážeme si ještě, že k výpočtu můžeme stejně tako použít bootstrap a to i pokud nemáme celá data (ale pouze parametry). Budeme dělat výběry z rozložení \\(B(\\) 0.037 \\(,\\) 1.2262^{4} \\()\\) a z \\(B(\\) 0.039 \\(,\\) 1.2257^{4} \\()\\). Vždy vypočítáme rodíl mezi výběrovou relativní četnosti \\(p_{1Boot}\\) a výběrovou relativní četnost \\(p_{2Boot}\\). Nakonec zobrazíme rozdělení těchto rozdílů. # počet simulaci s &lt;- 1e4 p_d_boostrap &lt;- rep(NA, s) for(i in 1:s) { vyber_1 &lt;- rbinom(n = 1, size = n1, prob = p1) vyber_2 &lt;- rbinom(n = 1, size = n2, prob = p2) p_d_boostrap[i] &lt;- vyber_1/n1 - vyber_2/n2 } hist(p_d_boostrap, col = &quot;#1f77b4&quot;, xlab = &quot;Rozdíl v relativní četnosti&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;) Graf 6.7: Boostrapované rozdělení rozdílu v relativní četnosti i_s &lt;- quantile(p_d_boostrap, probs = c(alpha/2, 1-alpha/2)) i_s &lt;- round(i_s, 3) 0.025 kvantil a 0.975 kvantil výběrového rozdílu dvou relativních četností jsou [-0.007, 0.003]. Opět tedy naše data nepodporují hypotézu, že rozdíl je menší než 0 a na základě dostupných dat nedokážeme určit, že by druhá varianta tlačítka s oblými rohy zvyšovala pravděpodobnost darování peněz. Více například zde.↩︎ Používali jsme ji už v 2.↩︎ V tomto případě je populační průěrný počet bodů teoretická hodnota, které bychom dosáhli, kdybychom verzi test dali nekonečně (nebo hodně velkému) počtu studentů.↩︎ V oběma směrech, protože počítáme oboustranný test.↩︎ "],["porovnání-více-relativních-četností.html", "Kapitola 7 Porovnání více relativních četností 7.1 Test dobré shody 7.2 Chi-kvadrát test nezávislosti", " Kapitola 7 Porovnání více relativních četností 7.1 Test dobré shody Tento test používáme, pokud chceme porovnat více výběrových relativních četností s teoretickými relativními četnostmi. Tedy, \\(H_0: F(x) = F_0(x)\\) a \\(H_1: F(x) \\ne F_0(x)\\). Testovací statistika \\(\\chi^2\\) se vypočítá jako \\(\\chi^2 = \\sum_{i=1}^k\\frac{(n_i - np_i)^2}{np_i}\\), kde \\(n_i\\) jsou pozorované četnosti pro danou kategorii a \\(np_i\\) je očekávaná četnost za předpokladu \\(H_0\\) a \\(k\\) je počet kategorií. Tato testovací statistika má \\(\\chi^2\\) rozložení s \\(k-1\\) stupni volnosti. Pojďme si ukázat tento test na příkladu hodu kostkou. Ze série vyrobených kostek byla náhodou vybrána jedna kostka a bylo provedeno 60 hodů. 9x padla hodnota 1, 12x hodnota 2, 7x hodnota 3, 5x hodnota 4, 14x hodnota 5 a 13x hodnota 6. # nase hody hody &lt;- c(9, 12, 7, 5, 14, 13) names(hody) &lt;- 1:6 n &lt;- sum(hody) Máme rozhodnout, zda jsou kostky z této série pravidelné. Pokud je kostka pravidelná, padne každé číslo se stejnou pravděpodobností \\(\\pi_i=\\frac{1}{6}\\). Výběrové relativní četnosti pro každé číslo jsou 0.15, 0.2, 0.12, 0.08, 0.23, 0.22. Graf 7.1 ukazuje výběrové rozdělení testovací statistiky \\(\\chi^2\\) za předpokladu \\(H_0 = \\frac{1}{6}\\) a tedy \\(H_1 \\ne \\frac{1}{6}\\). # p za predpokladu H0 p_0 &lt;- rep(1 / 6) # testovaci statistika chi2 &lt;- sum((hody - n * p_0)^2 / (n * p_0)) # stupne volnosti s_v &lt;- length(hody) - 1 alpha &lt;- 0.01 kriticka_mez &lt;- qchisq(1 - alpha, df = s_v) # zobrazime x &lt;- seq(0, 20, length.out = 1000) pdf &lt;- dchisq(x, df = s_v) p &lt;- 1 - pchisq(chi2, df = s_v) plot(x, pdf, xlab = &quot;Výběrové statistika&quot;, ylab = &quot;f(x)&quot;, main = paste0(&quot;p-hodnota: &quot;, round(p, 3), &quot; při alpha=&quot;, alpha), type = &quot;l&quot; ) abline(v = kriticka_mez, col = &quot;#1f77b4&quot;) # oblast zamitnuti H0 lines(x[x&gt;kriticka_mez], pdf[x&gt;kriticka_mez], type = &quot;h&quot;, col = &quot;#1f77b4&quot;) abline(v = chi2, lty = 2) legend(&quot;topright&quot;, c(paste0(&quot;X2~(&quot;, s_v, &quot;)&quot;), &quot;kritická mez&quot;, &quot;testovací statistika&quot;), col = c(&quot;black&quot;,&quot;#1f77b4&quot;, &quot;black&quot;), lty = c(1, 1, 2), cex = 0.7 ) Graf 7.1: Rozdělení výběrové statistiky X2 testu dobré shody za předpokladu H0 Pravděpodobnost, že bychom za předpokladu \\(H_0\\) sledovali naši výběrovou relativní četnost nebo extrémnější je 0.27, tedy větší, než námi zvolená \\(\\alpha=\\) 0.01. Nemáme tedy dostatek evidence, abychom mohli usoudit, že výběrové rozdíly nejsou způsobené náhodou a nezamítáme tedy \\(H_0\\). Stejně bychom toho mohli dosáhnout pomocí funkce chisq.test. chisq.test(x = hody, p = rep(p_0, 6)) ## ## Chi-squared test for given probabilities ## ## data: hody ## X-squared = 6.4, df = 5, p-value = 0.2692 7.2 Chi-kvadrát test nezávislosti Tento test použijeme, pokud máme dvě proměnné (například \\(x\\) a \\(y\\)), která nám tvoří kontingenční tabulku a zajímá nás, zda spolu tyto proměnné souvisí (jsou na sobě závislé). Nulová hypotéza je, že \\(x\\) a \\(y\\) na sobě nejsou závislé a alternativní, že \\(x\\) a \\(y\\) na sobě závislé jsou, tedy \\(H_0: x \\!\\perp\\!\\!\\!\\perp y\\) a \\(H_1: x \\not\\!\\perp\\!\\!\\!\\perp y\\). Testovací statistiku opět vypočítáme jako \\(\\chi^2 = \\sum_{i=1}^k\\frac{(O_i - E_i)^2}{E_i}\\), kde \\(O_i\\) značí pozorované četnosti v dané buňce kontingenční tabulky a \\(E_i\\) očekávané četnosti za předpokladu \\(H_0\\). Očekávané četnosti \\(O_i\\) pro danou buňku kontingenční tabulky můžeme vypočítat jako součet hodnot v řádku buňky \\(n_i\\) * součet hodnot v sloupci buňky \\(n_j\\) vydělené celkovým počtem pozorování ve všech buňkách \\(n\\), tedy \\(E_{ij} = \\frac{n_i*n_j}{n}\\). Tato testovací statistika nabývá \\(\\chi^2\\) rozložení s \\(k_x - 1 * k_y -1\\) stupni volnosti. Pojďme si tento test ukázat na následujícím příkladu. Dobrovonící jsou náhodně rozdělení do kontrolní (0) a experimentální skupiny (1). Experimentální skupině byla podána vakcína. Po nějaké době změříme u obou skupin počet lidí s danou nemocí (0 pokud nemoc jedinec neprodělal a 1 pokud prodělal). Zajímá nás, zda existuje souvislost mezi podáním vakcíny a výskytem nemoci v populaci. # nacteme data vaccine &lt;- read.csv(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/vaccine.csv&quot;) # pomoci xtabs muzeme vytvorit kontingencni tabulku t_o &lt;- xtabs(~ group + disease, data = vaccine) knitr::kable(t_o) 0 1 0 21683 86 1 21761 8 Naše \\(H_0\\) je, že neexistuje souvislost mezi podáním vakcíny a výskytem nemoci. \\(H_1\\) pak je, že existuje souvislost mezi podáním vakcíny a výskytem nemoci. Následující tabulka ukazuje očekávané četnosti za předpokladu nezávislosti obou proměnných. # nyni vypocitame ocekavane cetnosti za predpokladu H0 (tedy nezavislosti mezi promennymi) t_e &lt;- t_o # zkopirujeme si t_o, abychom nemuseli vytvařet objekt o stejnem poctu radku a sloupcu ## vypocitame ocekavane cetnosti podle postupu popsanem v textu nahore t_e[1, 1] &lt;- sum(t_o[1, ]) * sum(t_o[, 1]) / sum(t_o) t_e[1, 2] &lt;- sum(t_o[1, ]) * sum(t_o[, 2]) / sum(t_o) t_e[2, 1] &lt;- sum(t_o[2, ]) * sum(t_o[, 1]) / sum(t_o) t_e[2, 2] &lt;- sum(t_o[2, ]) * sum(t_o[, 2]) / sum(t_o) knitr::kable(t_e) 0 1 0 21722 47 1 21722 47 # vypocitame testovaci statistiku chi2 &lt;- sum((t_o - t_e)^2 / t_e) s_v &lt;- (nrow(t_e) - 1) * (ncol(t_e) - 1) alpha &lt;- 0.05 kriticka_mez &lt;- qchisq(1 - alpha, df = s_v) # vytvorime graf za predpokladu H0 x &lt;- seq(0, 65, length.out = 1000) pdf &lt;- dchisq(x, df = s_v) p &lt;- 1 - pchisq(chi2, df = s_v) plot(x, pdf, xlab = &quot;Výběrové statistika&quot;, ylab = &quot;f(x)&quot;, main = paste0(&quot;p-hodnota: &quot;, round(p, 3), &quot; při alpha=&quot;, alpha), type = &quot;l&quot; ) abline(v = kriticka_mez, col = &quot;#1f77b4&quot;) # oblast zamitnuti H0 lines(x[x&gt;kriticka_mez], pdf[x&gt;kriticka_mez], col = &quot;#1f77b4&quot;) abline(v = chi2, lty = 2) legend(&quot;topright&quot;, c(paste0(&quot;X2~(&quot;, s_v, &quot;)&quot;), &quot;kritická mez&quot;, &quot;testovací statistika&quot;), col = c(&quot;black&quot;,&quot;#1f77b4&quot;, &quot;black&quot;), lty = c(1, 1, 2), cex = 0.7 ) Graf 7.2: Rozdělení výběrové statistiky X2 testu nezávislosti za předpokladu H0 Graf 7.2 ukazuje výběrové rozdělení \\(\\chi^2\\) za předpokladu \\(H_0\\), tedy nezávislosti obou proměnných. Pravděpodobnost, že bychom za předpokladu \\(H_0\\) sledovali naše výběrové četnosti v buňkách tabulky (nebo extrémnější) je 0, tedy menší, než námi zvolená \\(\\alpha=\\) 0.05. Tato pravděpodobnost je dostatečně malá, abychom usoudili, že pozorované četnosti nejsou náhodné a mohli zamítnout \\(H_0\\). Stejně tak, bychom mohli použít funkci chisq.test. chi2_test &lt;- chisq.test(t_o, correct = FALSE) # ocekavane cetnosti muzeme extraktovat pomoci chi2_test$expected chi2_test ## ## Pearson&#39;s Chi-squared test ## ## data: t_o ## X-squared = 64.863, df = 1, p-value = 8.027e-16 "],["porovnání-více-průměrů.html", "Kapitola 8 Porovnání více průměrů 8.1 Problém více testů 8.2 ANOVA", " Kapitola 8 Porovnání více průměrů 8.1 Problém více testů Analýza rozptylu slouží k porovnání průměrů ve více skupinách. Nabýzelo by se použít několik dvouvýběrových t-testů, ale problém s takovýmto použitím t-testů je, že chyba I. druhu v těchto testech se sčítá (viz 5.6). Pokud máme \\(k\\) porovnání, pak pravděpodobnost, že uděláme chybu I. druhu je \\(FWER = 1-(1-\\alpha)^k\\). Počet porovnání \\(k\\) můžeme vypočítat z počtu skupin \\(m\\) jako \\(k = \\frac{m(m-1)}{2}\\). Pojďme si ukázat příklad se 4 skupinami a \\(\\alpha=0.05\\). m &lt;- 4 k &lt;- m*(m-1) / 2 alpha &lt;- 0.05 fwer = 1-(1-alpha)^k Pokud tedy máme 4 skupiny, je celkový počet testů 6 a pravděpodobnost chyby I. druhu 0.2649081. 8.2 ANOVA Řešením je tedy použít analýzu rozptylu (ANOVA). Nulová hypotéza (při 3 skupinách) tohoto testu je \\(H0: \\mu_1=\\mu_2=\\mu_3\\), alternativní hypotéza je \\(H1: \\mu_i \\ne \\mu_j (pro\\;nektere\\;skupiny\\;i\\;a\\;j)\\), tedy že alespoň jeden průměr je odlišný. Testovací statistiku \\(F\\) vypočítáme jako \\(F=\\frac{\\frac{\\sum_j^mn_j(\\bar{x_j}-\\bar{x})^2}{m-1}}{\\frac{\\sum_j^m\\sum_i^n(x_{ij} - \\bar{x_j})^2}{n-m}}\\). Na tento výpočet se můžeme podívat také jako na podíl rozptylu uvnitř skupin / rozptylu mezi skupinami. Zároveň platí, že \\(\\sum_j^m\\sum_i^n(x_{ij} - \\bar{x})^2=\\sum_j^m\\sum_i^n(x_{ij} - \\bar{x_j})^2 + \\sum_j^mn_j(\\bar{x_j}-\\bar{x})^2\\), tedy, že celkovou sumu čtverců můžeme rozložit na sumu čtverců uvnitř skupin a na sumu čtverců mezi skupinami. Testovací statistika \\(F\\) nabývá F rozdělení, které má 2 parametry \\(sv_1 = m-1\\) a \\(sv_2=n-m\\). Pojďme si tento výpočet ukázat na simulovaných datech. Nejdříve si vytvoříme data, která budou pocházet ze 3 skupin s průměry \\(\\mu_1=1, \\mu_2=1, \\mu_3=1.5\\). Budeme mít 40 pozorování v každé skupině. set.seed(42) mu &lt;- c(1,1,1.5) n &lt;- 120 #pocet pozorování m &lt;- 3 # pocet skupin # vyber z &lt;- rnorm(n = n, mean = mu) # nas vyber z populace k &lt;- rep(c(1,2,3), n/m) # vektor, ktery urcujez jake skupiny je pozorovani Pojďme se podívat na průměr skupin v našem výběru. aggregate(z~k, FUN = mean) ## k z ## 1 1 1.0474077 ## 2 2 0.9369615 ## 3 3 1.6045867 Na zobrazení rozložení v jednotlivých skupinách použijeme boxplot. Tento graf zobrazuje, co znamenají jednotlivé hranice boxplotu. Zdroj: Wikipedia boxplot(z~k, xlab = &quot;Skupina (nezávislá proměnná)&quot;, ylab = &quot;Závislá proměnná&quot;, main = &quot;&quot;) Graf 8.1: Rozložení závislé proměnné ve skupinách Pro ilustraci si překryjeme boxplot jednotlivými hodnotami. boxplot(z~k, xlab = &quot;Skupina (nezávislá proměnná)&quot;, ylab = &quot;Závislá proměnná&quot;, main = &quot;&quot;) points(jitter(k),z, pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.5)) Graf 8.2: Rozložení závislé proměnné ve skupinách s jednotlivými pozorováními V naší simulaci samozřejmě známe populační parametry, ale ve většině případů známe pouze výběrové hodnoty a podle nich se snažíme usoudit něco o populaci. Pojďme se tedy podívat, jaké hypotézy bychom pro tento test stanovili. \\(H_0\\) je, že všechny 3 průměry jsou v populaci stejné a \\(H_1\\), že alespoň jeden průměr se liší. Přistupme k výpočtu testovací staitistiky. sum_squares &lt;- function(x, prumer) { sum((x-prumer)^2) } ss_total &lt;- sum_squares(z, mean(z)) ss_uvnitr &lt;- 0 ss_mezi &lt;- 0 for(i in 1:m) { ss_uvnitr &lt;- sum_squares(z[k==i], mean(z[k==i])) + ss_uvnitr ss_mezi &lt;- sum(k==i) * sum_squares(mean(z[k==i]), mean(z)) + ss_mezi } s_v_1 &lt;- m - 1 s_v_2 &lt;- n - m f &lt;- (ss_mezi / s_v_1) / (ss_uvnitr / s_v_2) Celkový sum of squares je 137.5228637, sum of squares uvnitř skupin je 127.2779272 a sum of squares mezi skupinami je 10.2449365. Testovací statistika má hodnotu 4.71. Přistupme k interpretaci. Vypočítejme kritickou mez pro \\(\\alpha=0.05\\) a vypočítejme hodnotu \\(p\\). Jak vidíme, testovací statistika je v oblasti, kde zamítáme nulovou hypotézu. Tomu odpovídá také hodnota \\(p\\), která je menší než stanovená hladina \\(\\alpha\\). alpha &lt;- 0.05 kriticka_mez &lt;- qf(1-alpha, df1 = s_v_1, df2 = s_v_2) p &lt;- 1-pf(f, df1 = s_v_1, df2 = s_v_2) x &lt;- seq(0, 5, length.out=1000) pdf &lt;- df(x, df1=s_v_1, df2=s_v_2) plot(x, pdf, xlab = &quot;&quot;, ylab = &quot;f(x)&quot;, main = paste0(&quot;p-hodnota: &quot;, round(p, 3), &quot; při alpha=&quot;, alpha), type = &quot;l&quot;) abline(v = kriticka_mez, col = &quot;#1f77b4&quot;) # oblast zamitnuti H0 lines(x[x&gt;kriticka_mez], pdf[x&gt;kriticka_mez], type = &quot;h&quot;, col = &quot;#1f77b4&quot;) abline(v = f, lty = 2) legend(&quot;topleft&quot;, c(&quot;kritická mez&quot;, &quot;testovací statistika&quot;), col = c(&quot;#1f77b4&quot;, &quot;black&quot;), lty = c(1,2)) Graf 8.3: Rozdělení podílů rozptylu za předpokladu H0 V R bychom pro tento výpočet rozkladu rozptylu použít funkci aov. m_anova &lt;- aov(z~as.factor(k)) # pozor nezavisla promenna musi byt factor s_anova &lt;- summary(m_anova) # takto získáme nejzakladnejsi vysledky print(s_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(k) 2 10.24 5.122 4.709 0.0108 * ## Residuals 117 127.28 1.088 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 K výpočtu statistického testu pak použijeme funkci anova. anova_vystup &lt;- anova(m_anova) s_v_1 &lt;- anova_vystup$Df[1] s_v_2 &lt;- anova_vystup$Df[2] f &lt;- anova_vystup$`F value`[1] p &lt;- anova_vystup$`Pr(&gt;F)`[1] "],["korelace.html", "Kapitola 9 Korelace 9.1 Kovariance 9.2 Korelační koeficient 9.3 Statistické testování a interval spolehlivosti", " Kapitola 9 Korelace Korelace se používá na měření vztahu dvou proměnných. My si přiblížíme výpočet a princip pearsonova korelačního koeficientu, který se používá na měření lineární závislosti dvou spojitých proměnných. 9.1 Kovariance Kovariance je definována jako \\(cov(X, Y) = E[(X-E[X])(Y-E[Y])]\\). Pokud máme kardinální proměnnou pocházející z výběru o velikosti \\(n\\), pak lze kovarianci vypočítat jako \\(cov(X, Y) = \\frac{1}{n-1}\\sum_i^n(x_i - \\bar{x})(y_i-\\bar{y})\\). Kovariance tak představuje výpočet společné variability obou proměnných. Můžeme si ji také představit jako výpočet toho, jak moc se velké hodnoty jedné proměnné shodují s velkými hodnotami druhé proměnné. Jako “velkou hodnotu” rozumíme její kladnou odchylku od průměru a jako “malou hodnotu” rozumíme její negativní odchylku od průměru. Protože jako “velkou/malou hodnotu” měříme rozdíl od průměru měří kovariance míru lineární závislosti mezi dvěma proměnnými. set.seed(10) n &lt;- 10 x &lt;- rnorm(n) y &lt;- rnorm(n, mean = x) plot(x, y, main = &quot;&quot;, pch = 19, col = &quot;#1f77b4&quot;) abline(v = mean(x), col = &quot;black&quot;) abline(h = mean(y), col = &quot;black&quot;) Graf 9.1: X~N(0,1), Y~N(X,1) e_x &lt;- x - mean(x) e_y &lt;- y - mean(y) cov_xy &lt;- sum(e_x * e_y) / n # nebo take mean(e_x*e_y) plot(e_x * e_y, type = &quot;h&quot;, main = paste0(&quot;(X-E[X])(Y-E[Y])=&quot;, round(cov_xy, 2)), lwd = 15, col = &quot;#1f77b4&quot;) Graf 9.2: Příklad výpočtu kovariance Zkuste odpovědět na následující otázky: za jakých podmínek bude číslo pozitivní? za jakých podmínek bude číslo nezitivní? za jakých podmínek bude číslo zhruba 0? V R můžeme vypočítat \\(cov(X,Y)\\) pomocí funkce cov. Podobně jako u směrodatné odchylky nebo rozptylu počítá R s populační kovariencí, tedy dělíme \\(n-1\\) místo \\(n\\). Zkuste si nyní sami nasimulovat situace, v kterých bude kovariance: negativní, zhruba rovná 0 # negativni n &lt;- 10 x &lt;- rnorm(n) y &lt;- rnorm(n, mean = -1 * x) plot(x, y, main = &quot;&quot;, pch = 19, col = &quot;#1f77b4&quot;) abline(v = mean(x), col = &quot;black&quot;) abline(h = mean(y), col = &quot;black&quot;) Graf 9.3: X~N(0,1), Y~N(-X,1) e_x &lt;- x - mean(x) e_y &lt;- y - mean(y) cov_xy &lt;- sum(e_x * e_y) / n # nebo take mean(e_x*e_y) plot(e_x * e_y, type = &quot;h&quot;, main = paste0(&quot;(X-E[X])(Y-E[Y])=&quot;, round(cov_xy, 2)), lwd = 15, col = &quot;#1f77b4&quot;) Graf 9.4: Příklad negativní kovariance # zhruba 0 x &lt;- rnorm(n) y &lt;- rnorm(n, mean = 2) plot(x, y, main = &quot;&quot;, pch = 19, col = &quot;#1f77b4&quot;) abline(v = mean(x), col = &quot;black&quot;) abline(h = mean(y), col = &quot;black&quot;) Graf 9.5: X~N(0,1), Y~N(2,1) e_x &lt;- x - mean(x) e_y &lt;- y - mean(y) cov_xy &lt;- sum(e_x * e_y) / n # nebo take mean(e_x*e_y) plot(e_x * e_y, type = &quot;h&quot;, main = paste0(&quot;(X-E[X])(Y-E[Y])=&quot;, round(cov_xy, 2)), lwd = 15, col = &quot;#1f77b4&quot;) Graf 9.6: Příklad kovariance blížící se nule 9.2 Korelační koeficient Hodnota kovariance není teoreticky ohraničena a tedy \\(cov(X,Y) = (-\\infty; \\infty)\\). Toto není moc praktické, pokud chceme kovariance porovnávat mezi sebou. Proto při výpočtu pearsonova korelačního koeficientu kovarianci standardizujeme tak, že ji vydělíme produktem směrodatné odchylky obou proměnných, tedy \\(\\rho_{xy} = \\frac{\\frac{\\sum_i^n(x_i - \\bar{x})(y_i-\\bar{y})}{n}}{\\sigma_x\\sigma_y}\\), nebo po pokrácení \\(n\\) také, \\(\\rho_{xy} = \\frac{\\sum_i^n(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_i^n(x_i - \\bar{x})^2}\\sqrt{\\sum_i^n(y_i - \\bar{y})^2}}\\). Tento korelační koeficient \\(\\rho\\) je mezi -1 a 1. Přičemž -1 znamena perfektně lineárně negativní vztah a 1 znamena perfektně lineárně pozitivní vztah. Hodnota 0 znamena, že mezi proměnnými není žádný lineární vztah. Vypočítejme si korelační koeficient pro náš první příklad. n &lt;- 10 x &lt;- rnorm(n) y &lt;- rnorm(n, mean = x) e_x &lt;- x - mean(x) e_y &lt;- y - mean(y) r &lt;- sum(e_x * e_y) / (sqrt(sum(e_x^2)) * sqrt(sum(e_y^2))) print(r) ## [1] 0.4976903 Můžeme také použít funkci cor. Jak jsme si ukázali u kovariance, tento výpočet měří míru lineární závislosti. Pokud tedy funkce (vztah), mezi proměnnými není lineární (ale očividně tam nějaký vztah je), pearsonův korelační koeficient není nejlepším způsobem jak vyjádřit vztah mezi proměnnými. n &lt;- 1000 x &lt;- rnorm(n) y &lt;- ifelse(x &gt; 0, rnorm(n, x, sd = 0.5), rnorm(n, -1 * x, sd = 0.5)) r &lt;- cor(x, y) plot(x, y, main = paste0(&quot;r = &quot;, round(r, 2)), pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.3) ) Graf 9.7: Příklad nelineárního vztahu Pearsonův korelační koeficient je také velmi náchylný na odlehlá pozorování. To jsou taková pozorování, která nabývají výrazně jiných hodnot, než většina pozorování dané proměnné. # vytvorime dve nezavisle promenne n &lt;- 100 x &lt;- rnorm(n, mean = 100, sd = 10) y &lt;- rnorm(n, mean = 10, sd = 5) r &lt;- cor(x, y) par(mfrow = c(1,2)) plot(x, y, main = paste0(&quot;r = &quot;, round(r, 2)), pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.3) ) # pridame jedno extremni pozorovani x &lt;- c(x, 200) y &lt;- c(y, 50) r &lt;- cor(x, y) plot(x, y, main = paste0(&quot;r = &quot;, round(r, 2)), pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.3) ) points(x[101], y[101], col = &quot;red&quot;, pch = 19) Graf 9.8: Příklad vlivu odlehlého pozorování na r pro dvě nezávislé proměnné Stejně tak může málo odlehlých hodnot “vymazat” existijící lineární vztah # vytvorime dve zavisle promenne n &lt;- 100 x &lt;- rnorm(n, mean = 100, sd = 10) y &lt;- rnorm(n, x, sd = 10) r &lt;- cor(x, y) par(mfrow = c(1,2)) plot(x, y, main = paste0(&quot;r = &quot;, round(r, 2)), pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.3) ) # pridame jedno extremni pozorovani x &lt;- c(x, 200) y &lt;- c(y, 50) r &lt;- cor(x, y) plot(x, y, main = paste0(&quot;r = &quot;, round(r, 2)), pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.3) ) points(x[101], y[101], col = &quot;red&quot;, pch = 19) Graf 9.9: Příklad vlivu odlehlého pozorování na r pro dvě závislé proměnné Proto je vždy nutné si vztah obou proměnných zobrazit v grafu! 9.3 Statistické testování a interval spolehlivosti Nakonec si ukážeme jak můžeme zobecnit výběrový pearsonův korelační koeficient na celou populaci. Výběrový pearsonův korelační koeficient se označuje \\(r\\). Testovací statistika \\(t\\) se vypočítá jako \\(t = r\\sqrt{\\frac{n-2}{1-r^2}}\\) nabývá t-rozdělení s \\(n-2\\) stupni volnosti (při velkém výběru, \\(n&gt;30\\)). \\(H_0: \\rho=0\\) a \\(H1: \\rho\\ne0\\). Stejně tak ale můžeme mít pravostrannou a levostrannou \\(H_1\\). Pokud chcete testovat jinou nulovou hypotézu než 0, je potřeba k této inferenci použít interval spolehlivosti. Pojďme si tento výpočet ukázat na příkladu dat jedinců, u kterých známe výšku a váhu. Zajímá nás, zda mezi těmito dvěma proměnnými je pozitivní vztah. d &lt;- read.csv(&quot;https://raw.githubusercontent.com/schubertjan/cuni/master/stats/data/vyska_vaha.csv&quot;) head(d) ## vyska vaha pohlavi ## 1 174.0 65.6 0 ## 2 NA NA 0 ## 3 193.5 80.7 0 ## 4 186.5 72.6 0 ## 5 NA 78.8 0 ## 6 181.5 74.8 0 # nejprve se musime rozhodnout, co udelat s chybejicicmi hodnotami # vyradime vsechny, kde je vaha nebo vyska NA. To se rovna cor(d$vyska, d$vaha, use=&quot;complete.obs&quot;) d &lt;- d[!is.na(d$vyska) &amp; !is.na(d$vaha), ] # kontrola sum(is.na(d)) ## [1] 0 # vypocet r r &lt;- cor(d$vyska, d$vaha) n &lt;- nrow(d) # zobrazime vztah plot(d$vyska, d$vaha, xlab = &quot;Výška&quot;, ylab = &quot;Váha&quot;, main = paste0(&quot;r = &quot;, round(r, 2)), pch = 19, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.3) ) Graf 9.10: Vztah mezi výškou a váhou # H0: r &lt;= 0 # H1: r &gt; 0 t &lt;- r * sqrt((n - 2) / (1 - r^2)) s_v &lt;- n - 2 alpha &lt;- 0.01 kriticka_mez &lt;- qt(1 - alpha, df = s_v) p &lt;- 1 - pt(t, df = s_v) x &lt;- seq(-4, 24, length.out = 1000) pdf &lt;- dt(x, df = s_v) plot(x, pdf, xlab = &quot;Testovací statistika&quot;, ylab = &quot;f(x)&quot;, main = &quot;&quot;, sub = paste0(&quot;p-hodnota: &quot;, p, &quot; při alpha=&quot;, alpha), type = &quot;l&quot; ) abline(v = kriticka_mez, col = &quot;#1f77b4&quot;) # oblast zamitnuti H0 lines(x[x&gt;kriticka_mez], pdf[x&gt;kriticka_mez], type = &quot;h&quot;, col = &quot;#1f77b4&quot;) abline(v = t, lty = 2) legend(&quot;topleft&quot;, c(&quot;kritická mez&quot;, &quot;testovací statistika&quot;), col = c(&quot;#1f77b4&quot;, &quot;black&quot;), lty = c(1, 2) ) Graf 9.11: Rozložení testovací statistiky za předpokladu H0 # zamitame H0 ve prospech H1 Můžeme také použít funkci cor.test. cor.test(d$vyska, d$vaha, alternative = &quot;greater&quot;, conf.level = 1 - alpha) ## ## Pearson&#39;s product-moment correlation ## ## data: d$vyska and d$vaha ## t = 23.088, df = 503, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is greater than 0 ## 99 percent confidence interval: ## 0.6630375 1.0000000 ## sample estimates: ## cor ## 0.717292 Interval spolehlivosti pro \\(r\\) je poměrně těžké vypočítat analyticky a můžeme využít funkce cor.test, která ho vypočítá. Pokud chceme této funkce využít je z interpretačních důvodů lepší použít jako \\(H_1\\) oboustranný test. cor.test(d$vyska, d$vaha, alternative = &quot;two.sided&quot;, conf.level = 1 - alpha)$conf.int ## [1] 0.6567516 0.7686457 ## attr(,&quot;conf.level&quot;) ## [1] 0.99 Pokud bychom chtěli přeci jenom vypočítat interval spolehlivosti bez pomoci funkce cor.test nabízí se použití metody bootstrap. Tuto metodu je možné použít k výpočtu intervalu spolehlivosti (nebo standardních chyb) jekékoliv výběrové statistiky. Zakládá se na principu toho, že opakujeme velké množství výběrů s opakováním z našeho výběru o velikosti \\(n\\). Obecně platí, že pokud můžeme vypočítat interval spolehlivosti analyticky, je to preferované. # pocet opakovani k &lt;- 1000 r_bootstrap &lt;- rep(NA, k) for (i in 1:k) { # nahodny vyber s opakovanim ind &lt;- sample(1:n, size = n, replace = TRUE) # jaka pozorovani vybereme r_bootstrap[i] &lt;- cor(d$vyska[ind], d$vaha[ind]) # spocitame korelacni koeficient } # zobrazime hist(r_bootstrap, main = &quot;&quot;, col = &quot;#1f77b4&quot;) Graf 9.12: Bootsrapované rozložení r # pokud chceme zjistit 99% interval spolehlivosti quantile(r_bootstrap, probs = c(alpha / 2, 1 - alpha / 2)) ## 0.5% 99.5% ## 0.6567594 0.7673308 "],["neparametrické-testy.html", "Kapitola 10 Neparametrické testy 10.1 Znaménkový test 10.2 Mann-Whitney U test", " Kapitola 10 Neparametrické testy Neparametrické testy používáme, pokud jsou předopklady některých parametrických testů porušeny, nejčastěji normalita rozdělení nebo předpoklady o spojitosti proměnné. 10.1 Znaménkový test Nejjednodušším testem je znaménkový test. Tento test používáme, když chceme porovnat medián proměnné k populační hodnotě. Jde tedy o ekvivalement jednovýběrového t-testu (viz 5. Pojďme si znaménkový test ukázat na následujícím příkladu. Na filmové databázi IMDb mohou registrovaní uživatelé a uživatelky hodnotit filmy na škále od 1 do 10. Víte, že daný film na této databázi hodnotilo více než půl milionu uživatelů. Správce databáze vám při pracovním pohovoru na pozici datového analytika náhodně vygeneruje 14 hodnocení tohoto filmu: \\(x = [3, 7, 4, 10, 1, 6, 9, 8, 3, 6, 7, 5, 9, 8]\\). Vedoucí analytického týmu databáze IMDb vám při pracovním pohovoru stanoví následující úlohu: Jaká je pravděpodobnost, že se jedná o zcela výjimečný film, který dostal od poloviny hodnotících uživatelů databáze známky 9 nebo 10 (tedy medián všech hodnocení je 9 nebo větší)? Graf 10.1 ukazuje hodnocení od nejmenší po největší hodnotu. Horizontální přímka vyjadřuje teoretickou hodnotu 9. X &lt;- c(3, 7, 4, 10, 1, 6, 9, 8, 3, 6, 7, 5, 9, 8) k &lt;-length(X) plot(sort(X), col = &quot;#1f77b4&quot;, xlab = &quot;Pořadí&quot;, ylab = &quot;Hodnocení&quot;, type = &quot;h&quot;, lwd = 15) abline(h = 9) Graf 10.1: Sloupcový graf 14 hodnocení filmů V tomto případě tedy testujeme, zda \\(H_0: \\eta \\le 8\\), \\(H_1: \\eta &gt; 8\\) (tedy 9 a více). Začněme tím, že se podíváme, kolik hodnot je vyšších, než testovaná hodnota \\(8\\). Znaménko \"-\" značí hodnotu menší než 8 a znaménko \"+\" hodnotu větší než 8. h0 &lt;- 8 znamenka &lt;- X&gt;h0 znamenka_table &lt;- ifelse(znamenka, &quot;+&quot;, &quot;-&quot;) names(znamenka_table) &lt;- X znamenka_table ## 3 7 4 10 1 6 9 8 3 6 7 5 9 8 ## &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;+&quot; &quot;-&quot; &quot;-&quot; &quot;+&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;+&quot; &quot;-&quot; Pokud by populační hodnota byla rovna \\(\\eta=8\\), z 14 hodnot bychom očekávali 7 vyšších než \\(\\eta=8\\) a 7 menších než \\(\\eta=8\\). Jedná se tedy vlastně o příklad binomického rozložení (3.2) s \\(k=14\\) pokusy a pravděpodobností úspěchu \\(p=0.5\\), tedy \\(B(14, 0.5)\\). V našem případě je celkem 3 vyšších než testovaná hodnota 8. x &lt;- 0:14 p &lt;- 0.5 plusova_znamenka &lt;- sum(znamenka) pmf &lt;- dbinom(x, size = k, prob = p) p_hodnota &lt;- sum(pmf[x&gt;=plusova_znamenka]) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 15, xlim = c(0, k), main = paste0(&quot;p=&quot;, round(p_hodnota, 2)) ) lines(x[x&gt;=plusova_znamenka], pmf[x&gt;=plusova_znamenka], col = &quot;#1f77b4&quot;, type = &quot;h&quot;, lwd = 15) axis(1, 0:k, 0:k) Graf 10.2: Distribuce pozorování větších než teoretická hodnota za předpokladu H0. příklad jednosměrného testu. Graf 10.2 ukazuje rozdělení počtu hodnot větších než teoretická hodnota 8, pokud by v populaci platila \\(H_0\\). V našem případě jsme měli 3 hodnoty větší než teoretická hodnota 8. Pokud platí \\(H_0\\), pak je pravděpodobnost, že bychom z 14 pozorování viděli 3 nebo více větších hodnot než \\(H_0\\) je 0.99. Tedy nemůžeme zamítnout \\(H_0: \\eta \\le 8\\) ve prospěch \\(H_1\\). Nelze tedy říci, že by byl medián hodnocení větší nebo roven 9. Dále na stejných datech formulujeme hypotézu, že medián všech hodnocení tohoto filmu se nerovná 5. Formulujte odpovídající nulovou a alternativní hypotézu. Tedy \\(H_0: \\eta = 5\\), \\(H_1: \\eta \\ne 5\\). Opět spočítáme počet případů, kdy je hodnocení vyšší než teoretická hodnota. h0 &lt;- 5 znamenka &lt;- X&gt;h0 znamenka_table &lt;- ifelse(znamenka, &quot;+&quot;, &quot;-&quot;) names(znamenka_table) &lt;- X znamenka_table ## 3 7 4 10 1 6 9 8 3 6 7 5 9 8 ## &quot;-&quot; &quot;+&quot; &quot;-&quot; &quot;+&quot; &quot;-&quot; &quot;+&quot; &quot;+&quot; &quot;+&quot; &quot;-&quot; &quot;+&quot; &quot;+&quot; &quot;-&quot; &quot;+&quot; &quot;+&quot; Pokud by populační hodnota byla rovna 5, pak bychom opět v našem výběru čekali 7 pozorování menších, než teoretická hodnota a 7 větších, než teoretická hodnota. Opět tedy modelujele \\(B(14,0.5)\\). Graf 10.3 ukazuje toto binomické rozložení. Modře je opět vyznačena oblast zamítnutí \\(H_0\\). Protože používáme oboustranný test, pak je oblast zamítnutí symetricky na obou strannách od očekávané hodnoty. x &lt;- 0:14 p &lt;- 0.5 t &lt;- sum(znamenka) pmf &lt;- dbinom(x, size = k, prob = p) # *2, protože máme oboustranný test p_hodnota &lt;- sum(pmf[x&gt;=t])*2 alpha &lt;- 0.05 kriticka_hodnota &lt;- qbinom(c(alpha/2, 1-alpha/2), size = k, prob = p) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 15, xlim = c(0, k), main = paste0(&quot;p=&quot;, round(p_hodnota, 2)) ) lines(x[x&lt;kriticka_hodnota[1]], pmf[x&lt;kriticka_hodnota[1]], col = &quot;#1f77b4&quot;, type = &quot;h&quot;, lwd = 15) lines(x[x&gt;kriticka_hodnota[2]], pmf[x&gt;kriticka_hodnota[2]], col = &quot;#1f77b4&quot;, type = &quot;h&quot;, lwd = 15) abline(v = t, lty = 2) axis(1, 0:k, 0:k) legend(&quot;topright&quot;, legend = c( &quot;B(14,0.5)&quot;, &quot;Oblast zamítnutí&quot;, &quot;Testovací statistika&quot;), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;, &quot;black&quot;), lwd = c(5, 5, 1), lty = c(1,1, 2), cex = 0.7 ) Graf 10.3: Distribuce pozorování větších než teoretická hodnota za předpokladu H0. Příklad obousměrného testu. Jak je vidět, pravděpodobnost, že bychom za předpokladu \\(H_0: \\eta=5\\) sledovali více než 3 hodnot větších než teoretická hodnota nebo 11 menších než teoretická hodnota je 0.42. Nemůžeme tedy zamítnout \\(H_0\\) ve prospěch \\(H_1\\). V R je možné tento test provést pomocí funkce binom.test. # prvni priklad (H0: eta&lt;=8, H1: eta &gt; 8) # binom.test(3, 14, alternative = &quot;greater&quot;) # druhy priklad (H0: eta=5, H1: eta != 5) # binom.test(9, 14, alternative = &quot;two.sided&quot;) 10.2 Mann-Whitney U test Tento typ testu se používá, pokud chceme porovnat medián dvou nezávislých výběrů. Narozdíl od t-testu nemusí být proměnné normálně rozděleny. Pětadvacet studentů angličtiny bylo na začátku školního roku náhodně rozděleno do dvou seminárních skupin. V první skupině probíhala výuka standardním způsobem a ve druhé skupině byla v průběhu roku aplikována nová metoda výuky angličtiny. Na konci školního roku absolvovali obě skupiny studentů jazykovou zkoušku TOEFL (skládající se čtyř částí: reading, listening, speaking a writing). Souhrnné výsledky obou skupin shrnuje následující tabulka: Standardní metoda výuky 96 65 67 72 63 55 56 64 84 80 52 51 Nová metoda výuky 76 80 88 70 83 94 87 90 73 89 79 69 97 Zajímá nás, zda jsou mezi oběma metodami rozdíly u standardizované jazykové zkoušky TOEFL? Tedy \\(H_0: \\eta_1 = \\eta_2\\) a \\(H_1: \\eta_1 \\ne \\eta_2\\). standard &lt;- c(96, 65, 67, 72, 63, 55, 56, 64, 84, 80, 52, 51) experiment &lt;- c(76, 80, 88, 70, 83, 94, 87, 90, 73, 89, 79, 69, 97) eta_1 &lt;- median(standard) eta_2 &lt;- median(experiment) Medián v prvním výběru standardní výuky je 64.5. Medián v druhém výběru experimentální výuky je 83. Jako první vypočítáme pořadí všech hodnot, od nejmenšího po největší hodnotu a sečteme pořadí hodnot pro každou skupinu do \\(R_1\\) a \\(R_2\\). n1 &lt;- length(standard) n2 &lt;- length(experiment) r &lt;- rank(c(standard, experiment)) R1 &lt;- sum(r[1:n1]) R2 &lt;- sum(r[(n1+1):(n1+n2)]) Součet pořadí hodnot ve standardní skupině je 104.5. Součet pořadí hodnot v experimentální skupině je 220.5. Nyní vypočítáme testovací statistiku \\(U\\) pro oba výběry. \\(U_1 = R_1-\\frac{n_1(n_1+1)}{2}\\) a \\(U_2 = R_2-\\frac{n_2(n_2+1)}{2}\\). U1 &lt;- R1 - (n1*(n1+1)/2) U2 &lt;- R2 - (n2*(n2+1)/2) U &lt;- min(U1, U2) mu_U &lt;- n1*n2/2 sigma_U &lt;- sqrt((n1*n2*(n1+n2+1))/12) z &lt;- (U-mu_U) / sigma_U Testovací statistika pro výběr standardní výuky je 26.5 a pro výběr experimentální výuky je 129.5. Jako testovací statistiku \\(U\\) vybereme menší z obou hodnot, tedy \\(U=min(U_1, U_2)\\). V našem případě je tedy \\(U\\) rovno 26.5. Protože testovací statistika nabývá normálního rozdělení, abyhcom ji mohli porovnat k očekávanému rozdělní za platnosti \\(H_0\\), musíme vypočítat průměr a směrodatbou odchylku rozdělení této testovací statistiky. Průměr výběrového rozdělení testovací statistiku \\(U\\) vypočítáme jako \\(\\mu_U=\\frac{n_1n_2}{2}\\). V našem případě je tato hodnota rovna 78. Směrodatná chyba odhadu \\(U\\) se vypočítá jako \\(\\sigma_U = \\sqrt{\\frac{n_1n_2(n_1+n_2+1)}{12}}\\). Hodnotu kvantilu testovací statistiky vypočítáme jako \\(z=\\frac{U-\\mu_U}{\\sigma_U}\\). x &lt;- seq(10,150, length.out=1000) pdf &lt;- dnorm(x, mu_U, sigma_U) alpha &lt;- 0.05 kriticka_mez &lt;- qnorm(c(alpha/2, 1-alpha/2), mean = mu_U, sigma_U) p_hodnota &lt;- pnorm(z) *2 # p_hodnota &lt;- pnorm(U, mu_U, sigma_U) *2 # nebo take plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;U&quot;, ylab = &quot;PDF&quot;) abline(v = U, lty = 2) lines(x[x&lt;kriticka_mez[1]], pdf[x&lt;kriticka_mez[1]], type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 1) lines(x[x&gt;kriticka_mez[2]], pdf[x&gt;kriticka_mez[2]], type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 1) legend(&quot;topright&quot;, legend = c( paste0( &quot;N(&quot;, mu_U, &quot;, &quot;, round(sigma_U, 3), &quot;)&quot; ), &quot;Testovací statistika&quot;), col = c(&quot;grey&quot;, &quot;black&quot;), lwd = c(2, 1), lty = c(1,2), cex = 0.7 ) Graf 10.4: Výběrové rozdělení U za předpokladu H0 Pravděpodobnost, že bychom za předpokladu \\(H_0: \\eta_1 = \\eta_2\\) sledovali testovací statistiku takovou nebo extrémnější je 0.005, tedy pod hranicí zvolené \\(\\alpha=0.05\\). Zamítáme tedy \\(H_0\\) ve prospěch \\(H_1\\). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
