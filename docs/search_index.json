[["index.html", "Úvod do statistiky Kapitola 1 Úvod 1.1 Příklad popisné statistiky 1.2 Příklad inferenční statistiky 1.3 Simulace 1.4 O modelech", " Úvod do statistiky Jan Schubert 2022-03-01 Kapitola 1 Úvod Tato kniha navazuje na kurz Základy logiky a matematiky (JSB536), ale jeho absolvování není nutné k pochopení látky. Statistika je nástroj, který aplikuje matematiku na získání užitečných informací z dat. Roli statistiky je možné rozdělit na dva úkoly: Popisování většího množství dat (popisná statistika) Předpovídání nějakého fenomenu/určení míry nejistoty (inferenční statistika) 1.1 Příklad popisné statistiky Mnoho fenomenů v každodenním světě je možné vyjádřit pomocí dat. Fotka se dá vyjádřit jako 3D matice (red, green, blue) řádků a sloupců, která vyjadřuje jednotlivé pixely. Lidl Stiftung &amp; Co. KG, Public domain, via Wikimedia Commons obrazek &lt;- png::readPNG(&quot;../imgs/img1.1.svg.png&quot;) dim(obrazek) ## [1] 480 480 3 Můžeme například zobrazit hodnoty červené barvy prvních 5x5 pixelů. obrazek[1:5, 1:5, 1] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1.0000000 ## [2,] 1 1 1 1 1.0000000 ## [3,] 1 1 1 1 1.0000000 ## [4,] 1 1 1 1 1.0000000 ## [5,] 1 1 1 1 0.3215686 Můžeme zobrazit pouze nějaké řádky a sloupce: plot(0:1, 0:1, type = &quot;n&quot;, ann = FALSE, axes = FALSE) rasterImage(obrazek[125:325, 100:300, ], 0, 0, 1, 1) Můžeme vypočítat průměrnou nebo medianovou barvu. # vypocitame prumer pro kazdou barvu (cervena, zelena, modra) prumerna_barva &lt;- sapply(1:3, function(i) mean(obrazek[, , i])) # vypocitame barvu na rgb skale prumerna_barva &lt;- rgb(prumerna_barva[1], prumerna_barva[2], prumerna_barva[3]) # vypocitame median pro kazdou barvu (cervena, zelena, modra) median_barva &lt;- sapply(1:3, function(i) median(obrazek[, , i])) # vypocitame barvu na rgb skale median_barva &lt;- rgb(median_barva[1], median_barva[2], median_barva[3]) # zobrazime v grafu plot(1,1, type = &quot;n&quot;, axes = FALSE, ann = FALSE, xlim = c(0, 2), ylim = c(0, 1)) rect(0, 0, 1, 0.9, col = prumerna_barva) text(0.45, 1, labels = paste0(&quot;Průměrná barva: &quot;, prumerna_barva)) rect(1, 0, 2, 0.9, col = median_barva) text(1.45, 1,labels = paste0(&quot;Mediánová barva: &quot;, median_barva)) Protože obrázek je matice dat, můžeme na ni uplatnit různé statistické metody. Zajímá nás například, jaké odstiny barev jsou použité v tomto logu. Vidíme, že logo se skládá ze tří barev a můžeme extrahovat 3 barevy pomocí shlukovacího algoritmu. set.seed(42) k &lt;- 3 cervena &lt;- as.vector(obrazek[, , 1]) zelena &lt;- as.vector(obrazek[, , 2]) modra &lt;- as.vector(obrazek[, , 3]) m &lt;- kmeans(cbind(cervena, zelena, modra), centers = k) barvy &lt;- m$centers barvy_rgb &lt;- rep(NA, k) plot(seq(1, k * 10 + 10, length.out = 10), seq(1, 10, length.out = 10), axes = FALSE, ann = FALSE, type = &quot;n&quot; ) for (i in 1:k) { barvy_rgb[i] &lt;- rgb(barvy[i, 1], barvy[i, 2], barvy[i, 3]) rect(i * 10, 1, i * 10 + 10, 8, col = barvy_rgb[i]) text(i * 10 + 5, 9, labels = barvy_rgb[i]) } 1.2 Příklad inferenční statistiky Většina jevů okolo nás je ovlivněna náhodou, ať už z důvodu náhodného výběru nebo protože jsou součástí nějakého komplikovaného systému, který ovliňuje hodnoty jevu, který nás zajímá. Počet aut, které projedou na mostě, délka toaletního paríru, který je vyrobený v továrně, to jsou některé příklady jevů, které jsou ovlivněny náhodou. Statistika nám poskytuje soubor nástrojů, jak tuto náhodu (nejistotu) kvantifikovat. Pohlaví dětí je určeno náhodou a nově narozené dítě má zhruba stejnou pravděpodobnost, že bude děvče nebo chlapec. Řekněme, že z důvodu kapacitního plánování nás zajímá, kolik chlapců se narodí, pokud se v porodnoci denně narodí 25 dětí (sám počet narozených dětí by se dal modelovat jako náhodná proměnná). n &lt;- 25 p &lt;- 0.5 x &lt;- c(0:25) pmf &lt;- dbinom(x = x, size = n, p) plot(x, pmf, type = &quot;h&quot;, xlab = &quot;Počet chlapců z 25 narozených dětí&quot;, ylab = &quot;Pravděpodobnost&quot;, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 1.1: Pravděpodobnostní rozložení počtu chlapců z 25 narozených dětí Vidíme, že nejpravděpodobněji se narodí 12 chlapců (v 31% případů). Můžeme také z grafu vypočítat, že více než 15 chlapců se narodí zhruba v 11% případů, tedy zhruba 42 dní v roce. 1.3 Simulace K pochopení statistiky budeme používat programovací jazyk. Ten nám umožní, abychom si statistické koncepty osahali detailně. Budeme simulovat data, u kterých budeme vědět pravé hodnoty a sledovat, jak (ne)úspěšně různé statistické postupy pravé hodnoty odhadují. Cílem je, abychom statistiku pochopili tak, že ji budeme moci použít na konkrétní problém. Chceme dosáhnout toho, aby statistika byla jazykem, který můžeme použít na různé datové problémy. K pochopení statistiky budeme používat programovací jazyk R. Našim cílem je ale koncepty vysvětlovat a kódovat obecně tak, aby postupy byly lehko přenositelné do jiného programovacího jazyka. Vždy si tedy vysvětlíme konkétní výpočet nebo proceduru do podrobna a to i když existuje balíček nebo funkce, která by daný výpočet provedla za nás. 1.4 O modelech Modely jsou reprezentaci reality. Jsou (někdy) užitečné, protože zjednodušeně ukazují vlastnosti toho, co nás zajímá. Glóbus je příkladem modelu planety země. Glóbus nevystihuje přesně to, jak planeta vypadá. Nejsou na něm zaznamenány všechny ostrovy, jeho tvar neodpovídá přesně tvaru naší planety. Přesto jsou glóbusy užitečné k pochopení toho, jak planeta vypadá. Dalším příkladem modelu je mapa. Mapa je ještě více zkresleným modelem terénu než glóbus (mapa musí vněstnat 3D svět do 2D modelu). Mnoho map dokonce velmi nepřesně reprezentuje terén, přesto jsou ale mapy nesmírně užitečné když se potřebujeme dostat z bodu A do bodu B. Stejně je tomu s modely statistickými. Nejsou přesným vyjádřením reality, ale mohou být užitečným vyjádřením reality. Jejich užitečnost bude záviset na činnosti, pro který jsme tento model stvořili. Důležité je dodat, že naše modely (ne)fungují na datech, které jsme jim dodali. Model, který je užitečný na jedněch datech může být bezcenný na jiných datech. Je tedy vždy potřeba přemýšlet o tom, zda je náš model vhodný pro data a situaci, na kterou se ho snažíme použít. "],["popisná-statistika.html", "Kapitola 2 Popisná statistika 2.1 Míry centrální tendence 2.2 Míry rozptýlenosti 2.3 Míry polohy 2.4 Cvičení", " Kapitola 2 Popisná statistika Jak jsme zmínili v úvodu popisná statistika je jeden z hlavních cílů statistiky. Úkolem popisné statistiky je shrnout informace o našem výběru do pár čísel, které nám pomohou pochopit jaké má náš výběr vlastnosti. Hlavními vlastnostmi, které nás zajímají je: Jaká je typická hodnota měřené proměnné (míra centrální tendence) Na kolik se liší hodnoty jendotlivých pozorování (míra rozptýlenosti) Abychom si popisnou statistiku představili, budeme používat Novoroční/Vánoční projev prezidenta republiky. Nejdříve si data načteme. Text můžeme načíst různými způsoby, my použijeme funkci readLines, která vrátí zpět vektor. prezident &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/prezident.txt&quot;, encoding = &quot;UTF-8&quot;) Nejdříve text očistíme o mezery a čárky, přeneseme vše do malých písmen, rozdělíme na věty a potom na slova. Protože tuto proceduru budeme dělat vícekrát, uděláme si na to funkci. Výstupem této funkce bude list, jehož každý element reprezentuje jednu větu a v rámci této věty jaká obsahuje slova. vycistit_text &lt;- function(.text) { # vse malym pismem .text &lt;- tolower(.text) # odstranime prazdne radky .text &lt;- .text[.text != &quot;&quot;] # odstranime carky .text &lt;- gsub(pattern = &quot;,&quot;, replacement = &quot;&quot;, x = .text) # rozdelit na vety, pokud najdeme &quot;.&quot; nebo &quot;!&quot; nebo &quot;?&quot; .text &lt;- unlist(strsplit(.text, split = &quot;\\\\.|\\\\!|\\\\?&quot;)) # vymazeme mezery na zacatku a konci .text &lt;- trimws(.text, which = c(&quot;left&quot;)) .text &lt;- trimws(.text, which = c(&quot;right&quot;)) # odstranime prazdne prvky, ktere vznikly protoze po tecce neni zadny text .text &lt;- .text[.text != &quot;&quot;] # rozdelime na slova slova &lt;- list() for (i in 1:length(.text)) { slova[[i]] &lt;- unlist(strsplit(.text[i], split = &quot; &quot;)) } return(slova) } prezident_clean &lt;- vycistit_text(prezident) 2.1 Míry centrální tendence Míry centrální tendence se snaží popsat nějakou typickou hodnotu proměnné. My si představíme modus, medián, průměr, absolutní a relativní četnost. Jaké míry centrální tendence můžeme na proměnné vypočítat se liší podle typu proměnné. Nominální proměnná je taková proměnná, u které nemůžeme hodnoty seřadit od nejmenšího po největší a nemůžeme ani určit o kolik je jedna hodnota větší než jiná. Tou nejzákladnější popisnou statistikou je četnost nějakého jevu a z ní odvozená míra centrální tendence modus. Modus proměnné \\(x_i\\), který označujeme \\(\\hat{x}\\), je tedy nejčastější hodnota proměnné. Když se zamyslíte, tak u proměnné u které nemůžeme hodnoty seřadit ani jinak matematicky porovnat je nejčastější hodnota nejvíce vypovídající o typické hodnotě proměnné. Pojďme si jako příklad vypočítat nejčastější slovo z projevu. # nejprve prevedeme list na vekor slova &lt;- unlist(prezident_clean) # vypocitame cetnost hodnot tabulka_slov &lt;- table(slova) Naše tabulka četností je velká, obsahuje 651 hodnot. To mimo jiné znamená, že v projevu bylo použito 651 unikátních slov. Abychom získali nejčastější hodnotu musíme si tabulku seřadit od největší četnost po nejmenší a zobrazit první hodnotu. Jméno nejčastější hodnoty nám prozradí, modus této proměnné. tabulka_slov_serazena &lt;- sort(tabulka_slov, decreasing = TRUE) modus &lt;- names(tabulka_slov_serazena)[1] Modus této proměnné je hodnota “a,” která se vyskytla 45x. To je v textové analýze typické a tato slova se označují jako “stopwords” a jsou zpravidla a textové analýzy vyřazena. Další mírou centrální tendence je medián. Medián nám značí prostřední hodnotu nějaké proměnné. Můžeme si jeho výpočet představit tak, že hodnoty proměnné seřadíme od nejmenší po největší a vyberete hodnotu, která bude přesně uprostřed. Tato hodnota je medián. Matematicky se medián u proměnné \\(x_i\\) vypočítá jako \\[\\tilde{x} = x_{(n + 1)/2}\\] Pokud má naše proměnná sudý počet čísel, vypočítá se medián zpravidla jako průměr dvou prostředních hodnot, tedy \\[\\tilde{x} = \\frac{x_{n/2} + x_{n/2+1}}{2}\\]Řekněme, že bychom chtěli vědět medián počtu slov ve větě. Nejdříve si musíme pro každou větu (prvek listu prezident_clean) vypočítat počet slov. pocet_slov &lt;- sapply(prezident_clean, length) Velmi dobrým zvykem je si rozdělení hodnot proměnné zobrazit graficky. Grafické zobrzení nám vždy poví nejenom o typické hodnotě proměnné, ale i o tom, jak se hodnoty liší (viz Míry rozptýlenosti 2.2). Pokud máme číselnou proměnnou je nejčastějším způsobem, jak zobrazit hodnoty proměnné histogram. Histogram je: …grafické znázornění distribuce dat pomocí sloupcového grafu se sloupci stejné šířky, vyjadřující šířku intervalů (tříd), přičemž výška sloupců vyjadřuje četnost sledované veličiny v daném intervalu. Zdroj: Wikipedia Můžeme ho vytvořit pomocí funkce hist. Na grafu 2.1 vidíme, že nejčastěji věta obsahuje mezi 5 až 25 slovy, ale některé věty obsahují i více než 35 slov. hist(pocet_slov, col = &quot;#1f77b4&quot;, xlab = &quot;Počet slov ve větě&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram počtu slov ve větě&quot; ) Graf 2.1: Příklad histogramu na číselné (kardinální) proměnné Funkce hist automaticky zvolí vhodné intervaly pro sloupce v grafu. Pokud bychom je chtěli změnit, můžeme tak udělat pomocí argumentu breaks. hist(pocet_slov, col = &quot;#1f77b4&quot;, xlab = &quot;Počet slov ve větě&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram počtu slov ve větě&quot;, breaks = 15 ) Graf 2.2: Histogram s 15 intervaly n &lt;- length(pocet_slov) # protoze mame sudy pocet slov vypocitame prumer dvou prostrednich hodnot i1 &lt;- n / 2 i2 &lt;- i1 + 1 .median &lt;- mean(pocet_slov[c(i1, i2)]) # nebo pomoci funkce median(pocet_slov) Medián můžeme také vypočítat pomocí funkce median. Mediánový počet slov ve větě je 15.5. Pojďme si výpočet zobrazit graficky a seřadit si počet slov od nejmenšího po největší a zobrazit si medián na grafu. plot(1:n, sort(pocet_slov), col = &quot;#1f77b4&quot;, ylab = &quot;Počet slov ve větě&quot;, xlab = &quot;Pořadí&quot;, main = &quot;Počet slov ve větě seřazený podle velikosti&quot;, type = &quot;h&quot;, lwd = 5 ) # pridame prostredni hodnoty lines(i1, pocet_slov[i1], col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 5) lines(i2, pocet_slov[i2], col = &quot;grey&quot;, type = &quot;h&quot;, lwd = 5) # pridame median lines(mean(c(i1, i2)), .median, col = &quot;red&quot;, type = &quot;h&quot;, lwd = 1, lty = 2) lines(c(-2, mean(c(i1, i2))), c(.median, .median), col = &quot;red&quot;, type = &quot;l&quot;, lwd = 1, lty = 2) legend(&quot;topleft&quot;, col = c(&quot;grey&quot;, &quot;red&quot;), lwd = c(4, 4), legend = c(&quot;Prostřední hodnoty&quot;, &quot;Median&quot;), cex = 0.7 ) Graf 2.3: Seřazení proměnné pro ilustraci výpočtu mediánu Asi nejčastější mírou centrální tendence, která se používá je průměr. Průměr je možné vypočítat pouze pro kardinální proměnné. Technicky kardinální proměnné rozlišujeme na diskrétní a spojitou. Diskrétní nabývá celých čísel (1,2,3,4 etc., například počet dětí), tedy \\(\\in Z\\). Spojitá proměnná pak teoreticky nebývá nekonečně mnoho hodnot, prakticky je ale omezena tím, jak přesně dokážeme danou metriku měřit. Platí ale, že spojité proměnné nabývají racionálních čísel, tedy \\(\\in R\\). Průměr proměnné \\(x_i\\) vypočítáme jako \\[\\overline{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\] # prumer x_prumer &lt;- sum(pocet_slov) / length(pocet_slov) # nebo pomci funkce mean(pocet_slov) # vazeny prumer w &lt;- length(pocet_slov):1 x_prumer_w &lt;- sum(w * pocet_slov) / sum(w) # nebo pomci funkce weighted.mean(pocet_slov, w) Průměrný počet slov ve větě je 16.56. Někdy nechceme všem pozorováním při výpočtu průměru dát stejnou váhu. V takovém případě vypočítáme vážený průměr. Jeho vzorec je \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} w_ix_i}{\\sum_{i=1}^{n}w_i}\\] Řekněme například, že bychom průměrný počet slov chtěli vážit pozicí v textu a dát slovům v první větě větší váhu, než slovům, které se objevily později v textu. Takovýto vážený průměr je 17. Důležitý rozdíl mezi mediánem a průměr nastává pokud nejsou data symetricky rozdělena. Symetricky rozdělená data jsou taková, která mají podobný počet hodnot nalevo a napravo od průměru. V našem případě rozdělení počtu slov ve větě je medián menší než průměr. To je typické pro rozdělení, která mají delší konec napravo od průměru. Průměr je totiž náchylný na extrémní pozorování. Medián je založený na pořadí, takže ho extrémní pozorování tolik neovlivní. Většina proměnných, která je ohraničená zleva (nemůže mít menší hodnotu než nějaká hranice) má asymetrické rozdělení s více hodnotami napravo od průměru, např. příjem. V takových případech může být medián lepší mírou centrální tendence, ale výběr bude vždy záležet na otázce, kterou daty chceme zodpovědět. I pomocí měr centrální tendence se dají dělat zajívé analýzy. Řekněme, že nás zajímá rozdíl v relativní četnosti slov v projevu prezidenta a premiéra1. Graf 2.4 zobrazuje 20 slov2, které mají největší relativní četnost. Jak je vidět projev premiéra se zaměřoval častěji na problémy ČR, kdežto projev prezidenta se častěji zaměřoval na rozpočet a očkování3. # nacteme text projevu premiera premier &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/vlada.txt&quot;) premier_clean &lt;- vycistit_text(premier) # extrahujeme slova z vet do vektoru slova_premier &lt;- unlist(premier_clean) # nacteme a odstranime stopwords stopwords &lt;- readLines(&quot;https://raw.githubusercontent.com/schubertjan/uvod-do-statistiky/master/dats/stopwords.txt&quot;, encoding = &quot;UTF-8&quot;) # vybereme pouze slova, ktera nejsou ve stopwords slova_premier &lt;- slova_premier[!slova_premier %in% stopwords] slova &lt;- slova[!slova %in% stopwords] # vypocitame absolutni cetnost slov cetnost_premier &lt;- table(slova_premier) cetnost_prezident &lt;- table(slova) # vypocitame relativni cetnost cetnost_premier &lt;- cetnost_premier / sum(cetnost_premier) cetnost_prezident &lt;- cetnost_prezident / sum(cetnost_prezident) # vypocitame relativni cetnost # ted slouzime obe tabulky rel_cetnost &lt;- merge(data.frame(cetnost_premier), data.frame(cetnost_prezident), by.x = &quot;slova_premier&quot;, by.y = &quot;slova&quot; ) # prejmenuje sloupce, aby davaly vice smysl colnames(rel_cetnost) &lt;- c(&quot;slova&quot;, &quot;premier&quot;, &quot;prezident&quot;) head(rel_cetnost) ## slova premier prezident ## 1 alespoň 0.001686341 0.001547988 ## 2 bezpečnostní 0.001686341 0.001547988 ## 3 bojovat 0.001686341 0.001547988 ## 4 ceny 0.003372681 0.001547988 ## 5 české 0.006745363 0.001547988 ## 6 cesta 0.005059022 0.003095975 # vybereme nejcastejsich 20 slov pro každého top_premier &lt;- order(rel_cetnost$premier, decreasing = TRUE)[1:20] top_prezident &lt;- order(rel_cetnost$prezident, decreasing = TRUE)[1:20] # vyfiltrujeme top_index &lt;- unique(c(top_premier, top_prezident)) rel_cetnost_top &lt;- rel_cetnost[top_index, ] x &lt;- 1:nrow(rel_cetnost_top) plot(x, rel_cetnost_top$premier, type = &quot;h&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;Relativní četnost&quot;, col = &quot;black&quot;, lwd = 3 ) lines(x + 0.3, rel_cetnost_top$prezident, type = &quot;h&quot;, col = &quot;#1f77b4&quot;, lwd = 3 ) axis(1, at = x, labels = rel_cetnost_top$slova, las = 3, cex.axis = 0.8) legend(&quot;topright&quot;, legend = c(&quot;Premiér&quot;, &quot;Prezident&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(3, 3), cex = 0.8) Graf 2.4: Top 20 slov podle relativní četnosti v projevu prezidenta a premiéra 2.2 Míry rozptýlenosti Míry centrální tendence nám udávájí hodnotu typického pozorování proměnné. Míry rozptýlenosti nám říkají, jak jsou hodnoty rozptýleny okolo nějaké typické hodnoty. Nejjednodušší mírou rozptýlenosti je variační rozptyl, kdy nás zajímá rozdíl mezi největší a nejmenší hodnotou proměnní \\(x_i\\), tedy \\(VR = max(x_i) - min(x_i)\\). vr &lt;- max(pocet_slov) - min(pocet_slov) Minimální počet slov ve větě je 3, nejdelší věta má 43 slov. Variační rozpětí je tedy 40. Pro kardinální proměnné se nejčastěji používá rozptyl, který vypočítáme tak, že každou hodnotu proměnné odečteme od průměru a umocníme. Tyto hodnoty sečteme a vydělíme počtem pozorování. Matematicky bychom rozptyl \\(s^2\\) proměnné \\(x_i\\) vypočítali jako \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\overline{x})^2\\]4Ve statistice se také používá pro výpočet rozptýlenosti směrodatná odchylka \\(s\\), které se vypočítá jako \\[s = \\sqrt{s^2}\\]Protože umocněné vzdálenosti od průměru opět odmocníme, vrátíme je tím na stejnou škálu jako \\(x_i\\). Směrodatná odchylka tedy představuje typickou vzdálenost pozorování od průměru. Ukažme si příklad na počtu slov. n &lt;- length(pocet_slov) rozptyl &lt;- sum((pocet_slov - mean(pocet_slov))^2) / n # nebo v R pomoci var smerodatna_odchylka &lt;- sqrt(rozptyl) # nebo v R pomoci sd Rozptyl počtu slov ve větě je tedy 93.67 a směrodatná odchylka je 9.68. Typická vzdálenost od průměrného počtu slov ve větě, který je 16.56, je tedy zhruba 10 slov. Ukažme si princip rozptylu/směrodatné odchylky na imaginárních datech. Na ukázku si vytvoříme proměnnou, která má 10 pozorování a zobrazíme je do grafu 2.5 jako body. Červená čára označuje průměr těchto bodů. Horizontální čáry potom označují vzdálenost každého pozorování od průměrné hodnoty. Nejdříve si ukážeme příklad s menším rozptylem hodnot a pod ním příklad rozdělení s větším rozptylem hodnot. Protože mají oba příklady stejný počet pozorování (10), můžete si rozdíl v jejich směrodatné odhylce představit jako rozdíl vertikálních úseček, které vedou od průměru. par(mfrow = c(2, 1)) x &lt;- rnorm(1e4, mean = 5, sd = 1) prumer &lt;- mean(x) n &lt;- seq(1, 10) smerodatna_odchylka &lt;- sd(x) # prvni graf plot(x[n], n, main = paste0(&quot;Směrodatná odchylka: &quot;, round(smerodatna_odchylka, 2)), xlim = c(0, 10), xlab = &quot;&quot;, ylab = &quot;Číslo pozorovaní&quot;, pch = 19, col = &quot;#1f77b4&quot; ) abline(v = prumer, col = &quot;black&quot;, lwd = 2) for (i in n) { lines(c(prumer, x[i]), c(i, i), col = &quot;#1f77b4&quot;, lwd = 2) } legend(&quot;topright&quot;, legend = c(&quot;Průměr&quot;, &quot;Vzdal. od průměru&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) # druhy graf x2 &lt;- rnorm(1e4, mean = 5, sd = 2.5) prumer &lt;- mean(x2) smerodatna_odchylka &lt;- sd(x2) plot(x2[n], n, main = paste0(&quot;Směrodatná odchylka: &quot;, round(smerodatna_odchylka, 2)), xlim = c(0, 10), xlab = &quot;&quot;, ylab = &quot;Číslo pozorovaní&quot;, pch = 19, col = &quot;#1f77b4&quot; ) abline(v = prumer, col = &quot;black&quot;, lwd = 2) for (i in n) { lines(c(prumer, x2[i]), c(i, i), col = &quot;#1f77b4&quot;, lwd = 2) } legend(&quot;topright&quot;, legend = c(&quot;Průměr&quot;, &quot;Vzdal. od průměru&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) Graf 2.5: Ukázka výpočtu směrodatné odchylky Pro úplnost, jak by vypadal histogram rozdělení obou proměnných. V tomto případě jsme data generovali z normálního rozdělení (viz kapitola pravděpodobnostní rozdělení 3). hist(x, col = adjustcolor(&quot;#1f77b4&quot;, 0.9), breaks = 20, xlab = &quot;&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;, xlim = c(0, 10) ) hist(x2, col = adjustcolor(&quot;black&quot;, 0.4), breaks = 40, add = TRUE ) Graf 2.6: Histogram bodů z normálního rozdělení o průměru 5 a směrodatné odchylce 1 (modrá) a 2.5 (černá) 2.3 Míry polohy Poslední popisnou statistikou, kterou si představíme, jsou míry polohy. Ty nám udávají polohy nějakého pozorování proměnné vůči ostatním pozorováním. První mírou polohy, kterou si představíme je kvantil. Kvantil proměnné \\(x_i\\) se značí jako \\(x_q\\) a je to číslo, které nám říká podíl případů proměnné \\(x_i\\), které leží pod danou hodnotou proměnné \\(x_q\\). Medián proměnné je tedy vlastně specifická hodnota kvantilu \\(q=0.5\\), tedy \\(x_{0.5}\\). Pokud chceme data rozdělit na stejně velké podíly, můžeme použít jeden ze standardně používaných způsobů dělení, které jsou: Medián Kvantil rozdělující statistický soubor na dvě stejně početné množiny se nazývá medián, tzn. jedná se o kvantil \\(Q_{0.5}\\). Tercil Dva tercily rozdělují statistický soubor na třetiny. 1/3 prvků má hodnoty menší nebo rovné hodnotě prvního tercilu \\(Q_{1/3}\\), 2/3 prvků mají hodnoty menší nebo rovné hodnotě tercilu druhého \\(Q_{2/3}\\). Kvartil Tři kvartily rozdělují statistický soubor na čtvrtiny. 25 % prvků má hodnoty menší než dolní kvartil \\(Q_{0.25}\\) a 75 % prvků hodnoty menší než horní kvartil \\(Q_{0.75}\\); někdy se označují \\(Q_1\\) a \\(Q_3\\). Kvintil Čtyři kvintily dělí statistický soubor na pět stejných dílů. 20 % prvků souboru má hodnoty menší (nebo rovné) hodnotě prvního kvintilu, 80 % hodnoty větší (nebo &gt;rovné). Decil Decil dělí statistický soubor na desetiny. Jako k-tý decil označujeme \\(Q_{10/k}\\). Percentil Percentil dělí statistický soubor na setiny. Jako k-tý percentil označujeme \\(Q_{100/k}\\). Používá se například při vyhodnocení testů: Pokud má účastník umístění na 85. percentilu, znamená to, že 85 % účastníků mělo horší výsledek (a 15 % účastníků je lepších nebo stejných jako on [včetně jeho samého]). Znamená to, že účastník s nejlepším umístěním nebude mít percentil 100 %, ale nižší (o část vyjadřující procento jeho vlastního &gt;podílu na výsledku). Percentil tak vypočteme: \\(PR = \\frac{CF - (0.5 * F)}{N} * 100\\). Kde PR je hodnota percentilu, CF je kumulativní počet výsledků a F je počet výskytů počítaného výsledku (percentilu). Zdroj: Wikipedia V R můžeme empirický kvantil vypočítat pomocí funkce quantile. V argumentu prob stanovíme jakou pravděpodobnost (podíl hodnot menších) chceme vypočítat. Například vypočítejme kvartily počtu slov. kvartil &lt;- quantile(pocet_slov, prob = c(0.25, 0.5, 0.75)) kvartil ## 25% 50% 75% ## 9.00 15.50 22.75 Vidíme, že 25% vět má méně než 9 slov a že 25% vět má více než 23 slov (nebo že 75% vět má méně než 23 slov). Někdy se hodnoty kvantilů používají k výpočtu mezikvantilových rozpětí tak, že se honodty kvantilů odečtou mezi sebou. Například mazikvartilové rozpětí se vypočítá jako \\(Q_{0.75} - Q_{0.25}\\). 2.4 Cvičení Vytvořte funkci na výpočet percentilu. Vstupem do funkce bude numerický vektor \\(x\\) a hodnota kvantilu \\(q\\). Výstupem funkce bude hodnota percentilu. Načtěte projev Předsedy Senátu, Premiéra ČR a projev Prezidenta ČR z kapitoly nahoře. Který projev je celkově nejdelší? Který ústavní činitel má v průměru nejdelší věty? Jak se liší směrodatná odchylka počtu slov ve větě a co to znamená? Zobrazte rozdělení dat pomocí histogramu. Pro relativní četnost slov v dokumentu se používá termín term-frequency nebo také “tf.” Více informací zde.↩︎ Očištěných o stopwords.↩︎ Při textové analýze bychom potom dále využili nástroje jako bag-of-words abychom získali pouze kořeny slov apod.↩︎ V případě populačního rozptylu \\(\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^n (x_i - \\mu_x)^2\\).↩︎ "],["prob-dist.html", "Kapitola 3 Pravděpodobnostní rozdělení 3.1 Pravděpodobnostní rozdělení jako jazyk statistiky 3.2 Binomické 3.3 Normální 3.4 T 3.5 Uniformní 3.6 Poisson 3.7 Chi-kvadrát 3.8 Jak vybrat správný model pro data 3.9 Cvičení", " Kapitola 3 Pravděpodobnostní rozdělení Tuto kapitolu začneme videm. Matemateca (IME USP) / name of the photographer when stated, CC BY-SA 4.0 https://creativecommons.org/licenses/by-sa/4.0, via Wikimedia Commons Proč kuličky v tomto videu skončí v určitém množství v určitém sloupci? A proč skončí podobné množství kuliček v každém sloupci, pokud bychom kuličky pustili znova? Jak by se počet kuliček ve sloupcích lišil? Pokud si z této knihy máte odnést jednu kapitolu, pak by to měla být tato :). Jak jsme zmínili v úvodu knihy, většina proměnných okolo nás se chová náhodně. To znamená, že může nabývat náhodných hodnot podle nějakého klíče (procesu). Náhodnost může vycházet z faktu, že měříme jenom nějakou část populace (např. výběrové šetření), z fyzikálních vlastností (např. váha součástky vyrobená v továrně nebude vždy stejná) nebo z chyby měření (např. teploměr nezměří stejnou teplotu vždy stejně, ale hodnoty měření budou kolísat okolo nějakého čísla). Pravděpodobnostní rozdělení nám pomáhají kvantifikovat a predikovat míru nahodilosti. Typ pravděpodobnostního rozdělení, který na popsání náhodnosti uplatníme vychází z našeho porozumění vlastnostní rozdělení a jeho vhodnosti na daný problém. A vlastnosti pravděpodobnostních rozdělení je to, co si v této kapitole ukážeme. Na začátku jsme řekli, že budeme používat statistické modely a že tyto modely nejsou přesným vyjádřením reality, ale mohou být užitečným popsáním reality. V této kapitole začneme právě takové modely používat. 3.1 Pravděpodobnostní rozdělení jako jazyk statistiky V této kapitole si ukážeme spoustu nových značení, která se stanou našim jazykem, kterým budeme ve statistice komunikovat. Budeme je používat k tomu, abychom popsali očekávané chování náhodné proměnné. Každé rozdělení má svoje parametry, pomocí kterých ho můžeme popsat. Každé rozdělení má také svoji očekávanou hodnotu (typická hodnota rozdělení) a svůj roztyl (rozptýlenost okolo typické hodnoty). To, jakých hodnot a s jakou pravděpodobností náhodná proměnná nabývá, popisujeme pomocí pravděpodobnostní funkce (u diskrétních proměnných nabývajích celých čísel) nebo pomocí hustoty pravděpodobnosti (u spojitých proměnných nabývajích reálných čísel). V angličtině se pro hustotu pravděpodobnosti využívá pojem probability density function (PDF) a pro pravděděpodobnostní funkci pojem probability mass function (PMF). Integrál hustoty pravděpodobnosti/pravděpodobnostní funkce je vždy rovný jedné. To tedy znamá, že kdybychom sečetli všechny hodnoty PMF/PDF (pro spojité proměnné integrovali), tak by se výsledek rovnal jedné. To plyne z toho, že pravděpodobnost jevu nemůže být vyšší než jedna. Nakonec ještě trocha terminologie, které budeme používat. Náhodnou proměnnou nabývajících konkrétních hodnot budeme označovat jako \\(x_i\\). Budeme používat malá písmena, pokud výsledkem procesu bude vektor a velká písmena, pokud výsledkem bude matice. \\(i\\) označuje jednotlivá pozorování. Teda první hodnota náhodné proměnné, by se označovala jako \\(x_1\\). \\(n\\) označuje zpravidla počet pozorování proměnné. 3.2 Binomické Vraťme se k videu ze začátku kapitoly. Tomuto přístroji se říká Galton Box. Kuličky jsou puštěny do přístroje z jednoho stejného bodu a procházejí několika vrstvami (ve videu 10 vrstvami). V každé vrstvě míček narazí na bod, který ho může poslat na levou nebo na pravou stranu (zhruba se stejnou pravděpodobností 0.5). To, na jakou stranu se míček vydá je určeno náhodou. Kuličky potom spadnou do jednoho ze sloupců (ve videu 13 sloupců). Kdybysme nechali všechny kuličky spadnout, posbírali je a znovu spustili, tak skoro stejné množství kuliček skončí v každém sloupci. Jak je možné, že když spustíme tisíce kuliček, každý se může 10x odrazit nalevo nebo napravo, tak výsledný počet kuliček v každém sloupci je skoro stejný? Příčinou je pravděpodovnoství rozdělení. Prvním rozdělením, které si ukážeme je binomické rozdělení. Jevy mohou být generovany procesem, který vede k binomickému rozdělení, pokud máme \\(n\\) pokusů, jejichž výsledkem je úspěch nebo neúspěch, pokusy jsou nezávislé a mají konstantní pravděpodobnost úspěchu \\(p\\) (konstantní pro všechny pokusy). Obecně platí, že proměnná \\(x_i\\) pochází z binomického rozdělení, kde \\[x_i \\sim B(n, p)\\] a kde \\(n\\) značí počet pokusů a \\(p\\) pravděpodobnost úspěchu. Proměnná \\(x_i\\) je potom diskrétní. Pojďme si ukázat, jak se binomické rozdělení vztahuje ke Galtonově boxu. Každý míček prochází nejméně 10 pokusy, kde výsledek může být buď úspěch (řekněme, že míček spadne napravo) nebo neúspěch (řeknemě, že míček spadne nalevo). Výsledkem je potom zařazení do jednoho ze 13 sloupců. V extrému může náš proces skončit tak, že všechny kuličky budou nalevo nebo že všechny kuličky budou napravo. Kuličky jsou na sobě relativně nezávislé a všechny mají při každém pokusu relativně konstatní pravděpodobnost. Opět je nutné si uvědomit, že náš model dat (binomické rozdělení) není přesnou reprezentací procesu, který se snažíme popsat. Kuličky nejsou ve skutečnosti kompletně nezávislé, mohou do sebe narazit a tím se ovlivnit. Protože ale prochazí zhruba 10 pokusy (i když to se může lišit, některé kuličky se mohou odrazit po nárazu znovu nahoru), bereme proces jako dostatečně náhodný a nezávislý. Důležité je, jestli jsou předpoklady našeho modelu splněny dostatečně na to, aby jeho matematické vyjádření bylo validní pro reálný process, který se snažíme popsat. Pojďme si nyní ukázat, jak se Galtonův box dá připodobnit binomickým rozdělením. Náhodný process z binomického rozdělení můžeme generovat pomocí funkce rbinom. Ta jako argumenty předpokládá n počet pozorování, která chceme generovat, size počet pokusů (způsobů, jak může proces skončit) a p pravděpodobnost úspěchu. Budeme uvaživat, že spustíme 10 000 kuliček, které mohou skončit v 13 sloupcích, tedy \\[x_i \\sim B(13, 0.5)\\] set.seed(4) # pocet micku n &lt;- 10000 # pravdepodobnost uspechu (napravo) p &lt;- 0.5 # 0 by znamenalo všechny kulicky nalevo, 12 všechny kulicky napravo, # celkem 13 možností, jak mohou kulicky skončit s &lt;- 12 vysledek &lt;- rbinom(n = n, size = s, prob = p) cetnost &lt;- table(vysledek) plot(cetnost, xlab = &quot;x&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;Galtonův box s &quot;, n, &quot; kuličkami&quot;), col = &quot;#1f77b4&quot;, type = &quot;h&quot;, lwd = 15, xlim = c(0, 12) ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) Graf 3.1: Simulace Galtonova boxu s 13 sloupci Očekávanou hodnotu proměnné pocházející z binomické proměnné můžeme vypočítat jako \\[E(x_i) = np\\] rozptyl jako \\[Var(x_i) = npq\\] a směrodatbou odchylku jako \\[Sd(x_i) = \\sqrt{Var(X_i)}\\]V našem případě, kdy máme 12 možností, jak mohou kuličky skončit a očekávaná hodnota je tedy 6 a směrodatná odchylka 1.7320508. A ještě ukázka, že počet kuliček v každém sloupci je zhruba stejný, pokud bychom je pustili znova, řekněme 11x. N &lt;- 11 for (i in 1:N) { vysledek &lt;- rbinom(n = n, size = s, prob = p) micek &lt;- vysledek[1] cetnosti &lt;- table(vysledek) # pouzijeme funkci plot, abychom mohli pridat micek plot(cetnosti, xlab = &quot;x&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;Galtonův box s &quot;, n, &quot; kuličkami; pokus #&quot;, i), col = &quot;#1f77b4&quot;, lwd = 15, ylim = c(0, 2500), xlim = c(0, 12) ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) points(x = micek, y = 2, pch = 19, col = &quot;red&quot;) } Graf 3.2: Opakované spuštění kuliček v Galtonově boxu Jak jsme zmínili v 3.1 frekvenci hodnot diskrétní náhodné proměnné můžeme popsat matematicky pomocí pravděpodobnostní funkce (PMF). U binomického rozdělení můžeme PMF vypočítat jako \\[PMF(x_i) = \\binom{n}{k}p^kq^{n-k}\\] kde \\(k\\) je počet úspěšných pokusů a \\(q=1-p\\). V praxi můžeme využít funkce dbinom, která PMF vypočítá. Tato funkce argument x kam dosadíme hodnoty proměnné \\(x_i\\), pro které chceme PMF vypočítat. Argumenty size a prob jsou stejné jako u rbinom. Protože PMF je vypočítáno deterministicky (není tam žádná náhoda jako u generování náhodných čísel nahoře, čísla pouze dosadíme do vzorce), bude jeho výpočet stejný pokaždé, když zadáme stejné argumenty funkce. # muze padnou minimalne 0 (uplne nalevo) a maximalne 12 (uplne napravo) x &lt;- 0:12 s &lt;- 12 p &lt;- 0.5 pmf &lt;- dbinom(x = x, size = s, prob = p) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;Galtonův box&quot;), col = &quot;#1f77b4&quot; ) # vzdy ukazat vsechny moznosti i kdyz tam nejsou zadne micky axis(1, 1:13, 1:13) Graf 3.3: PMF pro Galtonův box s 13 sloupci Protože hodnoty \\(x_i\\) mohou nabývat pouze celých čísel, je PMF vyjádřením pravděpodobnosti toho, že náhodná proměnná \\(x_i\\) nabyde nějaké hodnoty. Nejpravděpodobněji skončí kulička ve sloupci 6 s pravděpodobností 0.23, nebo matematickým zápisem \\(P(x_i=\\) 6 \\()=\\) 0.23. Pravděpodobnost, že kulička skončí v 4 sloupci zleva je \\(P(x_i = 3)\\)=0.0537109. To, že dokážeme predikovat, kolik kuliček skončí v jakém sloupci, ještě neznamená, že víme, kde skončí jedna konkrétní kulička. V animaci 3.2 si všimněte červené tečky. Ta reprezentuje první kuličku, kterou jsme vhodili. Jak je vidět kulička cestuje mezi sloupci. To s jakou pravděpobností skončí v daném sloupci nám říká právě PMF. Stejně tak můžeme spočítat pravděpodobnost, že skončí v 7 sloupci a více napravo, tedy \\(P(x \\ge 7)\\)=0.39. Tomuto typu úlohy, kdy nás zajímá, zda náhodná proměnná \\(x_i\\) bude mít hodnotu menší/větší než nějaká hodnota kvantilu \\(q\\), říkáme kumulativní pravděpodobnosti (anglicky cumulative distribution function CDF). Matematicky bychom tento typ úlohy označili jako \\(P(x_i \\le q)\\) pokud by nás zajímala pravděpodobnost, že náhodná proměnná \\(x_i\\) bude menší nebo rovna než nějaká hodnota \\(q\\) a \\(P(x_i &gt; q)\\), pokud by nás zajímala pravděpodobnost, že hodnota náhodné proměnné \\(x_i\\) bude větší než nějaká hodnota \\(q\\). R můžeme na výpočet CDF u binomického rozdělení použít funkci pbinom. Jako argumenty očekává tato funkce q, hodnota náhodné proměnné vůči které porovnáváme (tedy kvantil), size a prob mají stejný význam jako u funkcí nahoře a argument lower.tail vyjadřující, zda chceme zjistit pravděpodobnost, že \\(x_i\\) bude menší rovno nebo větší než \\(q\\). Pokud má argument lower.tail hodnotu TRUE, pak počítáme \\(P(x_i \\le q)\\), pokud má hodnotu FALSE, pak počítáme \\(P(x_i &gt; q)\\). Pojďme si ukázat různé výpočty a jejich zobrazení na grafu. Začneme \\(P(x &gt; 7)\\). # P(x &gt; 7) p_x7 &lt;- pbinom(q = 7, size = s, prob = p, lower.tail = FALSE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &gt; 7)=&quot;, round(p_x7, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &gt; 7], pmf[x &gt; 7], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.4: Ukázka výpočtu P(x &gt; 7) pomocí PMF Dále si ukažme jak vypočítat \\(P(x \\le 5)\\). # P(x &lt;= 5) p_x5 &lt;- pbinom(q = 5, size = s, prob = p, lower.tail = TRUE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &lt;= 5)=&quot;, round(p_x5, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &lt;= 5], pmf[x &lt;= 5], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.5: Ukázka výpočtu P(x &lt;= 5) pomocí PMF A nakonec \\(P(x_i \\ge 6)\\). # P(x &gt;= 6) # pouzijeme FALSE na vypocet upper tail a za 1 dosadime 5, protoze... # ...lower.tail = FALSE pocita P(x_i &gt; q) p_x6 &lt;- pbinom(q = 5, size = s, prob = p, lower.tail = FALSE) plot(x, pmf, xlab = &quot;x&quot;, ylab = &quot;PMF&quot;, type = &quot;h&quot;, # jedna se o diskretni promennou lwd = 15, main = paste0(&quot;P(x &gt;= 6)=&quot;, round(p_x6, 2)), col = &quot;grey&quot; ) axis(1, 1:13, 1:13) # zobrazime graficky P(x &gt; 7) lines(x[x &gt;= 6], pmf[x &gt;= 6], type = &quot;h&quot;, # jedna se o diskretni promennou, lwd = 15, col = &quot;#1f77b4&quot; ) Graf 3.6: Ukázka výpočtu P(x &gt;= 6) pomocí PMF 3.3 Normální Normální rozdělení popisuje náhodný proces spojitých proměnných. Za určitých podmínek (velký výběr a \\(p\\) neblížící se 0 nebo 1) se binomické rozdělení bude podobat normálnímu. Normální rozdělení popisuje proces, kterým jsou generované náhodné spojité proměnné, které nejsou ohraničené zleva ani zprava. Jednotlivá pozorování jsou potom opět na sobě nezávislá. Protože mnoho fyzikálních jevů je spojitých5 a nejsou ohraničeny6, normální rozdělení se často vyskuteje v přírodě. Normální rozdělení má dva parametry, které určují jeho tvar, průměr \\(\\mu\\) a směrodatnou odchylku \\(\\sigma\\). Náhodná proměnná \\(x_i\\) pocházející z normálního rozdělení se zapíše jako \\[x_i \\sim N(\\mu, \\sigma)\\]Parameter \\(\\mu\\) ovlivňuje polohu rozdělení a parameter \\(\\sigma\\) jeho roztaženost nalevo a napravo od \\(\\mu\\). Protože náhodná proměnná \\(x_i\\) je spojitá, nabývá reálných čísel, může mít nekonečně mnoho hodnot. To představuje problém, pokud chceme vypočítat hustotu pravděpodobnosti (PDF). Aby byly zachovány vlastnosti pravděpodobnosti (\\(p \\in [0, 1]\\)), je integrál PDF rovný jedné. To ale znamená, že hodnota PDF už nevyjadřuje pravděpodobnost nějakého jevu. V angličtině se proto používá pojem density (odtud probability density function PDF). Vzorec pro PDF je \\[PDF(x_i) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} (\\frac{x - \\mu}{\\sigma})^2}\\] v R můžeme pro výpočet PDF normálného rozdělení použít funkci dnorm, které má argumenty x hodnoty náhodné proměnné \\(x_i\\), pro které chceme PDF vypočítat, mean průměr a sd směrodatnou odchylku. Pojďme si ukázat, jak by vypadala PDF pro tyto náhodné proměnné: \\[x_i \\sim N(0, 1)\\] \\[y_i \\sim N(3, 2)\\] \\[z_i \\sim N(-2, 3)\\] # definujeme si parametry mu &lt;- c(0, 3, -2) s &lt;- c(1, 2, 3) # definuje hodnoty nahodne promenne, pro kterou budeme chtit vypocitat pdf x &lt;- seq(-12, 12, length.out = 1000) # vypocitame pdf pdf_x &lt;- dnorm(x = x, mean = mu[1], sd = s[1]) pdf_y &lt;- dnorm(x = x, mean = mu[2], sd = s[2]) pdf_z &lt;- dnorm(x = x, mean = mu[3], sd = s[3]) # zobrazime plot(x, pdf_x, col = &quot;#1f77b4&quot;, lwd = 2, type = &quot;l&quot;, xlab = &quot;&quot;, ylab = &quot;PDF&quot;, xlim = c(-12, 12) ) lines(x, pdf_y, col = &quot;black&quot;, lwd = 2) lines(x, pdf_z, col = &quot;orange&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;x ~ N(0, 1)&quot;, &quot;y ~ N(3, 2)&quot;, &quot;z ~ N(-2, 3)&quot;), lwd = rep(2, 3), col = c(&quot;#1f77b4&quot;, &quot;black&quot;, &quot;orange&quot;), cex = 0.7 ) Graf 3.7: Ukázka vlivu průměru a směrodatné odchylky na tvar normálního rozdělení Normální rozdělení je symetrické. To znamená, že se nachází stejně hodnot nalevo a napravo od průměru. Podle hodnoty směrodatné odchylky dokážeme určit kolik případů bychom a jak daleko bychom očekávali, že budou ležet od průměru. Obrázek 3.8 ukazuje, že zhruba 68% hodnot náhodné proměnné bude ležet +/- jednu směrodatnou odchylku od průměru a zhruba 95% hodnot bude ležet +/- dvě směrodatnou odchylku od průměru a 99% hodnot +/- tři směrodatné odchylky od průměru. Graf 3.8: M. W. Toews, CC BY 2.5 https://creativecommons.org/licenses/by/2.5, via Wikimedia Commons Očekávanou hodnotou normálního rozdělení je průměr, tedy \\(E(x_i)=\\mu\\) a rozptylem je \\(var(x_i) = \\sigma^2\\), tedy směrodatnou odchylkou je \\(\\sigma = \\sqrt{\\sigma^2}\\). V dalších několika řádcích si představíme některé základní početní operace s náhodnými proměnnými pocházejícími z normálního rozdělení. Pojďme se nejdříve podívat, co se stane s funkcí normálního rozdělení, pokud k náhodné proměnné \\(x_i\\) přičteme skalár. Podíváme se na následující proměnnou \\(h_i \\sim N(178, 8)\\), tedy proměnnou, která pochází z normálního rozdělení s průměrem 178 a směrodatnou odchylkou 8. K této náhodné proměnné přičteme 20, tedy \\(h&#39;_i = h_i + 20\\) a spočítáme průměr a směrodatnou odchylku. Ke generování náhodných čísel z normálního rozdělení použijeme funkci rnorm, který má argumenty n počet pozorování, které chceme generovat, mean průměr rozdělení, z kterého chceme generovat a sd směrodatnou odchylku rozdělení, z kterého chceme generovat. n &lt;- 1e5 h &lt;- rnorm(n, mean = 178, sd = 8) h. &lt;- h + 20 par(mfrow = c(2, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(130, 240), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(h.), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(130, 240), main = paste0( &quot;h&#39;~N(&quot;, round(mean(h.), 0), &quot;, &quot;, round(sd(h.), 0), &quot;)&quot; ) ) Graf 3.9: Efekt přičtení skalaru na rozdělení proměnné Jak je vidět z 3.9 přičtení (nebo odečtění) má vliv pouze na průměr a tedy polohu rozdělení. V případě přičtení čísla \\(a\\) se celé rozdělení posune doprava o \\(a\\), v případě odečtení se potom posune celé rozdělení doleva. Přičtení/odečtení nemá vliv na směrodatnou odchylku, tedy na šířku rozdělení od průměru. Tento fakt můžeme zapsat jako \\[h&#39;_i \\sim N(\\mu_h \\pm a, \\sigma_h)\\] Dále se podíváme na to, co se stane s rozdělením stejné náhodné proměnné \\(h_i \\sim N(178, 8)\\) pokud ji vynásobíme skalarem. Náhodnou proměnnou \\(h_i\\) vynásobíme číslem 0.66, tedy \\(h&#39;_i = h_i * 0.66\\) a spočítáme průměr a směrodatnou odchylku. Jak je vidět z grafu 3.10 vynásobení (nebo jako v našem případě dělení) má vliv na průměr i směrodatnou odchylku. Oba parametry normálního rozdělení se změní o faktor \\(a\\) , kterým původní náhodnou proměnnou násobíme, tedy v našem případě se zmenší o třetinu. Tento fakt můžeme zapsat jako \\[h&#39;_i \\sim N(\\mu_h * a, \\sigma_h * a)\\] h. &lt;- h * 2 / 3 par(mfrow = c(2, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(80, 230), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(h.), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(80, 230), main = paste0( &quot;h&#39;~N(&quot;, round(mean(h.), 0), &quot;, &quot;, round(sd(h.), 0), &quot;)&quot; ) ) Graf 3.10: Efekt vynásobení skalarem na rozdělení proměnné Dále se podíváme, co se stane s průměrem a směrodatnou odchylkou, pokud k sobě přičteme dvě náhodné proměnné pocházející z normálního rozdělení. Budeme mít dvě náhodné proměnné, které pocházejí z normálního rozdělení \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(80, 10)\\). Budeme sledovat, co se stane s průměrem a směrodatnou odchylkou nové proměnné \\(k_i\\), která vznikne sečtením \\(h_i\\) a \\(l_i\\), tedy \\(k_i = h_i + l_i\\). Jak vidíme na grafu 3.11, nová proměnná má průměr rovný \\(\\mu_k = \\mu_h + \\mu_l\\) a \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\), tedy \\[k_i \\sim N(\\mu_h + \\mu_l, \\sqrt{\\sigma_h^2 + \\sigma_l^2})\\] l &lt;- rnorm(n = n, mean = 80, sd = 10) k &lt;- h + l par(mfrow = c(3, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(l), col = &quot;#1f77b4&quot;,lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) plot(density(k), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;k&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.11: Efekt přičtení dvou normálně rozdělených náhodných proměnných na rozdělení nové proměnné Jako další výpočetní operaci náhodných proměnných pocházejících z normálního rozdělení si ukážeme, co se stane pokud od sebe odečteme dvě náhodné proměnné pocházející z normálního rozdělení. Budeme opět uvažovat náhodné proměnné \\(h_i \\sim N(178, 8)\\) a \\(l_i \\sim N(80, 10)\\) a budeme počítat \\(k_i = h_i - l_i\\). Jak je vidět z grafu 3.12, nový průměr má hodnotu \\(\\mu_k = \\mu_h - \\mu_l\\), ale směrodatná odchylka je stále rovna \\(\\sigma_k = \\sqrt{\\sigma_h^2 + \\sigma_l^2}\\). To je důležitý poznatek o odečtu dvou normálně rozdělených proměnných. Směrodatná odchylka nové proměnné tedy bude vždy součtem směrodatných odchylek původních proměnných, protože rozptyl v datech se sčítá (jinak bychom mohli dosáhnout negativní \\(\\sigma\\), což není definováno, protože \\(\\sigma \\in [0, \\infty)\\). Tento jev se dá zapsat jako \\[k_i \\sim N(\\mu_h - \\mu_l, \\sqrt{\\sigma_h^2 + \\sigma_l^2})\\] k &lt;- h - l par(mfrow = c(3, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(l), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;l~N(&quot;, round(mean(l), 0), &quot;, &quot;, round(sd(l), 0), &quot;)&quot; ) ) plot(density(k), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;k&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, xlim = c(20, 330), main = paste0( &quot;k~N(&quot;, round(mean(k), 0), &quot;, &quot;, round(sd(k), 0), &quot;)&quot; ) ) Graf 3.12: Efekt odečtení dvou normálně rozdělených náhodných proměnných na rozdělení nové proměnné Poslední výpočetní operaci, kterou si ukážeme náhodnou proměnnou standardizuje tak, že její průměr bude rovný nule a směrodatná odchylka bude rovná jedné. Tento typ transformace je možné provést s proměnnou pocházející z jakéhokoliv rozdělení, ale my si ho zmiňujeme v kontextu normálního rozdělení, protože zde jej budem vídat nejčastěji. Tuto transformaci lze provést následujícím způsobem: \\[z_i = \\frac{(x_i - \\mu_x)}{sd(x_i)}\\]přičemž tedy platí, že pokud pochází \\(x_i\\) z normálního rozdělení, tak \\(z_i \\sim N(0, 1)\\)7. Pojďme si ukázat, jak by tato transofrmace vypadala pro náhodnou proměnnou \\(x_i \\sim N(178, 8)\\). Vzhledem k vlastnostem normálního rozdělení je tedy pro standardizovanou náhodnou proměnnou pocházející z normálního rozdělení snadné vypočítat kvantily (viz 3.8): \\(z_{0.023} = -2\\) \\(z_{0.159} = -1\\) \\(z_{0.5} = 0\\) \\(z_{0.841} = 1\\) \\(z_{0.977} = 2\\) z &lt;- (h - mean(h)) / sd(h) par(mfrow = c(2, 1)) plot(density(h), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;h&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, # urcime intrval od - 4 sd do + 4 sd xlim = c(178 - 4 * 8, 178 + 4 * 8), main = paste0( &quot;h~N(&quot;, round(mean(h), 0), &quot;, &quot;, round(sd(h), 0), &quot;)&quot; ) ) plot(density(z), col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;l&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, # urcime intrval od - 4 sd do + 4 sd xlim = c(-4, 4), main = paste0( &quot;z~N(&quot;, round(mean(z), 0), &quot;, &quot;, round(sd(z), 0), &quot;)&quot; ) ) Graf 3.13: Standardizovaná proměnná pocházející z normálního rozdělení Jak jsme zmínili, protože spojitá proměnná může nabývat nekonečně mnoho hodnot, nevyjadřuje PDF v nějakém konkrétním bodě pravděpodobnost tak, jako tomu je u PMF (např. u binomického rozdělení). Platí ale, že integrál PDF je rovný jedné. To znamená, že můžeme použít kumulativní pravděpodobnost (CDF) k výpočtu \\(P(x_i &gt; q)\\) nebo \\(P(x_i \\le q)\\). Funkce na výpočet CDF se jmenuje pnorm a stejně jako pbinom má argument q, který vyjadřuje hodnotu náhodné proměnné vůči které porovnáváme (kvantil), parametr lower.tail, který opět vyjadřuje pro jaký konec rozdělení chceme pravděpodobnost vypočítat (TRUE \\(P(x_i \\le q)\\), FALSE \\(P(x_i &gt; q)\\)) a potom parametry normálního rozdělení, tedy mean a sd, které udávají tvar rozdělení a mají stejný význam jako ve funkci rnorm a dnorm. Vraťme se zpátky k náhodné proměnné \\(h_i \\sim N(178, 8)\\). Ta představuje rozdělení výšky mužů v dospělé populaci. Řekněme, že by nás zajímalo, jaká je pravděpodobnost, že muž pocházející z této populace bude 185cm a vyšší, tedy \\(P(x_i \\ge 185)\\). Pro úplnost si opět tuto pravděpodobnost8 zobrazíme v grafu. x &lt;- seq(145, 210, length.out = 1000) q &lt;- 185 # vypoceteme si PDF, abychom ji mohli zobrazit v grafu pdf &lt;- dnorm(x = x, mean = 178, sd = 8) cdf &lt;- pnorm(q = q, mean = 178, sd = 8, lower.tail = FALSE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;h&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &gt;= &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &gt;= q], pdf[x &gt;= q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.14: Ukázka výpočtu P(x &gt;= 185) pomocí PDF Pro úplnost, ještě vypočítáme příklad kdy nás zajímá pravděpodobnost, že muž pocházející z této populace bude menší než 164cm, tedy \\(P(x_i &lt; 164)\\). q &lt;- 164 cdf &lt;- pnorm(q = q, mean = 178, sd = 8, lower.tail = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;h&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &lt; &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &lt; q], pdf[x &lt; q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.15: Ukázka výpočtu P(x &lt; 164) pomocí PDF 3.4 T Dalším z rozdělení, které si představíme je tzv. t-rozdělení, nebo také někdy nazývané studentovo. Toto rozdělení vychází z normálního rozdělení, ale má větší varibilitu. PDF normálního rozdělení rapidně klesá se vzdáleností od průměru (viz 3.8). Pokud potřebujeme popsat jev, který má více hodnot vzdálených od průměru, než bychom čekali u normálního rozdělení, je toho možné dosáhnout pomocí t-rozdělení. T-rozdělení je automaticky standardizované na \\(\\mu = 0\\) a má pouze jeden paramater - stupně volnosti (degrees of freedom) \\(\\nu\\), který určuje jak pravděpodobné jsou hodnoty více vzdálené od průměru. Čím je \\(\\nu\\) menší, tím je rozdělení více široké a naopak. Pokud je \\(\\nu\\) větší než 50, je t-rozdělení skoro totožné se standardizovaným normálním rozdělením \\(N(1, 0)\\). Pojďme si nyní ukázat PDF při různých stupních volnosti \\(\\nu\\) a jejich podobnost k \\(N(1, 0)\\). Ukážeme si \\(T(2)\\), \\(T(4)\\), \\(T(8)\\), \\(T(16)\\), \\(T(34)\\) a \\(T(64)\\). Jak je vidět z grafu 3.16, s rostoucím počtem stupňů volnosti se t-rozdělení přibližuje \\(N(1,0)\\), \\(T(64)\\) už skoro překrývá křivku PDF pro \\(N(1,0)\\) # stupne volnost sv &lt;- 2^c(1:6) # x pro ktere pocitame pdf x &lt;- seq(-5, 5, length.out = 1000) pdf_n &lt;- dnorm(x, mean = 0, sd = 1) pdf_t &lt;- lapply(sv, dt, x = x) # vybereme barvy pouzijeme nejsvetlejsi pro vetsi sv... # ...proto funkce rev, ktera obrati hodnoty cols &lt;- rev(RColorBrewer::brewer.pal(n = length(sv), name = &quot;Blues&quot;)) # zobrazime pdf N(1,0) plot(x, pdf_n, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, type = &quot;l&quot;, lwd = 3, col = &quot;black&quot; ) # zobrazime pdf vsech T for (i in 1:length(sv)) { lines(x, pdf_t[[i]], type = &quot;l&quot;, lwd = 2, col = cols[i]) } legend(&quot;topright&quot;, legend = c(&quot;N(1,0)&quot;, paste0(&quot;T(&quot;, sv, &quot;)&quot;)), col = c(&quot;black&quot;, cols), lwd = rep(2, length(sv) + 1), cex = 0.7 ) Graf 3.16: Podobnost t-rozdělení k N(1,0) K výpočtu PDF t-rozdělení jsme použili funkci dt, která má argument x hodnoty proměnné x, pro které chci PDF vypočítat a df, počet stupňů volnosti. Pokud bychom chtěli generovat náhodnou proměnnou z t-rozdělení, můžeme tak učinit pomocí funkce rt. Stejně tak CDF t-rozdělení můžeme vypočítat pomocí funkce pt. Očekávaná hodnota t-rozdělení je 0, tedy \\(E(x_i) = 0\\) a rozptyl je roven \\(var(x_i) = \\frac{\\nu}{\\nu-2}\\). R obsahuje pouze tradiční t-rozdělení, které je definováno pouze stupni volnosti \\(\\nu\\). Pokud bychom chtěli pomocí t-rozdělení popsat rozdělení, která mají jiný průměr a rozptyl, museli bychom t-rozdělení zobecnit. Pokud bychom tedy chtěli stanovit t-rozdělení s průměrem \\(\\mu\\) a směrodatnou odchylkou \\(\\sigma\\) pro náhodnou proměnnou \\(x_i\\), pak bude platit \\[x_i = \\mu + \\sigma T\\]Očekávaná hodnota náhodné proměnné pocházející z zobecněného t-rozdělení je \\(E(x_i) = \\mu\\) a rozptyl \\(var(x_i) = \\sigma^2 \\frac{\\nu}{\\nu-2}\\). PDF tohoto rozdělení bychom vypočítali jako: \\[PDF(x_i) = \\frac{1}{\\sigma} PDF_T(\\frac{x_i - \\mu}{\\sigma}, \\nu)\\]Všimněme si, že vlastně počítáme PDF t-rozdělení pro standardizovanou proměnnou a toto PDF násobíme faktorem \\(\\frac{1}{\\sigma}\\). Na grafu 3.17 ukazuje příklad PDF zobecněného t-rozdělení pro rozdělení s průměrem 178, směrodatnou odchylkou 8 a 5 stupni volnosti. Náhodnou proměnnou \\(x_i\\) pocházející z tohoto rozdělení bychom mohli zapsat jako \\(x_i \\sim T(178, 8, 5)\\). Jak je vidět menší stupně volnosti znamenají, že hodnoty více vzdálené od průměru jsou u takové náhodné proměnné více pravděpodobné, než u normálního rozdělení. # definujeme si parametry rozlozeni mu &lt;- 178 s &lt;- 8 sv &lt;- 5 # vytvorime x pro ktere chceme vypocitat pdf x &lt;- seq(178 - 5 * s, 178 + 5 * s, length.out = 1000) # vypocteme pdf N(178,8) pro porovnani pdf_n &lt;- dnorm(x, mean = mu, sd = s) # vypocteme pdf T(178,8,5) pdf_t &lt;- dt((x - mu) / s, df = 5) pdf_t. &lt;- 1 / s * pdf_t plot(x, pdf_n, type = &quot;l&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, col = &quot;grey&quot; ) lines(x, pdf_t., type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot; ) legend(&quot;topright&quot;, legend = c(&quot;N(178,8)&quot;, &quot;T(178,8,5)&quot;), col = c(&quot;black&quot;, &quot;#1f77b4&quot;), lwd = c(2, 2), cex = 0.7 ) Graf 3.17: Zobecněné t-rozdělení Pro úplnost ještě dodáme, že CDF náhodné proměnné \\(x_i\\) zobecněného t-rozdělení se vypočítá jako \\[CDF(x_i) = CDF_T(\\frac{x_i - \\mu}{\\sigma}, \\nu)\\]Zajímá nás jaká je pravdědobnost, zda hodnota náhodné proměnné pocházející z \\(x_i \\sim T(178,8,5)\\) bude mít hodnotu větší nebo rovnou než 185, tedy \\(P(x_i \\ge 185)\\). Porovnejte tuto pravděpodobnost se stejným výpočtem pro náhodnou proměnnou pocházející z \\(N(178,8)\\). q &lt;- 185 cdf_t. &lt;- pt((q - mu) / s, df = sv, lower.tail = FALSE) plot(x, pdf_t., type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;x&quot;, ylab = &quot;PDF&quot;, main = paste0(&quot;P(x &gt;= &quot;, q, &quot;)=&quot;, round(cdf_t., 2)) ) # zvoline type = &quot;h&quot; abychom zobrazili integral lines(x[x &gt;= q], pdf_t.[x &gt;= q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.18: CDF zobecněného t-rozdělení pro P(x &gt;= 185) 3.5 Uniformní Uniformní rozdělení popisuje náhodný proces, v kterém mají všechny hodnoty v nějakém intervalu stejnou pravděpodobnost, že budou vybrány. Uniformní rozdělení je definováno jak pro diskrétní, tak pro spojité proměnné. S diskrétním uniformním rozdělením jsme se setkali minulý rok při hodu kostkou nebo hodu mincí. Pokud náhodná proměnná \\(x_i\\) pochází z uniformního rozdělení, pak platí, že \\[x_i \\sim U(a,b)\\]Uniformní rozdělení je tedy možná vyjádřit pomocí dvou parametrů \\(a\\) a \\(b\\), které vyjadřují minimální a maximální možné hodnoty proměnné. PMF diskrétního uniformního rozdělení rozdělení lze vyjádřit jako \\[PMF(x_i) = \\frac{1}{n}\\]U spojitého uniformního rozdělení lze PDF vyjádřit jako \\[PDF(x_i) = \\frac{1}{b-a}; x_i \\in [a,b]\\]Pokud je hodnota \\(x_i\\) mimo \\([a,b]\\), pak je jeho PDF rovná nule (nebo jinak taková hodnota není možná). Očekávaná hodnota diskrétního i spojitéhouniformního rozdělení se spočítá jako \\(E(x_i) = \\frac{1}{2}(a+b)\\). Rozptyl u spojitého uniformního rozdělení se spočítá jako \\(Var(x_i) = \\frac{1}{12}(b-a)^2\\) a u diskrétního jako \\(Var(x_i) = \\frac{n^2-1}{12}\\). Protože generování čísel z diskrétního uniformního rozdělení můžeme použít funkci sample, kterou jsme používali při simulacích některých klasických pravděpodobnostních problémů. Pro generování hodnot náhodné proměnné pocházející ze spojitého uniformního rozdělení můžeme použít funkci runif. PDF poté vypočítáme pomocí funkce dunif a CDF pomocí punif. Jako příklad uniformního rozdělení si ukážeme čekání na tramvaj. Řekněme, že přijdeme na zastávku a máme se rozhodnout, zda se nám vyplatí čekat nebo jít pěšky. Čekání na tramvaj v takovém případě můžeme modelovat (připodobnit) uniformním rozdělením. Nevíme, kdy tramvaj přijede (v našem příkladu nejsou jízdní řády, nebo alespoň nejsou spolehlivé jízdní řády :). Může přijet hned (tedy za 0min), může přijet za 8min. Pravděpodobnost, za jak dlouho přijede (respektive, v jakém momentu jsme my přisli na zastávku) je rovnoměrně rozdělena v celém intervalu. Pojďme si tento proces vyjádřit v animaci. Červená tečka reprezentuje nás čekající na zastávce. Modrý čtverec reprezentuje tramvaj blížící se k zastávce. Jak je vidět, někdy přijdeme na zastávku a tramvaj je blízko. Někdy přijdeme a tramvaj je stále daleko. # muzeme cekat 0 minut az 8 minut a &lt;- 0 b &lt;- 8 # pocet simulaci N &lt;- 5 # kde bude tram, kdyz prijdeme na zastavku x &lt;- seq(0, 8, by = 0.5) # pro kazdy pokus for (n in 1:N) { # vybereme nahodne jak daleko od nas tram bude ... # ...-0.5 protoze pricitame +0.5 v prvnim kroku tramvaj &lt;- runif(1, min = a, max = b) - 0.5 my &lt;- 8 # dokud neni tramvaj u nas while (tramvaj &lt; my) { # zobrazime kazdych 0.5 minuty tramvaj &lt;- tramvaj + 0.5 # protoze pridavame kazdych 0.5 min ... # ...abychom zobrazili tramvaj u nas na zastavce if (tramvaj &gt; my) tramvaj &lt;- my plot(tramvaj, 1, xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;, xaxt = &quot;n&quot;, xlim = c(-1, 9), ylim = c(0, 2), pch = 22, col = &quot;#1f77b4&quot;, main = paste0(&quot;Pokus: &quot;, n), sub = paste0(&quot;Zbývající doba čekání: &quot;, round(my - tramvaj, 1), &quot;min&quot;) ) # zobrazime nas points(my, 1, pch = 19, col = &quot;red&quot;) # otocime labely cekani 8-0 axis(1, at = x, labels = rev(x)) # pridame popisek text(4, 0, &quot;Vdálenost tramvaje od naší zastávky&quot;) legend(&quot;topright&quot;, legend = c(&quot;my&quot;, &quot;tramvaj&quot;), col = c(&quot;red&quot;, &quot;#1f77b4&quot;), pch = c(19, 22), cex = 0.7 ) } } Graf 3.19: Ukázka čekání na tramvaj jako náhodný proces z uniformního rozdělení Pěšky je to do našeho cíle 6min. Tramvají 1min. Tramvaj jezdí jednou za 8min, tedy \\(t_i \\sim U(0, 8)\\). Vyplatí se nám čekat? ocekavana_hodnota &lt;- (a + b) / 2 Očekávané hodnota našeho čekání je 4. Když k tomu přičteme 1min cesty tramvají, pak očekávaná hodnota naší cesty tramvají je 5min. Řekněme, že je velmi důležité, abychom nedorazili pozdě. Jaká je pravděpodobnost, že tramvají do cíle dorazíme za dobu delší než 6min (tedy, že nám bude cesta trvat déle, než pěšky)? x &lt;- seq(-1, 9, length.out = 1000) q &lt;- 5 pdf &lt;- dunif(x = x, min = a, max = b) cdf &lt;- punif(q, min = a, max = b, lower.tail = FALSE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlab = &quot;t&quot;, ylab = &quot;PDF&quot;, xlim = c(-1, 9), main = paste0(&quot;P(x &gt; &quot;, q, &quot;)=&quot;, round(cdf, 2)) ) # type &quot;h&quot;, abychom vyjadrili integral lines(x[x &gt; q], pdf[x &gt; q], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) Graf 3.20: Příklad PDF pro t ~ U(0, 8) a CDF pro P(t &gt; 5) Jak je vidět, pravděpodobnost, že tramvaj přijede za 5 a více minut (musíme si nechat 1 minutu na cestu tramvají, proto chceme vědět \\(P(t_i &gt; 5)\\)) je 0.38, tedy docela vysoká. Tato situace by nastala zhruba 1 ze 3 případů. Pravděděpodobně bychom tedy v takovém případě šli pěšky. 3.6 Poisson Poissonovo rozdělení popisuje chování diskrétních náhodných proměnných v nějakém daném časovém intervalu. Od binomického rozdělení se liší tím, že popisuje proměnné, které nemají pevně stanovený počet pozorování, tedy \\(x_i \\in [0, \\infty)\\). Toto rozdělení má jediný parametr \\(\\lambda\\), který je zároveň očekávanou hodnotou a rozptylem. Stejně jako u binomického, normálního, t a uniformního rozdělení předpokládáme, že jednotlivé hodnoty náhodné proměnné jsou na sobě nezávislé. Náhodnou proměnnou \\(x_i\\) pocházející z tohoto rozdělení značíme jako \\(x_i \\sim Poisson(\\lambda)\\). Mnoho každodenních jevů, jejichž hodnoty jsou celé čísla se dají popsat poissonových rozdělením. U mnoha jevů totiže neznáme \\(n\\) a tedy nedokážeme říct, jaká je největší očekávaná hodnota. Jako příklady jevů, které je možné approximovat (připodobnit) poissonových rozdělením je například počet telefonátů za den, počet aut, které projedou silnicí za hodinu, počet gólů za zápas apod. Vrátíme se zpět do kapitoly popisné statistiky 2.1, kde jsme vypočítali počet slov ve větě v projevu prezideta republiky. Pro připomenutí ukážeme četnosti počtu slov ve větě. Graf 3.21: Četnost počtu slov ve větě v projevu prezidenta republiky Řekněme, že bychom chtěli predikovat počet slov ve větě v nějaké prezidentově dalším projevu a chtěli bychom to udělat tak, že bychom se snažili zjistit parametry procesu, který počet slov vyprodukoval. Víme, že minimálně můžeme mít ve větě jedno slovo. Maximální počet slov je sice prakticky ohraničen, ale my neznáme žádnou hranici, kterou bychom mohli uplatnit. Budeme tedy předpokládat, že ohraničený není9. Zároveň předpokládáme, že hodnoty náhodné proměnné vygenerované z poissonova rozdělení jsou nezávislé. V našem případě jsou hodnoty spíše silně závislé. Počet slov v jedné větě bude určitě ovlivňovat počet slov v následující(ch) větách. Jak je vidět na 3.22 po hodně dlouhých větách, častěji následují kratší věty. Málokdy je více krátkých vět za sebou a naopak. plot(pocet_slov, type = &quot;l&quot;, lwd = 2, ylab = &quot;Počet slov&quot;, xlab = &quot;Pořadí&quot;, col = &quot;#1f77b4&quot; ) Graf 3.22: Zkoumání závislosti jednotlivých hodnot proměnné počet slov lambda &lt;- mean(pocet_slov) Průměrný počet slov ve větě je 16.56, což je \\(\\lambda\\) našeho rozdělení. Modelujeme tedy počet slov \\(s_i\\) ve větě \\(s_i \\sim Poisson(\\) 16.56 \\()\\).Graf 3.23 vyjadřuje PMF tohoto rozdělení. Jak je vidět tento model na naše data moc nesedí. Náš model má moc malý rozptyl v porovnání s daty. Protože poissonovo rozdělení má pouze jeden parametr \\(\\lambda\\), který vyjadřuje očekávanou hodnotu i rozptyl, nemůžeme rozptyl nijak kontrolovat pomocí parametru rozdělení. Tento fakt je koneckonců zřejmý už z chrakteristik našich dat. Pokud by tato data opravdu byla vygenerována z poissonova rozdělení jejich průměr by se zhruba rovnal rozptylu Jak je ale vidět, tak tomu není protože průměr počtu slov je 16.56 a rozptyl je 95.11. x &lt;- 1:50 n &lt;- length(pocet_slov) pmf &lt;- dpois(x, lambda) plot(x, pmf, type = &quot;h&quot;, lwd = 5, col = &quot;#1f77b4&quot;, xlab = &quot;s&quot;, ylab = &quot;PMF&quot;, main = paste0(&quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot;) ) Graf 3.23: PMF poissonova rozdělení I když je náš model jednoduchý s pouze jedním parametrem \\(\\lambda\\), je to model, který můžeme použít pro predikci. Výsledky jakéhokoliv modelu je vždy dobré si zobrazit graficky, abychom si dokázali lépe představit, co predikuje. 3.24 ukazuje, kolik by náš model predikoval počtet slov ve větě. Protože PMF vyjadřuje pravděpodobnost, můžeme očekávaný (predikovaný) četnost počtu slov podle našeho modelu vypočítat jako \\(PMF * n\\). V našem případě máme 66 vět, tedy predikovaná četnost počtu slov \\(\\hat{s_i} = PMF * n\\). # zaokrouhlime, protoze musime mit cela cisla s_hat &lt;- round(pmf * n) # pridame data .x &lt;- as.numeric(names(cetnost)) # zobrazime nase data plot(.x, cetnost, type = &quot;h&quot;, lwd = 5, col = &quot;grey&quot;, xlab = &quot;Počet slov&quot;, ylab = &quot;Četnost&quot;, xlim = c(0, 50), ylim = c(0, 6), main = &quot;Porovnání modelu poissonova rozdělení s daty&quot; ) # pridame ocekavane hodnoty lines(x, s_hat, col = adjustcolor(&quot;#1f77b4&quot;, alpha.f = 0.4), type = &quot;h&quot;, lwd = 5 ) # aby y osa zdy vyjadrovala vsechna cisla axis(2, at = c(0:6), labels = c(0:6)) # legenda legend(&quot;topright&quot;, legend = c( &quot;Data&quot;, paste0( &quot;Poisson(&quot;, round(lambda, 2), &quot;)&quot; ) ), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) Graf 3.24: Očekávaná četnost počtu slov ve větě pro n=66 vět Toto se v reálném světě často. Tento jev se v angličtině označuje jako overdispersion. Jednou z možností, jak se s overdispersion vypořádat je, že modelujeme náhodnou proměnnou jako pocházející z jiného rozdělení. Pro náš jednoduchý příklad bychom mohli využít častého triku, který převede pozitivní diskrétní data s větším rozptylem na data spojitá. Tento trik využívá transformace proměnné a to konkrétně jejího logaritmu. Jak je známo logaritmus je definován pro definiční obor \\(\\in (0, \\infty)\\). Pro zopakovaní ukazujeme funkci přirozeného logaritmu v grafu 3.25. Jak je vidno, tak funkce logaritmu převede hodnoty do oboru hodnot \\(\\in (-\\infty, \\infty)\\) a zároveň budou její hodnoty spojité. Normální rozdělení je takto definováno a proto zkusíme modelovat počet slov ve větě \\(s_i\\) jako \\(log(s_i) \\sim T(\\mu_{log(s)}, \\sigma_{log(s)}, n)\\). curve(log(x), from = 0.01, to = 100, col = &quot;#1f77b4&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;Log(x)&quot;, main = &quot;Přirozený logaritmus&quot; ) abline(h = 0, v = 0, lwd = 1) Graf 3.25: Ukázka přirozeného logaritmu Graf 3.26 zobrazuje histogram logaritmu početu slov ve větě a teoretické PDF \\(T(2.61, 0.66, 65)\\). Jak je vidět takovýto model lépe vystihuje rozptyl v počtu slov. Vzhledem k tomu, že máme pouze 66 pozorování nebude taková náhodná proměnná přesně vystihovat teoretické rozdělení. Graf 3.27 ukazuje odhadnutou hustotu pravděpodobnosti pro 50 výběrů z \\(T(2.61, 0.66, 66)\\), každý o velikosti \\(n=\\) 66. Modrá křivka ukazuje naše data, šedé křivky potom simulované výběry. Je vidět, že několiktakových výběrů, které jsou podobně nahnuté doprava nastalo, což znamená, že taková data nejsou tak nepravděpodobná (pokud pocházejí z \\(log(s_i) \\sim T(2.61, 0.66, 66)\\)). # transfromujeme promennou s_log &lt;- log(pocet_slov) # vypocitame prumer mu &lt;- mean(s_log) # vypocitame rozptyl s &lt;- sd(s_log) # stanovime x pro ktere vypocitame pdf x &lt;- seq(0.01, 5, length.out = 1000) # vypocitame pdf zobecneneho t rozdeleni pdf &lt;- dt((x - mu) / s, df = n) * 1 / s hist(s_log, xlab = &quot;log(s)&quot;, ylab = &quot;Četnost&quot;, main = &quot;Histogram logaritmu počtu slov&quot;, col = &quot;grey&quot;, breaks = 10, xlim = c(0, 5) ) # pridame na druhou osu par(new = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, bty = &quot;n&quot; ) legend(&quot;topright&quot;, legend = c( &quot;log(s)&quot;, paste0(&quot;T(&quot;, round(mu, 2), &quot;,&quot;, round(s, 2), &quot;,&quot;, n,&quot;)&quot;) ), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) thicks &lt;- round(seq(min(pdf), max(pdf), by = 0.05), 2) axis(4, at = thicks, labels = thicks) Graf 3.26: rozdělení logaritmu dat a teoretická PDF T(2.61, 0.66, 66) # odhadneme hustotu pravdepodobnosti dat d_data &lt;- density(s_log) S &lt;- 50 plot(d_data, type = &quot;l&quot;, xlab = &quot;log(s)&quot;, ylab = &quot;Hustota pravděp. (density)&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, xlim = c(0, 5), ylim = c(0, 0.85), lwd = 2 ) for (i in 1:S) { # provedeme vybery z t rozdeleni o stejne velikosti vyberu vyber &lt;- rt(n, df = n) * s + mu lines(density(vyber), col = adjustcolor(&quot;black&quot;, alpha.f = 0.2)) } Graf 3.27: Příklad hustoty pravděpodobnosti 50 výběrů o velikosti n=66 z T(2.61, 0.66, 66) Nakonec se tedy vrátíme k naší původní otázce - predikce počtu slov ve větě. Jak jsme si ukázali, logaritmus počtu slov ve větě můžeme dobře approximovat normálním rozdělením. Na tomto rozdělení můžeme počítat CDF stejně tak, jak jsme si to ukázali v 3.3. Řekněme, že z důvodu srozumitelnosti nedoporučejeme věty delší než 20 slov. Zajímá nás, jaká je pravděpodonost, že libovolně zvolená věta bude delší než 20 slov, pak vlastně počítáme \\(P(log(s_i) &gt; log(20))\\) z našeho \\(log(s_i) \\sim N(2.61, 0.66)\\)10. q &lt;- log(20) cdf &lt;- pt((q - mu) / s, df = n, lower.tail = FALSE) Tato pravděpodobnost je rovna 0.28. Nebo jinak zhruba 28% vět bude delších než 20 slov. 3.7 Chi-kvadrát \\(\\chi^2\\) rozdělení popisuje náhodný proces, kterého nabývá součet umocněných stadandardizovaných normálně rozdělených proměnných. Jak jsme uvedli v 3.3, standardizovaná náhodná proměnná pocházející z normálního rozdělení \\(z_i\\) má \\(\\mu_z = 0\\) a \\(\\sigma_z = 1\\). Pokud máme \\(k\\) standardizovaných náhodných proměnných, pak můžeme zapsat \\(Q = \\sum_{i=1}^k z_i^2\\). Toto rozdělení se nepoužívá tak často k popsání jevů z fyzického světa jako spíše pro popsání chování různých parametrů, které se ve statistice odhadují. Protože mnoho odhadovaných paramtrů (jako např. průměr) mají normální rozdělení, můžeme jejich pravděpodobnostní rozdělení popsat právě pomocí \\(\\chi^2\\), což se hodí při posuzování významnosti různých procedur. \\(\\chi^2\\) má jediný parameter \\(k\\), který vyjadřuje počet stupňů volnsti (degrees of freedom). V 3.28 ukazujeme, jak \\(\\chi^2\\) rozdělení vzniká. Děláme výběr o velikosti \\(n=200\\) ze standardizovaného normálního rozdělení pro \\(k=2\\) proměnné, \\(k=4\\) a \\(k=16\\), tedy \\(z \\sim \\chi^2(2)\\), \\(z \\sim \\chi^2(4)\\) a \\(z \\sim \\chi^2(16)\\). Proměnné umocníme, sečteme a zobrazíme histogram rozdělení této nově vzniklé náhodné proměnné. # velikost vyber n &lt;- 200 # pocet standardizovanych promennych K &lt;- c(2, 4, 16) # zobrazit grafy vedle sebe par(mfrow = c(3, 1)) for (k in K) { Z &lt;- sapply(1:k, function(x) rnorm(n, 0, 1)) X2 &lt;- rowSums(Z^2) hist(X2, xlab = &quot;z&quot;, ylab = &quot;Četnost&quot;, main = paste0(&quot;X2(&quot;, k, &quot;)&quot;), col = &quot;#1f77b4&quot;, ) } Graf 3.28: Ukázka X2 rozdělení pro k=2, k=4 a k=16 Jak je vidět \\(\\chi^2\\) je definováno pouze pro \\([0, \\infty)\\), tedy pro pozitivní hodnoty spojité proměnné. To je proto že počítáme s mocninou, tedy žádné negativní hodnoty v našem definičním oboru být nemohou. Očekávanou hodnotou je \\(k\\), tedy \\(E(z_i) = k\\) a rozptyl je \\(2k\\), tedy \\(Var(z_i)=2k\\). S rostoucím počtem stupňů volnosti \\(k\\) se rozdělení tvarem podobá normálnímu rozdělení s velkým rozptylem (protože rozptyl je roven \\(2k\\)). Graf 3.29 zobrazuje PDF pro \\(z \\sim \\chi^2(2)\\), \\(z \\sim \\chi^2(4)\\), \\(z \\sim \\chi^2(8)\\), \\(z \\sim \\chi^2(16)\\) a \\(z \\sim \\chi^2(32)\\). x &lt;- seq(0, 50, length.out = 1000) K &lt;- 2^c(1:5) PDF &lt;- sapply(K, function(k) dchisq(x, k)) cols &lt;- rev(RColorBrewer::brewer.pal(n = length(K) + 1, name = &quot;Blues&quot;)) plot(x, PDF[, 1], type = &quot;l&quot;, xlab = &quot;z&quot;, ylab = &quot;PDF&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, lwd = 2 ) for (j in 2:length(K)) { lines(x, PDF[, j], type = &quot;l&quot;, lwd = 2, col = cols[j] ) } legend(&quot;topright&quot;, legend = paste0(&quot;X2(&quot;, K, &quot;)&quot;), col = c(&quot;#1f77b4&quot;, cols[2:length(K)]), lwd = rep(2, length(K)), cex = 0.7 ) Graf 3.29: PDF X2 rozdělení s k=2, k=4, k = 8, k=16 a k=32 3.8 Jak vybrat správný model pro data My jsme si v této kapitole ukázali některé nejčastější rozdělení, které popisují náhodné procesy vyskutující se ve fyzikálním světě okolo nás. Při analýze máte zpravidla nějaká data (náhodné proměnné), jejichž chování se nažíte popsat pomocí nějakého modelu. To jaký model na vaše data vybrat záleží především na povaze dat, tedy zda jsou data spojitá nebo diskrétní. Dále pak na tom, jaké hodnoty byste u vašich dat čekali, především, zda májí data nějakou hranici, za kterou data nejsou možná (např. data musí být pozitivní apod.). V poslední řadě pak záleží na tom, zda zvolený model věrohodně vystihuje pravděpodobnost, s kterou data nabývají různých hodnot. Znovu zopakujeme, že model představuje připodobnění reality a jeho užitečnost závisí na otázce, kterou se snažíme modelem zodpovědět. Vždy je dobrým zvykem si data zobrazit. Ze zvažovaného modelu si vygenerujte data a porovnejte je s reálnými daty. Uvažujte jakých extrémních hodnot by váš model mohl dosáhnout a zda jsou takové hodnoty pro problém který zkoumáte vůbec reálné. Zvažte, do jaké míry váše data (nebo jejich sběr) porušují předpoklady vašeho modelu11. Právě díky grafickému zobrazení různých simulací můžete pochopit, jak by mohly různé předpoklady váš model rozbít, nebo za jakých podmínek přestane věrohodně vystihovat vaše data. Nakonec představíme aplikaci, pomocí které si můžete vyzkoušet, jak různé parametry ovlivňují tvar různých rozdělení nebo vypočítat pravděpodobnost, že náhodná proměnná \\(x_i\\) nabyde určitých hodnot \\(a\\). Zdroj: https://github.com/ShinyEd/intro-stats/tree/master/dist_calc 3.9 Cvičení Vymyslete náhodné proměnné, jejichž pravděpodobnostní rozdělení by se dalo popsat binomickým, normálním a poissonovým rozdělením. Např. teplota, hmotnost, výška. Naše schonost tyto jevy měřit je ale často diskrétní, hmotnost můžeme měřit na mg apod. Protože jsou ale jednotky míry často malé, bereme je jako spojitou proměnnou.↩︎ Teoreticky jsou ohraničené, ale prakticky se v přírodě extrémy často nevyskutují. Například teplota neklesá k absolutní nule. Váha zvířat je zpravidla větší než 0 mg apod.↩︎ Pokud by \\(x_i\\) pocházela z jiného rozdělení byl by průměr stále rovný 0 a směrodatná odchylka 1, ale použitá notace by odpovídala rozdělení, z kterého pochází (např. U pro uniformní rozdělení apod.).↩︎ Jedná se o pravděpodobnost, protože počítáme PDF v nějakém intervalu CDF.↩︎ PMF poissonova rozdělení klesá s rostoucí hodnotou \\(x_i\\) rychle k zanedbatelné hodnotě, takže bude nepravděpodobné, že bychom generovali věty s nadpřirozenými počty slov.↩︎ Pokud bychom chtěli věrohodně zachytit nejistotu ohledně počtu slov v našich datech, měli bychom počítat s nejistotou odhadu průměru a směrodatné odchylky rozdělení. Něco, co si ukážeme v příští kapitole.↩︎ Skoro všechna data porušují do nějaké míry alespoň jedne z předpokladů modelu :).↩︎ "],["výběrové-rozdělení-parametrů.html", "Kapitola 4 Výběrové rozdělení parametrů 4.1 Procenta 4.2 Interval spolehlivosti 4.3 Bootstrap 4.4 Průměr 4.5 Rozptyl 4.6 Cvičení", " Kapitola 4 Výběrové rozdělení parametrů Během této kapitoly se dostaneme dále do základů statistiky a představíme si jeden z nejpoužívanějších principů statistiky, který nás bude provázat po zbytku semestru. Opět to bude náročnější kapitola a možná se k ní budete chtít vrátit zpět, jak budeme centrální limitní větu (CLV) aplikovat na další statistické problémy. V této kapitole si představíme její princip. V kapitole o pravděpodobnostních rozděleních 3 jsme si ukázali, že existují různé typy rozdělení, které je možné použít k popsání (připodobnění) toho, jakých hodnot bude proměnná nabývat a v jaké četnosti. Často nás zajímají nejenom hodnoty náhodné proměnné, ale také některé její parametry, jako například průměr nebo procenta. Už víme, že hodnoty náhodné proměnné jsou ovlivněny náhodou, které může mít různé důvody. Je asi intuitivní, že i parametry této proměnné budou touto náhodou ovlivněny a že nebudou vždy stejné. No a protože tyto parametry budou nabývat různých hodnot, pak můžeme jejich rozdělení zkoumat jako výběrové rozdělení parametrů. A jako u každého jiného pravděpodobnostního rozdělení můžeme popsat jaké hodnoty a s jakou frekvencí bychom u proměnné očekávali. Než se pustíme do principů CLV a výběrového rozdělení paramerů, zmíníme pár vět o termilogii, kterou budeme používat. Populační parametry, se zpravidla označují písmeny řecké abecedy. Výběrové parametry pak označujeme písmeny latinské abecedy. Parametr Výběr Populace průměr \\(\\overline{x}\\) \\(\\mu\\) směrodatná odchylka \\(s\\) \\(\\sigma\\) procento \\(p\\) \\(\\pi\\) 4.1 Procenta Vraťme se zpět ke Galtonově boxu. My jsme tento proces simulovali jako \\(x_i \\sim B(13, 0.5)\\). Řekněme, že nás zajímá zjistit jaké procento míčků skončí v prostředním (sedmém) sloupci \\(p\\). Procento je nějaký parametr naší proměnné, který jsme zvolili, ale mohli bychom si zvolit například medián. Toto procento bude pokaždé když kuličky spustíme znova trochu jiné. Centrální limitní věta nám pomůže kvantifikovat, jak jiné toto procento bude. set.seed(42) # pocet micku n &lt;- 10000 # pravdepodobnost uspechu (napravo) p &lt;- 0.5 # 0 by znamenalo všechny kulicky nalevo, 12 všechny kulicky napravo, # celkem 13 možností, jak mohou kulicky skončit s &lt;- 12 vysledek &lt;- rbinom(n = n, size = s, prob = p) cetnost &lt;- table(vysledek) # pouzijeme nazev 6 v tabulce, protoze 0 je prvni sloupec p_7 &lt;- cetnost[names(cetnost) == &quot;6&quot;] / n Například v simulaci, kterou jsme provedli v kodu nahoře je procento kuliček v sedmém sloupci \\(p_7\\) rovné 0.2227. Když jsme simulovali hodnoty náhodné proměnné, tak každá kulička představovala jedno pozorování. V simulaci, kterou si teď předvedeme, vždy spustíme kuličky, spočítáme procento v sedmém sloupci, kuličky pustíme znova, opět spočítáme procento kuliček v sedmém sloupci a tak dále. Každé procento, které takto vypočítáme, představuje jednu hodnotu nějaké nové náhodné proměnné. Nakonec si ukážeme jak vypadá histogram těchto procent. Předtím, než se na animaci podíváte zkuste pomocí vašich znalostí pravděpodobnostních rozdělení odhadnout, jaké bude mít rozdělení průměrů tvar. # funkce na vypocet p_7 rel_cetnost &lt;- function(x, k) { cetnost &lt;- table(x) p &lt;- cetnost[names(cetnost) == k] / sum(cetnost) return(p) } # pocet simulaci S &lt;- 1:90 p_7 &lt;- rep(NA, length(S)) for (i in S) { # spustime kulicky vysledek &lt;- rbinom(n = n, size = s, prob = p) # vypocitame procento kulicek v kazdem sloupci p_7[i] &lt;- rel_cetnost(vysledek, &quot;6&quot;) hist(p_7, breaks = seq(0.21, 0.24, by = 0.002), xlim = c(0.21, 0.24), main = paste0(&quot;Počet opakových spuštění kuliček: &quot;, S[i]), col = &quot;#1f77b4&quot;, xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;Četnost&quot; ) } Graf 4.1: rozdělení procenta kuliček v sedmém sloupci při opakovaném spuštění kuliček Jak je vidět z histogramu v 4.1, rozdělení procent z opakovaných spuštění kuliček je zhruba normálně rozloženo s průměrem 0.23 a směrodatnou odchylkou 0.004, nebo jinak \\(p \\sim N(\\) 0.23 \\(,\\) 0.004 \\()\\). To není žádné překvapení. Jak víme z 3.3 spojité proměnné, které se shlukují okolo nějakého bodu a mohou nabývat hodnot na levo a na pravo od tohoto bodu bývají často normálně rozděleny. Centrální limitní věta nám popisuje tento fenomen a říká nám, jak vypočítat parametry tohoto normálního rozdělení (tedy průměr a směrodatnou odchylku). V našem případě bychom parametry takového rozdělení mohli vypočítat jako \\[p \\sim N(\\pi, \\frac{\\sigma_{\\pi}}{\\sqrt{n}})\\]kde \\(n\\) je velikost výběru (10000 v našem případě), pričemž z 3.2 víme, že rozptyl u procent vypočítáme jako \\(\\sigma^2 = \\pi*(1-\\pi)\\) a směrodatnou odchylku jako \\(\\sigma = \\sqrt{\\pi*(1-\\pi)}\\). Protože teoretická rozdělení parametru využívá k aproximaci normální rozdělení, platí pro PDF stejné překlady, jaké jsme si ukazovali v 3.3, tedy, že jednotlivé hodnoty jsou na sobě nezávislé. Graf 4.2 ukazuje porovnání histogramu z 90 opakovaných puštění kuliček a jejich procent v sedmém sloupci s teoretickým rozdělením tohoto parametru podle CLV12. # vypocitame p p_hat &lt;- dbinom(6, size = 12, prob = 0.5) # vypocitame smerodatnou odchylku dat s_hat &lt;- sqrt(p_hat * (1 - p_hat)) # vypocitame pdf rozlozeni procent x &lt;- seq(0.21, 0.24, length.out = 1000) pdf &lt;- dnorm(x, mean = p_hat, sd = s_hat/sqrt(n)) hist(p_7, breaks = seq(0.21, 0.24, by = 0.002), xlim = c(0.21, 0.24), col = &quot;grey&quot;, main = &quot;&quot;, xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;Četnost&quot; ) # pridame na druhou osu par(new = TRUE) plot(x, pdf, type = &quot;l&quot;, lwd = 2, col = &quot;#1f77b4&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;, bty = &quot;n&quot; ) legend(&quot;topright&quot;, legend = c( &quot;rozdělení procent&quot;, paste0(&quot;N(&quot;, round(p_hat, 2), &quot;,&quot;, round(s_hat/sqrt(n), 3), &quot;)&quot;) ), lwd = c(2, 2), col = c(&quot;grey&quot;, &quot;#1f77b4&quot;), cex = 0.7 ) thicks &lt;- round(seq(0, max(pdf), by = 10), 2) axis(4, at = thicks, labels = thicks) Graf 4.2: Porovnání rozdělení 90 opakování procent kuliček v sedmém sloupci s teoretickým rozdělením procent kuliček v sedmém sloupci podle CLV Jak je zřejmé s parametrů rozdělení, poloha tohoto rozdělení určena průměrem a šířka rozdělení bude ovlivněna \\(\\frac{\\sigma_{\\pi}}{\\sqrt{n}}\\), tedy mírou variability v datech a velikostí našeho výběru. Čím větší velikost výběru, tím menší je směrodatná odchylka rozdělení parametru13. Aby nedocházelo k záměně směrodatné odchylky rozdělení parametru a směrodatné odchylky výběru, používá se pro směrodatnou odchylku rozdělení parametru pojem směrodatná chyba. Doposud jsme pro průměr rozdělení parametru a směrodatnou odchylku používali populační parametry, tedy věděli jsme hodnotu \\(\\pi\\) v populaci a také rozptyl dat \\(\\pi*(1-\\pi)\\). To vychází z toho, že u Galtonova boxu víme, jaký náhodný proces data generuje. Většinou ale tento náhodný proces neznáme a nemáme tedy informace o populačním průměru a směrodatné odchylce. V takovém případě odhadneme obě hodnoty z dat (výběru). Je asi intuitivní, že pokud bude velikost našeho výběru \\(n\\) malá, bude existovat větší nejistota o odhadu populačního průměru a směrodatné odchylky z dat. Abychom tuto dodatečnou nejistotu zachytili v teoretickém pravděpodobnostním rozdělení parametru, použijeme k jeho aproximaci t-rozdělení s \\(n-1\\) stupni volnosti, tedy \\[p \\sim T(p, \\frac{s_p}{\\sqrt{n}}, n-1)\\]14Jak víme z 3.4, t-rozdělení dává větší pravděpodobnost hodnotám více vzdáleným od průměru. Graf 4.3 ukazuje teoretické rozdělení procenta kuliček v sedmém sloupci \\(p_7\\) při \\(n=20\\), \\(n=50\\) a \\(n=100\\). Tedy místo 10000, pustíme pouze 20, 50 a 100 kuliček. Abychom ukázali použití t-rozdělení a odhadu průměru a směrodatné odchylky z výběru, budeme chvilku předstírat, že nevíme jejich teoretické hodnoty a proto \\(p_7\\) a \\(s_7\\) odhadneme z výběru Jak je vidět při puštění pouze 20 kuliček je náš odhad velmi nepřesný. Procento kuliček, které skončí v sedmém sloupci může klidně být od 0 do 0.415. To dává smysl, představte si, jaká míra nejistoty existuje, pokud do Galtonova boxu nasypeme pouze 20 kuliček. S tím, jak se zvětšuje velikost výběru \\(n\\), klesá míra naší nejistoty o odhadu procenta kuliček v sedmém sloupci, klesá směrodatná chyba odhadu a tím pádem o šířka teoretického rozdělení parametru. # definujeme velikost vyberu N &lt;- c(20, 50, 100) # definujeme hodnoty pro ktere budeme pocitat pdf x &lt;- seq(0.00,1, length.out = 1000) # vytvorime prazdný graf, do ktereho budeme pridavat plot(x, type= &quot;n&quot;, xlim = c(0, 0.8), ylim = c(0, 10), xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;PDF&quot;) cols &lt;- rev(RColorBrewer::brewer.pal(length(N) + 1, name = &quot;Blues&quot;) ) for(i in 1:length(N)) { # provedeme vyber a odhadneme p a sd n &lt;- N[i] .vyber &lt;- rbinom(n = n, size = s, prob = p) p_hat &lt;- rel_cetnost(.vyber, &quot;6&quot;) s_hat &lt;- sqrt(p_hat*(1-p_hat)) # vypocteme pdf pro T(p, s/sqrt(n), n-1) podle zobecneneho t-rozlozeni pdf_t &lt;- 1/(s_hat / sqrt(n)) * dt((x - p_hat)/(s_hat / sqrt(n)), df = n-1) # pridame t-rozlozeni lines(x, pdf_t, type = &quot;l&quot;, lwd = 2, col = cols[i]) } # pridame legendu legend(&quot;topright&quot;, legend = c(paste0(&quot;T(p, s/sqrt(n), &quot;, N-1, &quot;)&quot;)), col = cols[1:length(N)], cex = 0.7, lwd = c(2, 2) ) Graf 4.3: Porovnání vlivu velikosti výběru n na směrodatnou chybu odhadu 4.2 Interval spolehlivosti Pokud dokážeme pomocí PDF popsat předpokládáné rozdělení výběrového parametru, dokážeme pomocí CDF spočítat pravděpodobnost, že by výběrový parametr nabyl nějaké hodnoty nebo vyšší/menší. Tedy v případě výběrového parametru procent kuliček v sedmém sloupci, dokážeme spočítat \\(P(p \\ge q)\\) nebo \\(P(p &lt; q)\\). Například, můžeme vypočítat kvantily, které pokryjí 95% hodnot rozdělení výběrového parametru okolo průměru, tedy \\(p_{0.025}\\) a \\(p_{0.975}\\). Procento hodnot rozdělení výběrového parametru pokryté našim kvantilem označujeme jako \\(1-\\alpha\\) a tedy procento hodnot rozdělení výběrového parametru nepokryté našim kvantilem jako \\(\\alpha\\). Graf 4.4 ukazuje teoretické rozdělení výběrového parametru (procenta kuliček v sedmém sloupci) a modrá oblast ukazuje hodnoty p, které jsou menší než \\(p_{0.025}\\) nebo větší než \\(p_{0.975}\\). K vypočítání kvantilů rozdělení použijeme funkci qt. probs &lt;- c(0.025, 0.975) # vypocitame quantily zobecneneho t-rozlozeni pro probs q &lt;- qt(probs, df = n-1) q &lt;- q * s_hat/sqrt(n) + p_hat # vytvorime prazdný graf, do ktereho budeme pridavat plot(x, pdf_t, type = &quot;l&quot;, lwd = 2, col = &quot;grey&quot;, xlim = c(0, 0.8), ylim = c(0, 10), xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;PDF&quot;) # naznacime kvantily a hodnoty vetsi nez nebo mensi nez # pridame jako &quot;h&quot; abychom nazanacili integral lines(x[x &lt; q[1]], pdf_t[x &lt; q[1]], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) lines(x[x &gt; q[2]], pdf_t[x &gt; q[2]], col = &quot;#1f77b4&quot;, type = &quot;h&quot;) legend(&quot;topright&quot;, legend = c(&quot;p_0.025 | p_0.975&quot;, paste0(&quot;T(&quot;, round(p_hat, 2), &quot;,&quot;, round(s_hat / sqrt(n), 2), &quot;,&quot;, tail(N, 1)-1,&quot;)&quot;) ), col = c(&quot;#1f77b4&quot;, &quot;grey&quot;), lwd = c(2, 2), cex = 0.7) Graf 4.4: Kvantily p_0.025 a p_0.975 pro rozdělení T(p, s/sqrt(n), 99) Pojďme si ještě jednou shrnout, co jsme vypočítali. Pomocí CLV jsme vypočítali pravděpodobnostní rozdělení výběrového parametru (procenta kuliček v sedmém sloupci). Toto rozdělení nám říká, s jakou pravděpodobností bychom získali další parametry, pokud bychom provedli nové výběry. Představme si opět na chvilku, že neznáme proces, který procenta kuliček v sedmém sloupci generuje (tedy neznáme populační \\(\\pi_7\\)). Pomocí pravděpodobnostního rozdělení parametru výběru bychom tedy mohli usoudit něco o populačním parametru, konkrétně to, že pokud bychom výběry z populace opakovali, pak by 95% hodnot výběrového parametru bylo v rozmezí \\(p_{0.025}\\) a \\(p_{0.975}\\), tedy konkrétně mezi \\(p_{0.025}=\\) 0.16 a \\(p_{0.975}\\)= 0.34. To je nesmírně užitečná informace, která nám říká něco o populační hodnotě, kterou vůbec neznáme (v našem předstíraném případě). Skoro pokaždé budete při analýze pracovat s výběrem16 a pomocí CLV tedy můžete odhadnout populačního parametru. Gratuluji, právě jste sestrojili interval spolehlivosti! Formálně se interval spolehlivosti vypočítá jako \\[IS_{1-\\alpha} = \\overline{x} +/- t_{\\alpha/2; 1-\\alpha/2} \\frac{s}{\\sqrt{n}}\\]kde \\(t\\) je hodnota kvantilu t-rozdělení s \\(n-1\\) stupni volnosti pro \\(\\alpha/2\\) a \\(1-\\alpha/2\\). t_q &lt;- qt(probs, n-1) i_s &lt;- p_hat + t_q * s_hat/sqrt(n) Pokud bychom vypočítali interval spolehlivosti podle výše zmíněného vzorce pro \\(\\alpha=0.05\\), pak bychom měli hodnoty mezi [0.16, 0.34], tedy pokud bychom výběry z populace opakovali, pak by 95% z nich bylo v rozmezí [0.16, 0.34]. Ještě ukážeme to, co jsme zmínili nahoře, tedy, že interval spolehlivosti nám říká v jakém rozmezí bychom očekávali dané procento hodnot (toto procento je dané námi zvolenou hladinou \\(1-\\alpha\\)) výběrového parametru při opakovaných výběrech. Budeme simulovat 100 výběrů, u každého výběru spočítáme interval spolehlivosti a zobrazíme ho šedě, pokud nepokrývá populační hodnotu \\(\\pi_7\\) a modře pokud ji pokrývá. Protože naše hladina spolehlivosti je zvolena na 90%, očekáváme, že ze 100 intervalů spolehlivosti, jich asi 90 pokryje populační hodnotu a asi 10 ji nepokryje17. # vytvorime si funkci na vypocet is interval_spolehlivosti &lt;- function(p_hat, alpha, n) { # spocitame populacni sd s_hat &lt;- sqrt(p_hat *(1 - p_hat)) # vypocitame pravdepodobnosti 1-alpha/2 probs &lt;- c(alpha / 2, 1 - (alpha / 2)) # spocitame kvantily pro 1/alpha/2 t_q &lt;- qt(probs, n-1) # spocitame interval spolehlivosti i_s &lt;- p_hat + t_q * s_hat/sqrt(n) return(i_s) } # pocet simulaci S &lt;- 100 # velikost vyberu n &lt;- 100 # urcime hladinu spolehlivosti alpha &lt;- 0.1 # populacni hodnota pi &lt;- dbinom(x = 6, size = 12, prob = 0.5) # vyvorime prazdny graf plot(1:S, type= &quot;n&quot;, xlab = &quot;Pořadí&quot;, ylab = &quot;Procento kuliček v sedmém sloupci&quot;, ylim = c(0.05, 0.45)) for(x in 1:S) { # udelame vyber a spocitame ... # ...vyberovy prumer a smerodatnou odchylku vyber &lt;- rbinom(n, size = 12, prob = 0.5) p_hat &lt;- rel_cetnost(vyber, &quot;6&quot;) s_hat &lt;- sqrt(p_hat * (1 - p_hat)) # spocitame interval spolehlivosti i_s &lt;-interval_spolehlivosti(p_hat, alpha, n) # zvolime barvu podletoho, zda is pokryva populacni parametr .col &lt;- &quot;#1f77b4&quot; if(i_s[1] &gt; pi | i_s[2] &lt; pi) { .col &lt;- &quot;grey&quot; } # zobrazime v grafu bodovy odhad points(x, p_hat, pch = 19, col = .col) lines(c(x, x), i_s, lwd = 1, col = .col) } # pridame populacni prumer abline(h = pi, col = &quot;black&quot;, lwd = 2) legend(&quot;topright&quot;, legend = &quot;populační parametr&quot;, col = &quot;black&quot;, lwd = 2) Graf 4.5: Ukázka opakovaných výběrů z populace a jak hladina alpha ovlivňuje podíl intervalů pokrytých opakovaných výběrem Šířku intervalu spolehlivosti (nejistoty ohledně hodnoty populačního parametru tedy ovlivňuje): směrodatná chyba odhadu parametru \\(\\frac{s}{\\sqrt{n}}\\) (a tedy jaká je variabilita v našich výběrových datech, měřeno směrodatnou odchylkou a velikostí výběru) hladina spolehlivosti \\(\\alpha\\). Čím menší je alpha, tím je interval spolehlivosti užší, ale tím více procent opakovaných výběrů by hodnotu populačního parametru nepokrylo. Standardně tedy zvolíme \\(\\alpha\\) relativně malé, aby náš interval spolehlivosti pokryl populační parametr ve velké většině případů, a to i za cenu toho, že intrval spolehlivosti bude širší. 4.3 Bootstrap Pojďme se na chvilku vrátit ke grafu 4.1. Na tomto grafu jsme ukázali hypotetickou situaci, kdy jsme věděli jak vypadá rozdělení proměnné v celé populaci. Dělali jsme opakovené výběry, abychom ukázali, jak by vypadalo rozdělení parametru, kdybychom u každého výběru vypočítali parametr, který nás zajímá. Ukazuje se, že pomocí podobné metody můžeme vypočítat rozdělení parametru a to i bez toho, abychom měli data za celou populaci. Místo toho, abychom dělali náhodné výběry z populačních dat, tak budeme dělat náhodné výběry s opakováním z výběru o velikosti \\(n\\). Výstupem tohoto výpočtu je vektor parametru, který nás zajímá, o velikosti rovnající se počtu výběrů s opakováním. Je důležité zdůraznit, že přestože počítáme rozdělení parametru empiricky, některé ze předpokladů, které jsme zmínili u výpočtu teoretického rozdělení, jsou stále platné. Především předpoklad o nezávislosti jednotlivých pozorování a o náhodném výběru z populace (pokud chceme dělat úsudky o celé populaci). # pocet bootstrap vyberu S &lt;- 1e5 p_hat &lt;- rep(NA, S) for(i in 1:S) { # udelame vyber s opakovanim boostrap_vyber &lt;- sample(.vyber, size = n, replace = TRUE) # vypocitame parametr vyberu p_hat[i] &lt;- rel_cetnost(boostrap_vyber, &quot;6&quot;) } hist(p_hat, col = &quot;#1f77b4&quot;, xlab = &quot;Procento kuliček v sedmém sloupci&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;) Graf 4.6: Boostrap výpočet procenta kuliček v sedmém sloupci # vypocitale kvantil 0.025 a 0.975 alpha &lt;- 0.05 i_s &lt;- quantile(p_hat, probs = c(alpha/2, 1-alpha/2)) Jako příklad se vrátíme k našemu výběru z 4.5, kdy jsme na chvilku předstírali, že neznáme populační data, respektive náhodný proces, který data generuje. Graf 4.6 ukazuje empirické rozdělení procenta kuliček, které skončí v sedmého sloupci. Pokud bychom chtěli vypočítat interval spolehlivosti, vypočítáme emprirický kvantil, tak jako jsme dělali v kapitole o mírách polohy 2.3. 95% interval spolehlivosti je [0.17, 0.34]. Pokud můžeme vypočítat rozdělení parametru analyticky, proč bychom měli používat bootstrap, který je více náročný z hlediska potřeby provádět opakované výběry. Je to proto, že pomocí bootsrapu můžeme vypočítat empirické rozdělení i parametrů, pro které neexistuje jednoduché teoretické rozdělení. Pracujeme opět se stejným hypotetickým výběrem o velikosti \\(n=\\) 100. Pokud bychom například chtěli vypočítat interval spolehlivosti poměru kuliček v pátém a sedmém sloupci, tedy \\(\\frac{p_{5}}{p_{7}}\\), nebude jednoduché vypočítat směrodatnou chybu18. S pomocí bootstrapu můžeme udělat \\(S\\) výběrů s opakováním, u každého výběru vypočítat \\(\\frac{p_{5}}{p_{7}}\\). Získáme tak vektor o velikosti \\(S\\), který můžeme použít k vypočítání empirického rozdělení parametru \\(\\frac{p_{5}}{p_{7}}\\). # vytvorime funkci bootstrap &lt;- function(x, func, S) { n &lt;- length(x) parameter &lt;- rep(NA, S) # provedeme vybery s opakovanim for(i in 1:S) { # udelame vyber s opakovanim boostrap_vyber &lt;- sample(x, size = n, replace = TRUE) # vypocitame parametr vyberu parameter[i] &lt;- func(boostrap_vyber) } return(parameter) } # funkce na vypocet pomeru pomer &lt;- function(x) { ans &lt;- rel_cetnost(x, &quot;4&quot;) / rel_cetnost(x, &quot;6&quot;) # pokud nebudou zadne kulicky v jednom ze sloupcu ... # ...vratit NA if(length(ans) == 0) { return(NA) } else { return(ans) } } # vypocitame bootstrap parametru p_hat &lt;- bootstrap(.vyber, pomer, 1e5) hist(p_hat, col = &quot;#1f77b4&quot;, xlab = &quot;Poměr procenta kuliček v pátém a sedmém sloupci&quot;, ylab = &quot;Četnost&quot;, main = &quot;&quot;) # vypocitale kvantil 0.025 a 0.975 alpha &lt;- 0.05 i_s &lt;- quantile(p_hat, probs = c(alpha/2, 1-alpha/2), na.rm = TRUE) 95% interval spolehlivosti pro \\(\\frac{p_{5}}{p_{7}}\\) je [0.12, 0.67]. 4.4 Průměr Jak jsme uvedli v 4.1, CLV říká, že průměr nezávislých proměnných \\(\\theta\\) bude normálně rozložen se směrodatnou chybou rovnou \\(\\frac{s}{\\sqrt(n)}\\)19, tedy \\(\\theta \\sim N(\\theta, \\frac{s}{\\sqrt(n)})\\). U procent \\(p\\) jsme směrodatnou odchylku počítali jako \\(\\sqrt{p(1-p)}\\). U průměru počítáme směrodatnou odchylku jako \\(s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\overline{x})^2}\\), tedy jak jsme si ukazovali v 2.2. rozdělení výběrového průměru náhodné proměnné \\(x\\), tedy bude \\[\\overline{x} \\sim T(\\overline{x}, \\frac{s}{\\sqrt{n}}, n-1)\\]Platí přitom, že nezáleží na tom, zda náhodné proměnná pochází z normálního rozdělení. CLV se týká průměru této proměnné. Jako příklad si ukážeme hypotetickou situaci, v které budeme vědět populační rozdělení náhodné proměnné \\(x_i\\), která pochází z uniformního rozdělení \\(x_i \\sim U(-10, 10)\\). Provedeme 5000 výběrů o velikosti \\(n=100\\) z této populace, u každého spočítáme průměr a poté zobrazíme rozdělení těchto průměrů histogramem. # pocet simulaci S &lt;- 5000 # vektor, kam budeme ukladat prumery prumer &lt;- rep(NA, S) # velikost vyberu n &lt;- 100 # parametry uniformniho rozlozeni a &lt;- -10 b &lt;- 10 for(i in 1:S) { vyber &lt;- runif(n, min = -10, max = 10) prumer[i] &lt;- mean(vyber) } hist(prumer, col = &quot;#1f77b4&quot;, breaks = 10, xlim = c(-3, 3), main = &quot;&quot;, xlab = &quot;Průměr x~U(-10, 10)&quot;, ylab = &quot;Četnost&quot; ) legend(&quot;topright&quot;, legend = paste0(&quot;průměr: &quot;, round(mean(prumer), 2), &quot;\\n&quot;, &quot;směr.odchylka: &quot;, round(sd(prumer), 2)), lwd = 10, col = &quot;#1f77b4&quot;, cex = 0.7 ) Graf 4.7: Průměr 5000 výběrů o velikosti n=100 z x~U(-10, 10) Jak je vidět z grafu 4.7 rozdělení průměrů z 5000 výběrů je normálně rozloženo. Průměr těchto 5000 průměrů je 0 a směrodatná odchylka je 0.59. Porovnejme tyto hodnoty s teoretickým rozdělením podle CLV. Z kapitoly o uniformním rozdělení 3.5 víme, že průměr se vypočítá jako \\(\\mu = \\frac{1}{2}(a+b)\\), tedy pro \\(U(-10 ,10)\\) je to hodnota 0. Rozptyl je \\(\\sigma^2 = \\frac{1}{12}(b-a)^2\\), tedy pro \\(U(-10 ,10)\\) hodnota 33.33. Směrodatná chyba odhadu je \\(\\frac{s}{\\sqrt{n}}\\), tedy 0.58. Jak je vidět, parametry normálního rozdělení průměru podle CLV se shodují s údaji, které jsme získali při simulaci naší hypotetické situace. Pokud bychom chtěli vypočítat interval spolehlivosti, můžeme postupovat stejně jako jsme uvedli v kapitole o intervalu spolehlivosti 4.2. Pokud máme k dispozici výběr proměnné \\(x_i\\), pak platí \\[IS_{1-\\alpha} = \\overline{x} +/- t_{\\alpha/2; 1-\\alpha/2} \\frac{s}{\\sqrt{n}}\\]tedy, že k výběrovému průměru přičteme/odečteme hodnotu směrodatné chyby odhadu vynásobenou kvantilem t-rozdělení (podle zvolené hladiny \\(\\alpha\\)). # zvolime hladinu spolehlivosti alpha &lt;- 0.05 # vypocitame vyberovy prumer x_hat &lt;- mean(vyber) # vypocitame vyberovou smer. odchylku s_hat &lt;- sd(vyber) # vypocitame kvantil t rozdeleni pro n-1 t_q &lt;- qt(c(alpha/2, 1-alpha/2), df = n-1) # vypocitame interval spolehlivosti i_s &lt;- x_hat + t_q * s_hat/sqrt(n) # zaokrouhlime pro prehlednost i_s &lt;- round(i_s, 2) Abychom si ukázali výpočet intervalu spolehlivosti, použijeme poslední výběr z naší simulace. Průměr tohoto výběru o velikosti \\(n=\\) 100 je -0.9 a směrodatná odchylka je 6. 95% interval spolehlivosti je [-2.09, 0.29]. Tedy, pokud bychom výběry opakovali, pak by 95% výběrů mělo průměr v rozmezí [-2.09, 0.29]. Protože se jedná o hypotetickou situaci, tak my víme, že populační výběr pro \\(U(-10, 10)\\) je roven 0. Jak je vidět, přestože náš výběr je pouze o velikosti \\(n=\\) 100, pak dokážeme pomocí intervalu spolehlivosti získat informaci o populačním průměru. 4.5 Rozptyl Poslední z parametrů, jehož výběrové rozdělění si představíme je rozptyl, respektive směrodatná odchylka. Stejně jako platí u průměru, i rozptyl různých náhodných proměnných se bude lišit v závislosti na výběru. Interval spolehlivosti pro výběrový rozptyl \\(s^2\\) je možné zapsat analytickou formou jako \\[\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}\\]kde \\(\\chi^2\\) je kvantil chi-kvadrát rozdělení s \\(n-1\\) stupni volnosti. K výpočtu výběrového rozdělení rozptylu bychom také mohli použít bootstrap. Vraťme se zpátky k výpočtu počtu slov ve větě z kapitoly 3.6, kde jsme počet slov ve větě \\(s_i\\) modelovali jako \\(log(s_i) \\sim T(2.61, 0.66, 66)\\). Zkoumaný proslov můžeme chápat jako výběr z populace proslovů. Tedy vypočítaný výběrový průměr \\(log(\\overline{s}) = 2.61\\) a výběrová směrodatná odchylka rovná \\(0.66\\) se budou lišit v závislosti na výběru. Tuto nejistotu bychom měli vzít v potaz i při predikci procenta vět delších než 20 slov. Graf 4.8 ukazuje výběrové rozdělení průměru a výběrové rozdělení směrodatné odchylky. n &lt;- 66 vyberovy_prumer &lt;- 2.61 vyberova_sd &lt;- 0.66 smerodatna_chyba &lt;- vyberova_sd / sqrt(n) # vypocitame pdf vyberoveho rozdeleni prumery x &lt;- seq(2.2, 3, length.out = 1000) pdf_prumer &lt;- dt((x - vyberovy_prumer) / smerodatna_chyba, df = n-1) * 1/smerodatna_chyba par(mfrow = c(1, 2)) # vyberove rozdeleni prumeru plot(x, pdf_prumer, xlab = &quot;Logaritmus počtu slov ve větě&quot;, ylab = &quot;PDF&quot;, col = &quot;#1f77b4&quot;, main = &quot;Výběrové rozdělení průměru&quot;, type = &quot;l&quot;, lwd = 2, cex.main = 0.7) # vypocitame pdf vyberove směrodatné odchylky r_s &lt;- sqrt((n-1)*vyberova_sd^2 / rchisq(1e5, n-1)) pdf_s &lt;- density(r_s, n=1e5) # vyberove rozdeleni rozptylu plot(pdf_s, xlab = &quot;Logaritmus počtu slov ve větě&quot;, ylab = &quot;PDF&quot;, col = &quot;#1f77b4&quot;, main = &quot;Výběrové rozdělení směr.odchylky&quot;, type = &quot;l&quot;, lwd = 2, cex.main = 0.7) Graf 4.8: PDF výběrového průměru a směrodatné odchylky # vypocitame jeste interval spolehlivosti alpha &lt;- 0.05 q &lt;- c(alpha/2, 1-alpha/2) is_prumer &lt;- vyberovy_prumer + smerodatna_chyba * qt(q, df = n-1) is_sd &lt;- quantile(r_s, probs = q) 95% interval spolehlivosti pro průměr je [2.45, 2.77] a 95% interval spolehlivosti pro směrodatnou odchylku je [0.56, 0.8]. Pokud bychom převedli hodnoty zpět na původní škálu, pak by byl 95% interval spolehlivosti pro průměr počtu slov ve větě [11.56, 15.99] a 95% interval spolehlivosti pro směrodatnou odchylku je [1.76, 2.22]. Abychom tuto nejistotu vzali v potaz při naší predikci budeme simulovat výběry z t-rozdělení, kde zohledníme vypočítanou nejistotu ve výběrovém průměru a směrodatné odchylce. # pocet simulaci s &lt;- 1e5 # matrix na ukladani simulaci S &lt;- matrix(NA, nrow = s, ncol = n) for(i in 1:s) { S[i, ] &lt;- rt(n, df = n-1) * sqrt((n-1)*vyberova_sd^2 / rchisq(1, n-1)) + # vyberovy rozptyl rnorm(1, vyberovy_prumer, smerodatna_chyba) # vyberovy prumer } # zobrazime prvnich 100 plot(s, type = &quot;l&quot;, xlab = &quot;log(s)&quot;, ylab = &quot;Hustota pravděpodobnosti (PDF)&quot;, main = &quot;&quot;, col = &quot;#1f77b4&quot;, xlim = c(0, 5), ylim = c(0, 0.85), lwd = 2 ) for (i in 1:50) { # provedeme vybery z t rozdeleni o stejne velikosti vyberu lines(density(S[i, ]), col = adjustcolor(&quot;black&quot;, alpha.f = 0.2)) } Graf 4.9: Prvních 50 simulací výběrů z t-rozdělení při n=66 a zohlednění nejistoty výběrového průměru a směrodatné odchylky # vypocitame (P &gt; log(20)) q &lt;- log(20) cdf &lt;- apply(S, 1, function(x) sum(x &gt; q) / n) i_s &lt;- quantile(cdf, probs = c(alpha/2, 1-alpha/2)) Provedeme 10^{5} simulaci, v každé simulaci provedeme výběr ze zobezněného t rozdělení, o velikosti \\(n=\\) 66. U každého výběru vezmeme v potaz nejistotu o odhadu výběrového průměru a výběrové směrodatné odchylky. Graf 4.9 ukazuje prvních 50 simulací počtu slov ve větě \\(log(s_i)\\). Abychom spočítali pravděpodobnost, že v libovolném budoucím proslovu o 66 větách bude věta delší než 20 slov, tedy \\(P(log(s_i) &gt; log(20))\\), pak u každé simulace spočítáme % hodnot větších než \\(log(20)\\). Protože existuje nejistota ohledně každého výběru, pak je i ohledně odhadu \\(P(log(s_i) &gt; log(20))\\) nejistota. Průměr odhadu \\(P(log(s_i) &gt; log(20))\\) je 0.28 a směrodatná odchylka 0.07. Můžeme použít empirický kvantil a výpočítat, že 95% interval spolehlivosti pro podíl vět s počet slov větších než 20 je [15.15, 42.42]20. Jak je tedy vidět interval spolehlivosti je široký. To plyne z toho, že v samotných datech je velký rozptyl v počtu slov ve větě a také z toho, že naše data obsahují pouze 66 vět. 4.6 Cvičení Spočtěte, v jakém intervalu se bude pohybovat průměrná doba čekání na tramvaj, když tramvaj pojede jednou za 8 minut. Spočtěte 90% interval spolehlivosti a okomentuje, co tento interval vyjadřuje. \\(p_7 \\sim N(0.19, \\sqrt {\\frac{0.19*0.81}{n}})\\)↩︎ Tento vztah není lineární, tedy 2x větší výběr neznamená 2x menší směrodatnou odchylku rozdělení parametru, směrodatná odchylka rozdělení klesá \\(\\frac{1}{\\sqrt{n}}\\).↩︎ Směrodatná odchylka odhadnutá z dat se označuje \\(s\\).↩︎ Velikost tohoto výběru je dokonce tak malá, že aproximace t-rozdělením nemusí být přesná.↩︎ Ať už výběrem z populace jedniců, nebo výběrem výrobků z nějakého výrobního procesu nebo výběrem teplot měřených v nějaký čas dne apod.↩︎ Protože náš výběr je pouze 100, nebude přesně 10 a 90.↩︎ Pro představu zde je analytické řešení.↩︎ Směrodatnou odchylku parametru nazýváme směrodatná chyba, aby nedocházelo k záměně se směrodatnou odchylkou výběru.↩︎ Při libovolném budoucím proslovu o 66 větách \\(n=66\\). Pokud by byl budoucí proslov delší, byl by interval spolehlivosti pro odhad \\(P(log(s_i) &gt; log(20))\\) menší, protože bychom měli více vět a tím pádem stabilnější odhad.↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
